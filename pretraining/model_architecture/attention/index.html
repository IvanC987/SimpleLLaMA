
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../embeddings/">
      
      
        <link rel="next" href="../normalization/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Attention - SimpleLLaMA Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#attention" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="SimpleLLaMA Documentation" class="md-header__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SimpleLLaMA Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Attention
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="SimpleLLaMA Documentation" class="md-nav__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SimpleLLaMA Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Pretraining
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset Preparation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tokenization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Tokenization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/algorithms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Usage
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" checked>
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Model Architecture
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Model Architecture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Attention
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Attention
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vanilla-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Vanilla Self-Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-this-project-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      In This Project: Implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feedforward/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FeedForward
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../decoder_block/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Layer Block
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../end_to_end/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    End To End
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Beginner)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Beginner)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/model_config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Configurations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/dataset_and_batching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset and Batching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/scheduler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/training_loop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Loop
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Advanced)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Advanced)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/recap/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basics Recap
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/optimizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimizer Details
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/loss_function/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cross Entropy Loss
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/gradient_accumulation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gradient Accumulation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/ddp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Data Parallel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/throughput_optimizations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Throughput Optimizations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/ckpt_and_eval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Checkpointing and Evaluations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/final_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Final Walkthrough
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Supervised Fine-Tuning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/prompt_formatting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Formatting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/finetuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning Process
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            RLHF
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rlhf/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rlhf/methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Misc
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Misc
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/benchmarking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmarking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Custom Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Custom Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/pretraining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#vanilla-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Vanilla Self-Attention
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#in-this-project-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      In This Project: Implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="attention">Attention</h1>
<p>Attention is the heart of the transformer. It’s the mechanism that lets the model decide:<br />
<strong>“Which other tokens in the sequence are relevant for predicting the next one?”</strong></p>
<hr />
<h2 id="vanilla-self-attention">Vanilla Self-Attention</h2>
<p>In the vanilla self-attention, we first start out with a given tensor, <code>x</code>, of shape <code>(batch, seq_len, n_embd)</code> as mentioned in the previous <code>embeddings</code> section.</p>
<p>Given that tensor, we compute three tensors, Each of them is a linear projection from the input tensor <code>x</code><br />
- <strong>Query (Q)</strong>: what each token is looking for.<br />
- <strong>Key (K)</strong>: what each token offers.<br />
- <strong>Value (V)</strong>: the information carried by each token.  </p>
<p>This is done by either creating 3 separate linear layers in the constructor for the self-attention class: </p>
<div class="highlight"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>

<span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>  <span class="c1"># This will be used and touched upon later on</span>
</code></pre></div>
<p>Where it can later be invoked to create the Q, K, and V tensors that will be used in attention computation:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Takes an input tensor x of shape (batch, seq_len, n_embd) and linearly project it into another tensor, retaining the shape</span>
<span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  
<span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<p>However a more common method is the merge all of those linear layers into a single one, for more efficient computation:
<div class="highlight"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">)</span>

<span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
</code></pre></div></p>
<p>Then later on, use that to get: </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Input shape: (batch, seq_len, n_embd), output shape: (batch, seq_len, 3 * n_embd)</span>
<span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Split the tensor along the last dimension </span>
<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<p>Both of those will produce the same result. </p>
<p>At this point, given an input tensor <code>x</code>, we now have 3 separate tensors <code>q</code>, <code>k</code>, and <code>v</code>, all of the shape <code>(batch, seq_len, n_embd)</code>  </p>
<p>Next, we 'expand' the tensor from 3d to 4d, using the <code>n_heads</code> hyperparameter. 
<code>n_heads</code> defines how many 'heads' we want in attention. Further explanation as to what it does will be given below. </p>
<p>We would use the last dimension, <code>n_embd</code> and divide that into <code>(n_heads, head_dim)</code>, based on the formula <code>head_dim = n_embd // n_heads</code>
For example, given <code>n_embd=1024</code>, <code>n_heads=16</code>, then <code>head_dim=1024//16=64</code>, meaning we transform our 1024 embedding dimensions into 16 heads, each head have 64 dimension to work with. 
It is crucial to add an assertion that <code>n_embd % n_heads == 0</code> to make sure it evenly divides. </p>
<p>Given the hyperparameter <code>n_heads</code>, calculate <code>head_dim</code>, then view/reshape the tensor accordingly as follows: 
<div class="highlight"><pre><span></span><code><span class="c1"># (batch, seq_len, n_embd) -&gt; (batch, seq_len, n_heads, head_dim)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</code></pre></div></p>
<p>Finally, we swap the dimension for <code>seq_len</code> and <code>n_heads</code></p>
<div class="highlight"><pre><span></span><code><span class="c1"># (batch, seq_len, n_heads, head_dim) -&gt; (batch, n_heads, seq_len, head_dim)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div>
<p>Now that our Q, K, V matrices are all of shape <code>(batch, n_heads, seq_len, head_dim)</code>, we apply the self attention formula: </p>
<div class="arithmatex">\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + Mask \right) V
\]</div>
<p>The first step is to compute <span class="arithmatex">\(<span class="arithmatex">\(QK^\top\)</span>\)</span>
Looking at the shape of the tensors, we will get an output:
<code>(batch, n_heads, seq_len, head_dim) @ (batch, n_heads, head_dim, seq_len) = (batch, n_heads, seq_len, seq_len)</code><br />
which we can denote as <code>attn_scores = QK^T</code>  </p>
<p>Note the <code>(seq_len, seq_len)</code> ending dimensions. That's the reason why people say that attention computation scales quadratically w.r.t the seq_length of the model</p>
<p>Next, we apply element-wise division on the computed matrix, where the divisor is the sqrt of <code>d_k</code>, which from the paper refers to the dimensions of each head (<code>head_dim</code>)</p>
<p><code>attn_scores = attn_scores / math.sqrt(head_dim)</code></p>
<p>The attention scores is normalized by head-dim primarily because of the softmax operation that will immediately take place next.<br />
In short, typically the Q, K, V matrices have unit gaussian distribution, when we apply matrix multiplication, the variance scales by <code>head_dim</code>
However with how softmax works, values that are more than, say, 3 units less than the maximum in the dimension of interest will be heavily squashed to approximately 0. 
In our example scenario, if <code>head_dim=64</code>, that means the std increased from 1 to 8, which would compress the tensor into something similar to a one-hot vector.  </p>
<p>Moving along, after we normalize the attention scores, which doesn't change the shape of the tensor, we would need to apply a triangular mask to <code>attn_scores</code>.  </p>
<p>Typically, LLMs would add in these mask to prevent the model from 'cheating' by looking at future tokens.<br />
Sort of like:<br />
If someone gives you a sentence, 'I love my dog' and asks, 'What is the word after 'my'?'
It's trivial. The answer is 'dog'. However masking prevents that by, as the name suggests, masking out the next token. 
In that same example, the other person would give 'I love my _' and then ask, 'What is the word after 'my'?'</p>
<p>Now in this example, let's use the sentence: "That is a blue dog" tokenized into <code>[That, is, a, blue, dog]</code> (Since there are only 5 tokens, that means <code>seq_len=5</code> in this example)
After going through the embedding layer and above steps, we will reach a tensor of shape <code>(batch, n_heads, 5, 5)</code>
Grabbing one of the (5, 5) matrices arbitrarily might look something like: </p>
<div class="highlight"><pre><span></span><code>&quot;That&quot;   &quot;is&quot;     &quot;a&quot;      &quot;blue&quot;   &quot;dog&quot;
----------------------------------------------
[ 1.93    1.49     0.90    -2.11     0.68 ]   ← &quot;That&quot;
[-1.23   -0.04    -1.60    -0.75    -0.69 ]   ← &quot;is&quot;
[-0.49    0.24    -1.11     0.09    -2.32 ]   ← &quot;a&quot;
[-0.22   -1.38    -0.40     0.80    -0.62 ]   ← &quot;blue&quot;
[-0.59   -0.06    -0.83     0.33    -1.56 ]   ← &quot;dog&quot;
</code></pre></div>
<p>That tells us how much attention each token pays to each other. 
Now these are un-normalized values, so it would be hard to interpret, at least for now. </p>
<p>We then apply the triangular mask that prevents tokens to look ahead. It would be something like: </p>
<div class="highlight"><pre><span></span><code>[  0   -∞   -∞   -∞   -∞ ]
[  0    0   -∞   -∞   -∞ ]
[  0    0    0   -∞   -∞ ]
[  0    0    0    0   -∞ ]
[  0    0    0    0    0 ]
</code></pre></div>
<p>Applying via element-wise addition:
<code>attn_scores = attn_scores + mask</code> </p>
<p>The result would now look like: 
<div class="highlight"><pre><span></span><code>&quot;That&quot;   &quot;is&quot;     &quot;a&quot;      &quot;blue&quot;   &quot;dog&quot;
----------------------------------------------
[  1.93    -∞      -∞      -∞      -∞  ]   ← &quot;That&quot;
[-1.23   -0.04     -∞      -∞      -∞  ]   ← &quot;is&quot;
[-0.49    0.24   -1.11     -∞      -∞  ]   ← &quot;a&quot;
[-0.22   -1.38   -0.40     0.80     -∞  ]   ← &quot;blue&quot;
[-0.59   -0.06   -0.83     0.33   -1.56 ]   ← &quot;dog&quot;
</code></pre></div></p>
<p>we pass it through the softmax function to transform it into something like a probability distribution. </p>
<div class="highlight"><pre><span></span><code><span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&quot;That&quot;   &quot;is&quot;     &quot;a&quot;      &quot;blue&quot;   &quot;dog&quot;
----------------------------------------------
[1.0000   0.0000   0.0000   0.0000   0.0000 ]   ← &quot;That&quot;
[0.2330   0.7670   0.0000   0.0000   0.0000 ]   ← &quot;is&quot;
[0.2759   0.5753   0.1488   0.0000   0.0000 ]   ← &quot;a&quot;
[0.2032   0.0632   0.1699   0.5637   0.0000 ]   ← &quot;blue&quot;
[0.1566   0.2658   0.1236   0.3942   0.0596 ]   ← &quot;dog&quot;
</code></pre></div>
<p>As you can see, starting out with the token corresponding to 'That', it can only pay attention to itself, since it's the first token in the sequence
Next is the word 'is', which splits the attention between 'That' and itself
So on and so forth. This tells the model how much each token attends to each other. </p>
<p>At this point, the <code>attn_weights</code> tensor is still of shape <code>(batch, n_heads, seq_len, seq_len)</code> since both normalization and softmax doesn't change the tensor shape  </p>
<p>Now we process the final steps: </p>
<div class="highlight"><pre><span></span><code><span class="c1"># (batch, n_heads, seq_len, seq_len) @ (batch, n_heads, seq_len, head_dim) = (batch, n_heads, seq_len, head_dim)</span>
<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">v</span>

<span class="c1"># (batch, n_heads, seq_len, head_dim) -&gt; (batch, seq_len, n_heads, head_dim) -&gt; (batch, seq_len, n_embd)</span>
<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>

<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</code></pre></div>
<p>The final step here is to first matrix multiply with the <code>v</code> tensor to get the tensor of shape <code>(batch, n_heads, seq_len, head_dim)</code>
We revert the permutation and viewing to get back the original input shape <code>(batch, seq_len, n_embd)</code>
Finally, apply the output projection matrix to <code>attn_output</code> before returning. </p>
<p>So what's the purpose of output matrix? </p>
<p>One can think of it as a way to combine the information that each head learned. 
Recall that when we apply attention, we use multiple heads. Each head process their own 'chunk' of embedding dimensions compeltely separately from each other. 
It's beneficial to allow them to learn their own information, however at the end, we merely concatenate them together. </p>
<p>The final output projection matrix allows the information to get 'aggregated' and combined. </p>
<hr />
<h2 id="in-this-project-implementation">In This Project: Implementation</h2>
<p>In <code>MHSelfAttention</code>, queries, keys, and values are created together, where it's first viewed then chunked along the last dimension:</p>
<div class="highlight"><pre><span></span><code><span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  
<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">h_dim</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Each input token embedding is linearly projected into Q, K, V vectors.  </li>
<li>Shape after splitting: <code>(batch, seq_len, n_heads, h_dim)</code>.  </li>
</ul>
<p>Then, before computing attention, <strong>Rotary Position Embeddings (RoPE)</strong> are applied to Q and K:</p>
<div class="highlight"><pre><span></span><code><span class="n">q</span> <span class="o">=</span> <span class="n">apply_rotary_embeddings</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">freqs_complex</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">apply_rotary_embeddings</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">freqs_complex</span><span class="p">)</span>
</code></pre></div>
<p>Why? Token embeddings alone tell the model what a word is, but not where it appears. Without positional info,  </p>
<ul>
<li>“The cat sat on the mat.”  </li>
<li>“The mat sat on the cat.”  </li>
</ul>
<p>would look identical.</p>
<p>Instead of adding positional vectors (like the original Transformer’s sinusoidal method), RoPE rotates Q and K in the complex plane by an amount proportional to their position. This makes attention directly sensitive to relative distances between tokens.</p>
<p>For our purposes, you can think of RoPE as: “a lightweight operation on Q and K that encodes order, without changing tensor shapes.”</p>
<p>(If you want to dive deeper, check the RoPE paper on arxiv.)</p>
<p>Next, permute the tensors then apply scaled dot-product attention:</p>
<div class="highlight"><pre><span></span><code><span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">h_dim</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">@</span> <span class="n">v</span>
</code></pre></div>
<ul>
<li>The causal mask ensures each token can only attend to past tokens (left-to-right).  </li>
<li>Softmax converts similarity scores into weights.  </li>
<li>Weighted sum with V produces the final attended representation.</li>
</ul>
<p>Finally, results from all heads are concatenated and passed through a linear projection to mix information.</p>
<hr />
<h2 id="notes">Notes</h2>
<ol>
<li>
<p><strong>Why not Multi-Query Attention?</strong><br />
   The original LLaMA-2 paper uses <em>multi-query attention</em> (MQA), where all heads share the same K and V but have separate Q.<br />
   This greatly reduces KV-cache memory usage, which is important for scaling to very large models and efficient inference.<br />
   For this project, memory pressure from KV-cache isn’t a bottleneck, so standard multi-head attention is simpler and sufficient.</p>
</li>
<li>
<p><strong>What about DeepSeek’s MLA?</strong><br />
   This project includes an optional implementation of <strong>Multi-Head Latent Attention (MLA)</strong>, which is a refinement that reduces KV-cache memory even further while keeping multiple latent spaces.<br />
   It’s more efficient than MQA, but again — KV-cache isn’t the limiting factor here.<br />
   Since the focus is educational clarity, SimpleLLaMA sticks with classic multi-head attention.</p>
</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.math"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>