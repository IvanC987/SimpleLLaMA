
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../scheduler/">
      
      
        <link rel="next" href="../../training_advanced/recap/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Training Loop - SimpleLLaMA Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#instantiation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="SimpleLLaMA Documentation" class="md-header__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SimpleLLaMA Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Training Loop
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="SimpleLLaMA Documentation" class="md-nav__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SimpleLLaMA Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Pretraining
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset Preparation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tokenization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Tokenization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/algorithms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Usage
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Model Architecture
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Model Architecture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/feedforward/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FeedForward
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/decoder_block/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Layer Block
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/end_to_end/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    End To End
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" checked>
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Beginner)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Beginner)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model_config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Configurations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dataset_and_batching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset and Batching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../scheduler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Training Loop
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Training Loop
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#instantiation" class="md-nav__link">
    <span class="md-ellipsis">
      Instantiation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-model-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      The Model Forward Pass
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      The Loss Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-backward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      The Backward Pass
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-optimizer-step" class="md-nav__link">
    <span class="md-ellipsis">
      The Optimizer Step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-minimal-training-loop" class="md-nav__link">
    <span class="md-ellipsis">
      A Minimal Training Loop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation and Monitoring
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bringing-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Bringing It All Together
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Advanced)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Advanced)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/recap/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basics Recap
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/optimizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimizer Details
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/loss_function/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cross Entropy Loss
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/gradient_accumulation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gradient Accumulation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/ddp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Data Parallel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/throughput_optimizations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Throughput Optimizations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/ckpt_and_eval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Checkpointing and Evaluations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_advanced/final_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Final Walkthrough
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Supervised Fine-Tuning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/prompt_formatting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Formatting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/finetuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning Process
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            RLHF
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rlhf/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rlhf/methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Misc
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Misc
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/benchmarking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmarking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Custom Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Custom Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/pretraining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#instantiation" class="md-nav__link">
    <span class="md-ellipsis">
      Instantiation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-model-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      The Model Forward Pass
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      The Loss Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-backward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      The Backward Pass
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-optimizer-step" class="md-nav__link">
    <span class="md-ellipsis">
      The Optimizer Step
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-minimal-training-loop" class="md-nav__link">
    <span class="md-ellipsis">
      A Minimal Training Loop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation and Monitoring
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bringing-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Bringing It All Together
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Training Loop</h1>

<p>Training a large language model (LLM), even a small‑scale one like in this project, comes down to a repeated cycle: take a batch of data, run it through the model, calculate how wrong the predictions are, push the error backwards to update the weights, and repeat. This cycle is what we call the <strong>training loop</strong>. This section will discuss in detail through the core parts of the loop. </p>
<hr />
<h3 id="instantiation">Instantiation</h3>
<p>Before we can train the model, we need to set up all the core building blocks. Once everything is in place, the training loop itself becomes a straightforward repetition of forward pass, loss calculation, backward pass, and optimization.</p>
<hr />
<p><strong>1. Configuration Object</strong></p>
<p>The first thing we need is a configuration object that stores all of our hyperparameters. Instead of scattering values like batch size, learning rate, and number of layers across different files, it’s cleaner to place them in a single file/class. This makes the code easier to manage, debug, and extend.  </p>
<p>In this project, it will be the <code>TrainingConfig</code> class, located within the <code>simple_llama/pretraining/config.py</code> file</p>
<div class="highlight"><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">:</span>
    <span class="c1"># === Paths and Dataset ===</span>
    <span class="n">dataset_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;short&quot;</span><span class="p">)</span>       <span class="c1"># Path to tokenized training data</span>
    <span class="n">tokenizer_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;bpe_8k.json&quot;</span><span class="p">)</span>          <span class="c1"># Path to tokenizer model</span>
    <span class="n">ckpt_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;pretraining&quot;</span><span class="p">,</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">)</span>   <span class="c1"># Directory to store checkpoints</span>
    <span class="n">log_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;pretraining&quot;</span><span class="p">,</span> <span class="s2">&quot;training_progress.txt&quot;</span><span class="p">)</span>  <span class="c1"># File to log training progress</span>

    <span class="c1"># === Batch &amp; Sequence ===</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>             <span class="c1"># Minibatch size</span>
    <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>         <span class="c1"># Maximum sequence length per sample</span>
    <span class="n">tokens_per_update</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">19</span>  <span class="c1"># ~512K tokens per optimizer update</span>

    <span class="c1"># === Model Architecture ===</span>
    <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>               <span class="c1"># Embedding dimension</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>                <span class="c1"># Number of attention heads</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span>               <span class="c1"># Number of transformer layers</span>
    <span class="n">multiple_of</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>           <span class="c1"># Feedforward dim multiple for efficient matmul</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span>                <span class="c1"># Epsilon value to prevent div-by-zero in normalization layers</span>
    <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span>              <span class="c1"># Theta for RoPE rotation frequency</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>             <span class="c1"># Dropout rate; typically 0.0 for pretraining</span>

    <span class="o">...</span>  <span class="c1"># And many more</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">TrainingConfig</span><span class="p">()</span>
</code></pre></div>
<p>This way, if we want to adjust hyperparameters like <code>n_heads</code> or experiment with a different <code>max_lr</code>, it’s a single line change.</p>
<hr />
<p><strong>2. Dataset Loader</strong></p>
<p>Next, instantiate a dataset loader object that is defined, passing in hyperparameters as needed, extracted from the configuration object:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">simple_llama.pretraining.dataset_loader</span> <span class="kn">import</span> <span class="n">DatasetLoader</span>

<span class="n">dataset_loader</span> <span class="o">=</span> <span class="n">DatasetLoader</span><span class="p">(</span><span class="n">batch</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span> 
                               <span class="n">process_rank</span><span class="o">=</span><span class="n">ddp_rank</span><span class="p">,</span> <span class="n">num_processes</span><span class="o">=</span><span class="n">ddp_world_size</span><span class="p">,</span> 
                               <span class="n">dataset_dir</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>
<hr />
<p><strong>3. The Model</strong></p>
<p>Now comes the centerpiece: the transformer model itself. In this project, we’ve implemented <code>LLaMaTransformer</code>, which includes embeddings, attention blocks, feedforward layers, normalization, and output projection.</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">LLaMaTransformer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Here:  </p>
<ul>
<li><code>config</code> gives the model hyperparameters.  </li>
<li><code>tokenizer</code> provides the vocabulary size.  </li>
<li><code>device="cuda"</code> places the model on GPU.</li>
</ul>
<p>Initially, the model’s parameters are random. Training gradually adjusts them so that token predictions become more accurate.</p>
<hr />
<p><strong>4. The Loss Function</strong></p>
<p>Next, we define how the model will be judged. For language modeling, the go-to choice is <strong>cross-entropy loss</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</code></pre></div>
<p>Cross-entropy measures how “surprised” the model is by the correct next token.  </p>
<ul>
<li>If the model assigns high probability → low loss.  </li>
<li>If it assigns low probability → high loss.</li>
</ul>
<hr />
<p><strong>5. The Optimizer</strong></p>
<p>Finally, we define the optimizer. We use <strong>AdamW</strong>, which is the de facto standard for transformers because it combines Adam’s adaptive gradient updates with weight decay for stability.</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">max_lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_args</span><span class="p">)</span>
</code></pre></div>
<p>This way, every training step will use the optimizer to update the model parameters and the scheduler to adjust the learning rate.</p>
<p>(More about AdamW optimizer will be covered in the Advanced section of the documentation)</p>
<hr />
<p><strong>6. Learning Rate Scheduler</strong></p>
<p>Finally, the <strong>learning rate scheduler</strong>. The scheduler controls how the learning rate evolves over time, which is important for training.</p>
<p>We’re using the custom <code>Scheduler</code> class implemented earlier, which supports <em>linear decay</em>, <em>cosine decay</em>, or just a <em>constant</em> learning rate.</p>
<div class="highlight"><pre><span></span><code><span class="n">scheduler</span> <span class="o">=</span> <span class="n">Scheduler</span><span class="p">(</span><span class="n">torch_optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                      <span class="n">schedule</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>
                      <span class="n">training_steps</span><span class="o">=</span><span class="n">optimization_steps</span><span class="p">,</span>
                      <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_iterations</span><span class="p">,</span>
                      <span class="n">max_lr</span><span class="o">=</span><span class="n">max_lr</span><span class="p">,</span>
                      <span class="n">min_lr</span><span class="o">=</span><span class="n">min_lr</span><span class="p">)</span>
</code></pre></div>
<hr />
<p>At this point, we’ve instantiated:  </p>
<ul>
<li>Configuration object</li>
<li>Dataset loader</li>
<li>Transformer model</li>
<li>Loss function</li>
<li>Optimizer</li>
<li>Learning rate scheduler</li>
</ul>
<p>All the main components are ready. The next step is to actually run them inside the training loop.</p>
<hr />
<h3 id="the-model-forward-pass">The Model Forward Pass</h3>
<p>We begin with a batch of input tokens, grabbed from the DatasetLoader object via the <code>get_batch()</code> method. Each integer corresponds to a token ID from our vocabulary.  </p>
<p>Let’s say our batch size is <code>B = 4</code>, and the sequence length we train on is <code>T = 16</code>. The shape of a batch from the dataset loader would look like:</p>
<div class="highlight"><pre><span></span><code>x.shape = (B, T) = (4, 16)
</code></pre></div>
<p>So <code>x</code> is a 2D tensor of integers. Each row is one training sequence, and each entry is a token ID.  </p>
<p>When we feed this into the model:</p>
<div class="highlight"><pre><span></span><code><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<p>the transformer runs all of its layers: embedding lookup, multiple decoder blocks, attention, feedforward layers, normalization, and finally a linear projection back to vocabulary size.  </p>
<p>The key here is the shape change:  </p>
<ul>
<li>Input: <code>(B, T)</code> — integers.  </li>
<li>Output: <code>(B, T, C)</code> — floats, where <code>C</code> is the vocabulary size.  </li>
</ul>
<p>Why <code>(B, T, C)</code>? Because for every position in every sequence, the model outputs a vector of size <code>C</code>, which are the raw unnormalized scores for each possible token in the vocabulary. These are called <strong>logits</strong>.</p>
<hr />
<h3 id="the-loss-function">The Loss Function</h3>
<p>Once we have logits, we want to measure how good the predictions are. That is the role of the <strong>loss function</strong>. For language modeling, the standard is <strong>cross entropy loss</strong>.</p>
<p>The goal is simple: the model is asked to predict the next token in the sequence. If the input sequence is <code>[The, cat, sat, on, the]</code>, the correct output is <code>[cat, sat, on, the, mat]</code>. Each token should map to the next token.  </p>
<p>Cross entropy measures how “surprised” the model is by the correct answer. If the model already places high probability on the true next token, the loss is small. If the model thought another token was much more likely, the loss is large.  </p>
<p>In PyTorch, we use:</p>
<div class="highlight"><pre><span></span><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</code></pre></div>
<p>However, <code>CrossEntropyLoss</code> expects inputs of shape <code>(N, C)</code> where <code>N</code> is the number of items and <code>C</code> is the number of classes, and targets of shape <code>(N,)</code>.  </p>
<p>Our logits are <code>(B, T, C)</code> and our targets are <code>(B, T)</code>. So we flatten them:</p>
<div class="highlight"><pre><span></span><code><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">targets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>
<p>This reshapes:</p>
<ul>
<li><code>logits.view(-1, C)</code> → <code>(B*T, C)</code>  </li>
<li><code>targets.view(-1)</code> → <code>(B*T,)</code>  </li>
</ul>
<p>Effectively, we treat the whole batch as one big list of token predictions.</p>
<p>Mathematically, cross entropy loss is:</p>
<div class="arithmatex">\[
L = -\frac{1}{N} \sum_{i=1}^{N} \log \big( \text{softmax}(\text{logits})_{i, y_i} \big)
\]</div>
<p>where <code>y_i</code> is the true class (the correct next token).  </p>
<p>More details will be covered in the Advanced Training section</p>
<hr />
<h3 id="the-backward-pass">The Backward Pass</h3>
<p>Now comes the critical part: telling the model how wrong it was. This is done with:</p>
<div class="highlight"><pre><span></span><code><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div>
<p>This triggers PyTorch’s <strong>autograd engine</strong>, which walks backwards through the computational graph.  </p>
<p>Every tensor operation in PyTorch (matrix multiplies, nonlinearities, normalizations) records how it was computed. During <code>.backward()</code>, PyTorch applies the chain rule of calculus to compute gradients of the loss with respect to every parameter in the model.</p>
<p>So, if our model has parameters θ = {W1, W2, …}, then after <code>loss.backward()</code> we now have stored gradients ∂L/∂W for each parameter. These gradients are stored in each parameter tensor within the <code>.grad</code> attribute, which is a matrix of gradients the shape as the weight matrix. </p>
<p>These gradients tell us: “If you nudge this weight slightly, the loss would go up/down this much.” </p>
<p>Effectively, they are the signals that will guide weight updates.</p>
<hr />
<h3 id="the-optimizer-step">The Optimizer Step</h3>
<p>With gradients calculated, we actually update the weights. This is the job of the optimizer.  </p>
<p>In this project, we use <strong>AdamW</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div>
<p>AdamW is a variant of stochastic gradient descent that adapts learning rates per parameter and includes proper weight decay. It’s widely used in training transformers.</p>
<p>The update cycle is:</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># reset gradients</span>

<span class="c1"># Between these two steps, perform forward pass, calculate loss, back propagation</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>       <span class="c1"># update parameters using gradients</span>
</code></pre></div>
<p>Zeroing the gradients is crucial because PyTorch accumulates gradients by default. If we didn’t zero them, gradients from multiple steps would pile up, leading to artificially large parameter updates, quickly destabilizing the model.</p>
<p>So the full cycle is:</p>
<ol>
<li>Zero gradients → prepare for next step.</li>
<li>Forward pass → compute logits and loss.</li>
<li>Loss calculation → use criterion to calculate loss.</li>
<li>Backward pass → compute gradients.  </li>
<li>Optimizer step → update weights.  </li>
</ol>
<hr />
<h3 id="a-minimal-training-loop">A Minimal Training Loop</h3>
<p>Putting everything together:</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="c1"># Get a batch</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset_loader</span><span class="o">.</span><span class="n">get_batch</span><span class="p">()</span>   <span class="c1"># x: (B, T), y: (B, T)</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                   <span class="c1"># (B, T, C)</span>

    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Backward pass</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Granted the actual implementation in <code>simple_llama.pretraining.train</code> file is much more complex, however this is the backbone of training. Every sophisticated training pipeline — from GPT to LLaMA — reduces to these few lines.  </p>
<hr />
<h3 id="evaluation-and-monitoring">Evaluation and Monitoring</h3>
<p>Training is only half the story. We need to know if the model is improving. The simplest way is to track the <strong>training loss</strong>. Over time, as the model sees more data, loss should decrease, which means the model is getting progressively better at predicting the next token, given an input sequence of tokens.</p>
<p>At the very beginning, before the model has learned anything meaningful, predictions are essentially random. In this case, the expected loss can be approximated by the natural logarithm of the vocabulary size, since each token is equally likely under a uniform distribution.  </p>
<p>For our project, the vocabulary size is 8192. So if the predictions were truly uniform, the expected initial loss would be:</p>
<div class="highlight"><pre><span></span><code>ln(8192) ≈ 9.01
</code></pre></div>
<p>However, in practice, most parameters in the model (such as linear layers) are initialized from Gaussian distributions using Kaiming or Xavier initialization. This breaks the perfect uniformity and introduces biases. As a result, the observed loss at the very start of training would likely be higher than the theoretical value — for example, around 9.2 or 9.3 instead of exactly 9.01.  </p>
<hr />
<p><strong>Why the log of vocab size?</strong></p>
<p>Cross-Entropy Loss (CEL) is essentially a Negative Log Likelihood (NLL) loss. For a dataset of size <span class="arithmatex">\(N\)</span> with true labels <span class="arithmatex">\(y_i\)</span> and predicted probabilities <span class="arithmatex">\(p(y_i)\)</span>:</p>
<div class="arithmatex">\[
CEL = -\frac{1}{N} \sum_{i=1}^{N} \log p(y_i)
\]</div>
<p>For a single example where the true class is <span class="arithmatex">\(c\)</span>:</p>
<div class="arithmatex">\[
CEL = -\log p(c)
\]</div>
<p>If the model predicts uniformly over all <span class="arithmatex">\(V\)</span> classes, then <span class="arithmatex">\(p(c) = \frac{1}{V}\)</span>. Plugging this in:</p>
<div class="arithmatex">\[
CEL = -\log \left(\frac{1}{V}\right) = \log V
\]</div>
<p>So under uniform predictions, the expected loss equals the log of vocabulary size.</p>
<p><strong>Example:</strong>  </p>
<ul>
<li><span class="arithmatex">\(V = 8192\)</span>  </li>
<li><span class="arithmatex">\(CEL = \log(8192)\)</span>  </li>
<li><span class="arithmatex">\(CEL \approx 9.01\)</span>  </li>
</ul>
<p>This is the theoretical baseline for random guessing. In practice, initialization bias may push it to ~9.3 at step 0.</p>
<hr />
<p><strong>Training Dynamics</strong></p>
<p>As training continues, the loss should decrease steadily. For instance, a drop from ~9.3 to ~3 means the model is learning meaningful statistical patterns in the data. Lower loss translates directly into the model being less “surprised” when predicting the next token.</p>
<p>Think of it this way:  </p>
<ul>
<li>At loss ≈ 9, the model is basically clueless, assigning ~1/8192 probability to every token.  </li>
<li>At loss ≈ 3, the model assigns ~1/20 probability to the correct token on average.  </li>
<li>At loss ≈ 1, the model is strongly confident, giving ~1/3 probability to the correct token.</li>
</ul>
<p>Even at a loss of around 3.0, the model assigns roughly 5% probability to the observed “correct” token on average. That may sound low if one interpret it as "The model only have a 5% chance of choosing the correct token, for a given sequence"
However that is a bit misleading. In English (or just about all languages) there is natural entropy to it. Vast majority of the time, there are multiple valid answers to a given sequence.  </p>
<p>Taking the previous example, we give the model: <code>[The, cat, sat, on, the]</code> and want it to predict the next token. Our true label should be the token corresponding to the word <code>mat</code> however, in general, there isn't just a single right-wrong answer. 
Words like <code>floor</code>, <code>ground</code>, <code>couch</code> and such are also completely valid. Hence a probability of 1/20 chance choosing the 'correct' token isn't as bad a it may numerically seem to be. </p>
<hr />
<p><strong>Validation?</strong></p>
<p>It’s also common to periodically <strong>evaluate</strong> on a held-out validation set. This prevents overfitting, since training loss always decreases but validation loss may rise if the model memorizes.  </p>
<p>However, in this project, no validation set is used. Why? Because the dataset (50B tokens gathered from FineWebEdu) is mostly unique. Training is done in a single epoch — the model will only see each token sequence once. Under this regime, overfitting is theoretically impossible.  </p>
<p>In fact, if a model with ~1B parameters <em>were</em> able to fully overfit on 50B unique tokens, that would be remarkable — it would essentially mean the model is acting as a form of near-lossless compression of the dataset. From that perspective, it might even be considered desirable. But in practice, that's nearly impossible. Here we will only go through one pass using the 50B tokens, simply track training loss as the main signal of progress.</p>
<hr />
<p><strong>A Tiny Generation Example</strong></p>
<p>Even early in training, it’s fun (and useful) to test the model by generating text.  </p>
<p>We take a prompt, tokenize it, and call a generate function:</p>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Once upon a time&quot;</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span>
</code></pre></div>
<p>At the start, the output will be nonsense — the model has learned almost nothing. But as loss decreases, generated samples gradually improve. They start forming grammatical sentences, then coherent paragraphs.</p>
<p>This qualitative check is as important as loss curves, because it directly shows what the model is learning.</p>
<p>Here is an example output log print when training a small model:</p>
<div class="highlight"><pre><span></span><code>Step: 256 steps   |   Training Progress: 0.02%   |   Training Loss: 8.0160   |   Perplexity: 3029.09   |   Learning Rate: 0.00008   |   Norm: 1.0915   |   Tokens Processed: 8M (8M)   |   tok/s: 157961   |   Time: 53s
----------------
Step: 512 steps   |   Training Progress: 0.04%   |   Training Loss: 7.0701   |   Perplexity: 1176.23   |   Learning Rate: 0.00015   |   Norm: 0.2549   |   Tokens Processed: 16M (16M)   |   tok/s: 142851   |   Time: 58s
----------------
----------------
Step: 768 steps   |   Training Progress: 0.06%   |   Training Loss: 6.5323   |   Perplexity: 686.96   |   Learning Rate: 0.00023   |   Norm: 0.1649   |   Tokens Processed: 25M (25M)   |   tok/s: 187962   |   Time: 44s
----------------
----------------
Step: 1024 steps   |   Training Progress: 0.07%   |   Training Loss: 5.8950   |   Perplexity: 363.23   |   Learning Rate: 0.00031   |   Norm: 0.2274   |   Tokens Processed: 33M (33M)   |   tok/s: 187884   |   Time: 44s
----------------
----------------
Step: 1280 steps   |   Training Progress: 0.09%   |   Training Loss: 5.6318   |   Perplexity: 279.16   |   Learning Rate: 0.00038   |   Norm: 0.2636   |   Tokens Processed: 41M (41M)   |   tok/s: 187881   |   Time: 44s
----------------
----------------
Step: 1536 steps   |   Training Progress: 0.11%   |   Training Loss: 5.2796   |   Perplexity: 196.28   |   Learning Rate: 0.00046   |   Norm: 0.2596   |   Tokens Processed: 50M (50M)   |   tok/s: 187936   |   Time: 44s
----------------
----------------
Step: 1792 steps   |   Training Progress: 0.13%   |   Training Loss: 5.1112   |   Perplexity: 165.87   |   Learning Rate: 0.00054   |   Norm: 0.2780   |   Tokens Processed: 58M (58M)   |   tok/s: 187930   |   Time: 44s
----------------
----------------
Step: 2048 steps   |   Training Progress: 0.15%   |   Training Loss: 5.0083   |   Perplexity: 149.65   |   Learning Rate: 0.00060   |   Norm: 0.4782   |   Tokens Processed: 67M (67M)   |   tok/s: 188105   |   Time: 44s
----------------
----------------
Step: 2304 steps   |   Training Progress: 0.17%   |   Training Loss: 4.7851   |   Perplexity: 119.71   |   Learning Rate: 0.00060   |   Norm: 0.3195   |   Tokens Processed: 75M (75M)   |   tok/s: 188087   |   Time: 44s
----------------
----------------
Step: 2560 steps   |   Training Progress: 0.19%   |   Training Loss: 4.6413   |   Perplexity: 103.68   |   Learning Rate: 0.00060   |   Norm: 0.4755   |   Tokens Processed: 83M (83M)   |   tok/s: 187940   |   Time: 44s
----------------
----------------
Step: 2816 steps   |   Training Progress: 0.21%   |   Training Loss: 4.6120   |   Perplexity: 100.68   |   Learning Rate: 0.00060   |   Norm: 0.2608   |   Tokens Processed: 92M (92M)   |   tok/s: 187844   |   Time: 44s
----------------
----------------
Step: 3072 steps   |   Training Progress: 0.22%   |   Training Loss: 4.4724   |   Perplexity: 87.57   |   Learning Rate: 0.00060   |   Norm: 0.6141   |   Tokens Processed: 100M (100M)   |   tok/s: 187720   |   Time: 44s
</code></pre></div>
<p>Focusing on the training loss curve, we can see that it declines rapidly before slowly starts to plateau a bit. 
Perplexity and Norm metric will be covered in the Advanced Guide section</p>
<p>Here is an example generation output by the model at the very start, after just 256 steps:</p>
<div class="highlight"><pre><span></span><code>&lt;SOS&gt; Classify the sentiment as &quot;Positive&quot; or &quot;Negative&quot;:
Input: I absolutely loved the movie, it was fantastic!
Sentiment: Positive

Input: The service was terrible and I won&#39;t come back.
Sentiment: Negative

Input: This book was boring and hard to finish.
Sentiment: Negative

Input: The app keeps crashing and it&#39;s really frustrating.
Sentiment: Negative

Input: I&#39;m impressed by how fast and easy this process was.
Sentiment: Positive

Input: The food was delicious and the atmosphere was wonderful.
Sentiment: told preventing capac appoint aboutterfacesstandingomeotheods char AcuseAAes applications governments Theseind energy
be electroesietThese Discussteries contains spendasma critaries treatels 190 facilitaintically majority might 13 calculate honey Colle robot Orony soils Fin dest confirmed7 financialcom. highest Denheastoet rec branch
</code></pre></div>
<p>In the above example, the provided text sequence ended at:</p>
<div class="highlight"><pre><span></span><code>Input: The food was delicious and the atmosphere was wonderful.
Sentiment: 
</code></pre></div>
<p>Where after the 'Sentiment: ' part, we can see that the output is garbled, as we would expect. 
However, at the next evaluation, the following resulted from the model:</p>
<div class="highlight"><pre><span></span><code>&lt;SOS&gt; Write a polite email reply:
Input: Hi, can you send me the report by tomorrow? Thanks.
Reply: Hi, sure thing! I&#39;ll send the report to you by tomorrow. Let me know if you need anything else.

Input: Hi, just checking if you&#39;re available for a meeting this Friday.
Reply: Hi, thanks for reaching out. I&#39;m available on Friday - what time works best for you?

Input: Hi, could you help me with the project deadline?
Reply: Hi, of course. Let me know what you need help with, and I&#39;ll do my best to assist.

Input: Hi, do you have the updated slides for the presentation?
Reply: Hi, yes - I&#39;ll send over the updated slides shortly. Let me know if you&#39;d like me to walk you through them.

Input: Hi, do you have time to meet with the manager later today?
Reply: How do I have no force?
Learn&#39;t have an error in good, and I am looking for a call while you should check.
Use students learn about three different environments, including:
Reportness (such as CCM)
You can use a sample is
Click
</code></pre></div>
<p>Here, the provided text sequence is different (to provide variations during evaluations)</p>
<p>In the above example, the provided text sequence ended at: </p>
<div class="highlight"><pre><span></span><code>Input: Hi, do you have time to meet with the manager later today?
Reply: 
</code></pre></div>
<p>The model is clearly learning valid words and starts to piece them together in a somewhat structured way.</p>
<hr />
<h3 id="bringing-it-all-together">Bringing It All Together</h3>
<p>To summarize, each training step does:</p>
<ol>
<li>Take a batch <code>(B, T)</code> of token IDs.  </li>
<li>Run through model → get logits <code>(B, T, C)</code>.  </li>
<li>Compute cross entropy loss with targets <code>(B, T)</code>.  </li>
<li>Backpropagate loss → compute gradients.  </li>
<li>Optimizer updates weights.  </li>
<li>Zero gradients.  </li>
</ol>
<p>This loop runs millions of times. At small scale, it might be just tens of thousands of steps. At large scale (GPT‑3, LLaMA), training can take trillions of tokens.</p>
<p>But the essence is always the same. The beauty of the transformer is that all of this complexity — embeddings, attention, normalization, feedforward layers — reduces down to the training loop you’ve just seen.</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.math"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>