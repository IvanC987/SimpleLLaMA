
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../ddp/">
      
      
        <link rel="next" href="../ckpt_and_eval/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Throughput Optimizations - SimpleLLaMA Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#throughput-optimizations" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="SimpleLLaMA Documentation" class="md-header__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SimpleLLaMA Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Throughput Optimizations
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="SimpleLLaMA Documentation" class="md-nav__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SimpleLLaMA Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Pretraining
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset Preparation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tokenization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Tokenization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/algorithms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Usage
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Model Architecture
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Model Architecture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/feedforward/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FeedForward
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/decoder_block/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Layer Block
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture/end_to_end/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    End To End
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Beginner)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Beginner)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/model_config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Configurations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/dataset_and_batching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset and Batching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/scheduler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../training_beginner/training_loop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Loop
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" checked>
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Advanced)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Advanced)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../recap/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basics Recap
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimizer Details
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../loss_function/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cross Entropy Loss
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gradient_accumulation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gradient Accumulation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ddp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Data Parallel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Throughput Optimizations
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Throughput Optimizations
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-bf16-vs-fp32fp16-the-precision-trade-off" class="md-nav__link">
    <span class="md-ellipsis">
      1. BF16 vs FP32/FP16: The Precision Trade-off
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. BF16 vs FP32/FP16: The Precision Trade-off">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#understanding-numerical-precision" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding Numerical Precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-bf16-wins-for-llm-training" class="md-nav__link">
    <span class="md-ellipsis">
      Why BF16 Wins for LLM Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-automatic-mixed-precision-amp-with-autocast" class="md-nav__link">
    <span class="md-ellipsis">
      2. Automatic Mixed Precision (AMP) with Autocast
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Automatic Mixed Precision (AMP) with Autocast">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-autocast-works" class="md-nav__link">
    <span class="md-ellipsis">
      How Autocast Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-torchcompile-graph-level-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      3. Torch.Compile: Graph-Level Optimizations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Torch.Compile: Graph-Level Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#from-eager-mode-to-graph-mode" class="md-nav__link">
    <span class="md-ellipsis">
      From Eager Mode to Graph Mode
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-compilation-optimizes" class="md-nav__link">
    <span class="md-ellipsis">
      What Compilation Optimizes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-world-example-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Real-World Example: Attention Mechanism
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-fused-optimizer-kernels" class="md-nav__link">
    <span class="md-ellipsis">
      4. Fused Optimizer Kernels
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      5. FlashAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. FlashAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Standard Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#flashattention-approach" class="md-nav__link">
    <span class="md-ellipsis">
      FlashAttention Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-usage" class="md-nav__link">
    <span class="md-ellipsis">
      Example Usage
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ckpt_and_eval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Checkpointing and Evaluations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../final_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Final Walkthrough
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Supervised Fine-Tuning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/prompt_formatting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Formatting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../sft/finetuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning Process
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            RLHF
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rlhf/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../rlhf/methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Misc
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Misc
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/benchmarking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmarking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../misc/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Custom Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Custom Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/pretraining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../custom_training/rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-bf16-vs-fp32fp16-the-precision-trade-off" class="md-nav__link">
    <span class="md-ellipsis">
      1. BF16 vs FP32/FP16: The Precision Trade-off
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. BF16 vs FP32/FP16: The Precision Trade-off">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#understanding-numerical-precision" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding Numerical Precision
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-bf16-wins-for-llm-training" class="md-nav__link">
    <span class="md-ellipsis">
      Why BF16 Wins for LLM Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-automatic-mixed-precision-amp-with-autocast" class="md-nav__link">
    <span class="md-ellipsis">
      2. Automatic Mixed Precision (AMP) with Autocast
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Automatic Mixed Precision (AMP) with Autocast">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-autocast-works" class="md-nav__link">
    <span class="md-ellipsis">
      How Autocast Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-torchcompile-graph-level-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      3. Torch.Compile: Graph-Level Optimizations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Torch.Compile: Graph-Level Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#from-eager-mode-to-graph-mode" class="md-nav__link">
    <span class="md-ellipsis">
      From Eager Mode to Graph Mode
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-compilation-optimizes" class="md-nav__link">
    <span class="md-ellipsis">
      What Compilation Optimizes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#real-world-example-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Real-World Example: Attention Mechanism
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-fused-optimizer-kernels" class="md-nav__link">
    <span class="md-ellipsis">
      4. Fused Optimizer Kernels
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      5. FlashAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. FlashAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Standard Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#flashattention-approach" class="md-nav__link">
    <span class="md-ellipsis">
      FlashAttention Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-usage" class="md-nav__link">
    <span class="md-ellipsis">
      Example Usage
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="throughput-optimizations">Throughput Optimizations</h1>
<p>Achieving high training throughput is crucial for practical LLM development, and this section outlines key optimizations in the training script that deliver noticeable speedups.</p>
<hr />
<h2 id="1-bf16-vs-fp32fp16-the-precision-trade-off">1. BF16 vs FP32/FP16: The Precision Trade-off</h2>
<h3 id="understanding-numerical-precision">Understanding Numerical Precision</h3>
<ul>
<li>
<p><strong>FP32</strong>: 32-bit floating point (standard float)  </p>
<ul>
<li>Range: ±1.7×10³⁸, Precision: ~7 decimal digits  </li>
<li>High precision, large memory footprint  </li>
</ul>
</li>
<li>
<p><strong>FP16</strong>: 16-bit floating point (half precision)  </p>
<ul>
<li>Range: ±65,504, Precision: ~4 decimal digits  </li>
<li>Small memory, but limited range can cause overflow/underflow  </li>
</ul>
</li>
<li>
<p><strong>BF16</strong>: Brain Floating Point (Google's solution)  </p>
<ul>
<li>Range: ±3.4×10³⁸ (same exponent as FP32, much larger than FP16)  </li>
<li>Precision: ~3 decimal digits (7 mantissa bits, less than FP16)</li>
<li><strong>Ideal for deep learning</strong>: preserves range, sacrifices precision  </li>
</ul>
</li>
</ul>
<h3 id="why-bf16-wins-for-llm-training">Why BF16 Wins for LLM Training</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># BF16 has the dynamic range of FP32 with the memory footprint of FP16</span>
<span class="n">fp32_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 4MB</span>
<span class="n">bf16_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>  <span class="c1"># 2MB</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;FP32 range: ~10^38, BF16 range: ~10^38 (same!)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;FP16 range: ~10^4 (much smaller - risk of overflow)&quot;</span><span class="p">)</span>
</code></pre></div>
<p>The key is that LLM training is more sensitive to range than precision. Gradient values can span many orders of magnitude, making BF16's preserved range crucial.</p>
<p>Do note that although this would reduce memory usage substantially, but would not directly halve the memory usage, a few layers/activations in the model are kept in FP32 due to precision sensitivity, along with optimizer state dicts which stays in FP32. </p>
<hr />
<h2 id="2-automatic-mixed-precision-amp-with-autocast">2. Automatic Mixed Precision (AMP) with Autocast</h2>
<p>As mentioned, not all operations benefit from lower precision. Some need FP32 for numerical stability:</p>
<ul>
<li><strong>BF16-friendly:</strong> Heavy matrix multiplications, elementwise operations, gradients</li>
<li><strong>FP32-required:</strong> Reductions, loss computation, certain activations that requires numeric precision  </li>
</ul>
<p>PyTorch handles this by providing <code>torch.autocast</code> to automatically manage precision:</p>
<div class="highlight"><pre><span></span><code><span class="n">use_amp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">in</span> <span class="n">device</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
                   <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">if</span> <span class="n">use_amp</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="c1"># Everything in this context uses mixed precision</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model_handle</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Most ops in BF16</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">))</span>  <span class="c1"># Loss in FP32</span>
</code></pre></div>
<h3 id="how-autocast-works">How Autocast Works</h3>
<ul>
<li><strong>Operator-Level Precision Rules:</strong> PyTorch has built-in rules for each operation type  </li>
<li><strong>Automatic Casting:</strong> Inputs are cast to appropriate precision  </li>
<li><strong>Output Casting:</strong> Results are cast back to expected types  </li>
</ul>
<p>Overall, this results not only in memory usage reduction, operations in BF16 is much faster compared to in FP32 on modern tensor cores. </p>
<hr />
<h2 id="3-torchcompile-graph-level-optimizations">3. Torch.Compile: Graph-Level Optimizations</h2>
<h3 id="from-eager-mode-to-graph-mode">From Eager Mode to Graph Mode</h3>
<p>PyTorch normally runs in eager mode: each operation executes immediately. This is flexible but inefficient due to Python overhead.</p>
<p><code>torch.compile</code> converts your model to a graph that can be optimized:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Before compilation (eager mode)</span>
<span class="k">if</span> <span class="n">enable_compilation</span><span class="p">:</span>
    <span class="n">model_handle</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># After compilation - same API, much faster execution</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">model_handle</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Now runs as optimized graph</span>
</code></pre></div>
<h3 id="what-compilation-optimizes">What Compilation Optimizes</h3>
<ul>
<li>Kernel Fusion: Combine multiple operations into single kernels  </li>
<li>Memory Layout Optimization: Optimize tensor memory access patterns  </li>
<li>Dead Code Elimination: Remove unnecessary operations  </li>
<li>Constant Propagation: Precompute constant expressions  </li>
</ul>
<h3 id="real-world-example-attention-mechanism">Real-World Example: Attention Mechanism</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># Eager mode: many small operations</span>
<span class="k">def</span> <span class="nf">attention_eager</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># Separate kernel</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>        <span class="c1"># Separate kernel  </span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>           <span class="c1"># Separate kernel</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>                 <span class="c1"># Separate kernel</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<p>Compiled: potentially fused into fewer kernels.  </p>
<p>The compiler can merge these operations for better memory locality.</p>
<hr />
<h2 id="4-fused-optimizer-kernels">4. Fused Optimizer Kernels</h2>
<p>Similar to how <code>torch.compile</code> offers kernel fusion operation in the model, optimizers can also provide fusion operations to more efficiently compute and update model parameters. </p>
<p>In a more naive implementation pytorch may launch a lot of small kernels which would be relatively inefficient with overhead costs, coming from many small GPU operations.</p>
<p>Here we can inspect if the optimizer supports fused operations, and pass that kwarg during optimizer instantiation to combine multiple operations into single kernels:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Check for fused optimizer support</span>
<span class="n">fused_available</span> <span class="o">=</span> <span class="s1">&#39;fused&#39;</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">)</span><span class="o">.</span><span class="n">parameters</span>
<span class="n">use_fused</span> <span class="o">=</span> <span class="n">fused_available</span> <span class="ow">and</span> <span class="p">(</span><span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">or</span> <span class="n">ddp</span><span class="p">)</span>
<span class="n">extra_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">use_fused</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">()</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
    <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
    <span class="n">lr</span><span class="o">=</span><span class="n">max_lr</span><span class="p">,</span> 
    <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">),</span> 
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> 
    <span class="o">**</span><span class="n">extra_args</span>  <span class="c1"># Uses fused kernels if available</span>
<span class="p">)</span>
</code></pre></div>
<p>In general, there would be some speed up optimizations, though that would be dependent upon the optimizer and operations within the model. 
It reduces memory bandwidth usage and is especially beneficial for models with many small parameters  . 
The speedup varies by hardware and model size, but it’s typically a “free” optimization worth enabling when available.</p>
<hr />
<h2 id="5-flashattention">5. FlashAttention</h2>
<p>One of the biggest computational and memory bottlenecks in Transformers comes from the <strong>attention mechanism</strong>.<br />
Standard attention computes a large score matrix of shape <code>(B, n_heads, T, T)</code>. </p>
<p>Imagine we train a language model using a sequence length, <code>T</code>, with a moderate value like 32k, batch size of 4, and 32 heads. A single matrix of that shape using BF16 would require over 250GB of memory alone!
Granted, that's the most naive implementation, but for long sequences, this quadratic cost in both time and memory quickly dominates training, due to it's quadratic nature.</p>
<p><strong>FlashAttention</strong> is a memory-efficient attention algorithm that reorders the way attention is computed so that the large intermediate matrix never needs to be materialized in GPU memory. Instead, it streams blocks of the computation through high-bandwidth GPU SRAM, drastically reducing memory usage and speeding up execution. Overall computation requirement remains quadratic, however the memory requirement is now linear with respect to sequence length. </p>
<h3 id="standard-attention">Standard Attention</h3>
<ol>
<li>Compute scores: <code>QK^T / sqrt(d_head)</code> → shape <code>(B, n_heads, T, T)</code>  </li>
<li>Apply softmax → still <code>(B, n_heads, T, T)</code>  </li>
<li>Multiply by <code>V</code>  </li>
</ol>
<p>Both the score matrix and the softmax output is stored for backpropagation, which can be quite large, depending on the sequence length.</p>
<h3 id="flashattention-approach">FlashAttention Approach</h3>
<ul>
<li>The key trick is to compute attention <strong>block by block</strong> directly in GPU SRAM, never materializing the full <code>(T, T)</code> matrix.  </li>
<li>Uses a numerically stable online softmax to handle blocks sequentially.  </li>
<li>Backward pass recomputes pieces instead of storing them, further saving memory.</li>
</ul>
<p>The benefits is that this approach reduces activation memory from O(T²) to O(T), allowing much longer context lengths without blowing up VRAM and often results in 2–4× faster computation for attention at long sequences.  </p>
<p>It's important to recognize that this is just a very high level overview of what it does. Flash Attention is quite complex and explaining the details would fall outside the scope of this project. 
If interested, here's the link to read more about this: <a href="https://arxiv.org/pdf/2307.08691">Flash Attention Paper</a></p>
<h3 id="example-usage">Example Usage</h3>
<p>In this project, Flash Attention is applied as: </p>
<div class="highlight"><pre><span></span><code><span class="c1"># (batch, seq_len, n_heads, h_dim) -&gt; (batch, n_heads, seq_len, h_dim)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attention</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># (batch, n_heads, seq_len, h_dim) @ (batch, n_heads, h_dim, seq_len) -&gt; (batch, n_heads, seq_len, seq_len)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">h_dim</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># (batch, n_heads, seq_len, seq_len) @ (batch, n_heads, seq_len, h_dim) -&gt; (batch, n_heads, seq_len, h_dim)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">@</span> <span class="n">v</span>
</code></pre></div>
<p>Given the Q, K, V matrices of shape <code>(batch, n_heads, seq_len, h_dim)</code>, we pass it to <code>nn.functional.scaled_dot_product_attention</code>, telling it that this uses causal masking, and would use flash attention to compute the result. </p>
<p>Whether or not it's used (depending if compatible with torch version/gpu device) is set in the configuration, which is then passed to the model during instantiation. </p>
<hr />
<h2 id="summary">Summary</h2>
<p>Throughput optimizations transform LLM training from impractical to feasible. This implementation demonstrates common approaches used in development:</p>
<ul>
<li>Layered optimizations that compound benefits  </li>
<li>Automatic precision management with AMP  </li>
<li>Graph-level optimizations via torch.compile  </li>
<li>Hardware-aware kernels for maximum performance  </li>
</ul>
<p>While each provides individual benefits, these combination of optimizations enables the scale of modern LLM training.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.math"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>