
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.18">
    
    
      
        <title>Concat - SimpleLLaMA Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#datasetmd" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="SimpleLLaMA Documentation" class="md-header__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SimpleLLaMA Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Concat
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="SimpleLLaMA Documentation" class="md-nav__button md-logo" aria-label="SimpleLLaMA Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SimpleLLaMA Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Pretraining
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset Preparation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tokenization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Tokenization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/tokenization/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/tokenization/algorithms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Algorithms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/tokenization/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/tokenization/project/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Project Usage
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Model Architecture
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            Model Architecture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/model_architecture/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/model_architecture/embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/model_architecture/attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/model_architecture/normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Normalization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/model_architecture/feedforward/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FeedForward
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/model_architecture/decoder_block/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Layer Block
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/model_architecture/end_to_end/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    End To End
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Beginner)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Beginner)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_beginner/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_beginner/model_config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Configurations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_beginner/dataset_and_batching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset and Batching
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_beginner/scheduler/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Scheduler
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_beginner/training_loop/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Loop
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_6" >
        
          
          <label class="md-nav__link" for="__nav_2_6" id="__nav_2_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Training Process (Advanced)
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_6">
            <span class="md-nav__icon md-icon"></span>
            Training Process (Advanced)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/recap/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Basics Recap
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/optimizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Optimizer Details
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/loss_function/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cross Entropy Loss
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/gradient_accumulation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gradient Accumulation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/ddp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Distributed Data Parallel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/throughput_optimizations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Throughput Optimizations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/ckpt_and_eval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Checkpointing and Evaluations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pretraining/training_advanced/final_walkthrough/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Final Walkthrough
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Supervised Fine-Tuning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Supervised Fine-Tuning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sft/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sft/dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dataset
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sft/prompt_formatting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Formatting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sft/utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Utilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sft/finetuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Finetuning Process
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            RLHF
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rlhf/overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rlhf/methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Methods
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Misc
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Misc
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../misc/benchmarking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmarking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../misc/inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../misc/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Custom Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Custom Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../custom_training/pretraining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pretraining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../custom_training/sft/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../custom_training/rlhf/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RLHF
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#datasetmd" class="md-nav__link">
    <span class="md-ellipsis">
      dataset.md
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dataset-preparation" class="md-nav__link">
    <span class="md-ellipsis">
      Dataset Preparation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-addons" class="md-nav__link">
    <span class="md-ellipsis">
      Additional Addons:!!!!!!!!!!!!!!!!!
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#make-sure-to-mention-this-is-trained-on-ascii-only" class="md-nav__link">
    <span class="md-ellipsis">
      Make sure to mention this is trained on ascii only!
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Make sure to mention this is trained on ascii only!">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataset-gathering-sharding" class="md-nav__link">
    <span class="md-ellipsis">
      Dataset Gathering &amp; Sharding
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Dataset Gathering &amp; Sharding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-sharding" class="md-nav__link">
    <span class="md-ellipsis">
      Why Sharding?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#short-medium-long-buckets" class="md-nav__link">
    <span class="md-ellipsis">
      Short / Medium / Long Buckets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intentional-leakage" class="md-nav__link">
    <span class="md-ellipsis">
      Intentional “Leakage”
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-snippet-simplified" class="md-nav__link">
    <span class="md-ellipsis">
      Example Snippet (Simplified)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overviewmd" class="md-nav__link">
    <span class="md-ellipsis">
      overview.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pretraining-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Pretraining Overview
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pretraining Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      What is Pretraining?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Dataset
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Model Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-process" class="md-nav__link">
    <span class="md-ellipsis">
      Training Process
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaway" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaway
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_advancedmd" class="md-nav__link">
    <span class="md-ellipsis">
      training_advanced.md
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training_beginnermd" class="md-nav__link">
    <span class="md-ellipsis">
      training_beginner.md
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-introduction-what-are-we-doing" class="md-nav__link">
    <span class="md-ellipsis">
      1. Introduction: What Are We Doing?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Introduction: What Are We Doing?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-idea-in-plain-english" class="md-nav__link">
    <span class="md-ellipsis">
      The Idea in Plain English
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from-text-to-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      From Text to Tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-the-model-thinks" class="md-nav__link">
    <span class="md-ellipsis">
      How the Model Thinks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-this-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why This Matters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-model-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      2. Model Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-dataset-batching" class="md-nav__link">
    <span class="md-ellipsis">
      3. Dataset &amp; Batching
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Dataset &amp; Batching">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-structure-of-x-y" class="md-nav__link">
    <span class="md-ellipsis">
      The Structure of (x, y)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example-with-toy-data" class="md-nav__link">
    <span class="md-ellipsis">
      Code Example with Toy Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-batching-matters" class="md-nav__link">
    <span class="md-ellipsis">
      Why Batching Matters
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inside-the-dataset-loader-how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      Inside the Dataset Loader: How It Works
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Inside the Dataset Loader: How It Works">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#getting-a-batch" class="md-nav__link">
    <span class="md-ellipsis">
      Getting a Batch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-this-design" class="md-nav__link">
    <span class="md-ellipsis">
      Why This Design?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-learning-rate-scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      4. Learning Rate Scheduler
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Learning Rate Scheduler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-use-a-scheduler" class="md-nav__link">
    <span class="md-ellipsis">
      Why Use a Scheduler?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scheduler-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Scheduler Implementation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visualizing-the-schedules" class="md-nav__link">
    <span class="md-ellipsis">
      Visualizing the Schedules
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-training-loop" class="md-nav__link">
    <span class="md-ellipsis">
      5. Training Loop
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Training Loop">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#instantiation" class="md-nav__link">
    <span class="md-ellipsis">
      Instantiation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-model-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      The Model Forward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-loss-function" class="md-nav__link">
    <span class="md-ellipsis">
      The Loss Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-backward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      The Backward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-optimizer-step" class="md-nav__link">
    <span class="md-ellipsis">
      The Optimizer Step
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-minimal-training-loop" class="md-nav__link">
    <span class="md-ellipsis">
      A Minimal Training Loop
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluation and Monitoring
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-tiny-generation-example" class="md-nav__link">
    <span class="md-ellipsis">
      A Tiny Generation Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bringing-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Bringing It All Together
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_architectureattentionmd" class="md-nav__link">
    <span class="md-ellipsis">
      model_architecture\attention.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      Attention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vanilla-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Vanilla Self-Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-this-project-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      In This Project: Implementation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notes" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_architecturedecoder_blockmd" class="md-nav__link">
    <span class="md-ellipsis">
      model_architecture\decoder_block.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer-decoder-block" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer Decoder Block
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformer Decoder Block">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#structure-of-a-decoder-block" class="md-nav__link">
    <span class="md-ellipsis">
      Structure of a Decoder Block
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-walkthrough" class="md-nav__link">
    <span class="md-ellipsis">
      Example Walkthrough
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example Walkthrough">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-first-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      1. First Normalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-multi-head-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      2. Multi-Head Self-Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-first-residual-connection" class="md-nav__link">
    <span class="md-ellipsis">
      3. First Residual Connection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-second-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      4. Second Normalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-feedforward-network" class="md-nav__link">
    <span class="md-ellipsis">
      5. Feedforward Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-second-residual-connection" class="md-nav__link">
    <span class="md-ellipsis">
      6. Second Residual Connection
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-this-project" class="md-nav__link">
    <span class="md-ellipsis">
      In This Project
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_architectureembeddingsmd" class="md-nav__link">
    <span class="md-ellipsis">
      model_architecture\embeddings.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#token-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Token Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embeddings-in-this-project" class="md-nav__link">
    <span class="md-ellipsis">
      Embeddings in This Project
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#walkthrough-example" class="md-nav__link">
    <span class="md-ellipsis">
      Walkthrough Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_architectureend_to_endmd" class="md-nav__link">
    <span class="md-ellipsis">
      model_architecture\end_to_end.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#end-to-end-walkthrough" class="md-nav__link">
    <span class="md-ellipsis">
      End-to-End Walkthrough
    </span>
  </a>
  
    <nav class="md-nav" aria-label="End-to-End Walkthrough">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-from-raw-text-to-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      1. From Raw Text to Tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-batching-and-shaping-the-data" class="md-nav__link">
    <span class="md-ellipsis">
      2. Batching and Shaping the Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-passing-through-the-transformer-model" class="md-nav__link">
    <span class="md-ellipsis">
      3. Passing Through the Transformer Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-step-by-step-inside-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      4. Step-by-Step Inside the Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Step-by-Step Inside the Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-embedding-layer" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Embedding Layer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-positional-information" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Positional Information
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-decoder-blocks" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Decoder Blocks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-final-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 Final Normalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45-final-linear-layer" class="md-nav__link">
    <span class="md-ellipsis">
      4.5 Final Linear Layer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-from-logits-to-predictions" class="md-nav__link">
    <span class="md-ellipsis">
      5. From Logits to Predictions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-training-loop-connection" class="md-nav__link">
    <span class="md-ellipsis">
      6. Training Loop Connection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaway_1" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaway
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_architecturefeedforwardmd" class="md-nav__link">
    <span class="md-ellipsis">
      model_architecture\feedforward.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#feedforward-networks-ffn-in-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Feedforward Networks (FFN) in Transformers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Feedforward Networks (FFN) in Transformers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-do-we-need-feedforward-layers" class="md-nav__link">
    <span class="md-ellipsis">
      Why Do We Need Feedforward Layers?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vanilla-transformer-ffn" class="md-nav__link">
    <span class="md-ellipsis">
      Vanilla Transformer FFN
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vanilla Transformer FFN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#implementation-example" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-style-ffn-swiglu" class="md-nav__link">
    <span class="md-ellipsis">
      LLaMA-Style FFN (SwiGLU)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLaMA-Style FFN (SwiGLU)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#breaking-it-down-step-by-step" class="md-nav__link">
    <span class="md-ellipsis">
      Breaking It Down Step by Step
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-balancing" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Balancing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shapes-in-action" class="md-nav__link">
    <span class="md-ellipsis">
      Shapes in Action
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-of-trick" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple-of Trick
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Dropout
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary_1" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_architecturenormalizationmd" class="md-nav__link">
    <span class="md-ellipsis">
      model_architecture\normalization.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#normalization" class="md-nav__link">
    <span class="md-ellipsis">
      Normalization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Normalization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#layer-normalization-layernorm" class="md-nav__link">
    <span class="md-ellipsis">
      Layer Normalization (LayerNorm)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#root-mean-square-normalization-rmsnorm" class="md-nav__link">
    <span class="md-ellipsis">
      Root Mean Square Normalization (RMSNorm)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-rmsnorm" class="md-nav__link">
    <span class="md-ellipsis">
      Why RMSNorm?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#putting-it-together" class="md-nav__link">
    <span class="md-ellipsis">
      Putting It Together
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary_2" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model_architectureoverviewmd" class="md-nav__link">
    <span class="md-ellipsis">
      model_architecture\overview.md
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-architecture-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Model Architecture Overview
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model Architecture Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#big-picture" class="md-nav__link">
    <span class="md-ellipsis">
      Big Picture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder-only-design" class="md-nav__link">
    <span class="md-ellipsis">
      Decoder-Only Design
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Why Transformers?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenizationalgorithmsmd" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization\algorithms.md
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-a-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      Training a Tokenizer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-bpe-in-action" class="md-nav__link">
    <span class="md-ellipsis">
      Example: BPE in Action
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example: BPE in Action">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-characters-bytes" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1. Characters → Bytes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-count-frequent-pairs" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2. Count frequent pairs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-merge-and-replace" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3. Merge and replace
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-repeat-the-process" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4. Repeat the process
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-maintain-reverse-mapping" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5. Maintain reverse mapping
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#final-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Final Tokenization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recap" class="md-nav__link">
    <span class="md-ellipsis">
      Recap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenizationexamplesmd" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization\examples.md
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenization-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tokenization Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#character-level-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Character-Level Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word-level-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Word-Level Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#subword-level-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Subword-Level Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-subword-wins" class="md-nav__link">
    <span class="md-ellipsis">
      Why Subword Wins
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#final-notes-about-bpe" class="md-nav__link">
    <span class="md-ellipsis">
      Final Notes About BPE
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenizationoverviewmd" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization\overview.md
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-is-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      What is Tokenization?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-big-picture" class="md-nav__link">
    <span class="md-ellipsis">
      The Big Picture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-subword-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Why Subword Tokenization?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenizationprojectmd" class="md-nav__link">
    <span class="md-ellipsis">
      tokenization\project.md
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenization-in-simplellama" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization in SimpleLLaMA
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Tokenization in SimpleLLaMA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-the-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      Training the Tokenizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training the Tokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#special-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      Special Tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-script" class="md-nav__link">
    <span class="md-ellipsis">
      Example Script
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-design-choices" class="md-nav__link">
    <span class="md-ellipsis">
      Key Design Choices
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoding-the-dataset" class="md-nav__link">
    <span class="md-ellipsis">
      Encoding the Dataset
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Encoding the Dataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-script_1" class="md-nav__link">
    <span class="md-ellipsis">
      Example Script
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#notes_1" class="md-nav__link">
    <span class="md-ellipsis">
      Notes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h2 id="datasetmd">dataset.md</h2>
<h2 id="dataset-preparation">Dataset Preparation</h2>
<ol>
<li>Why the Dataset Matters</li>
</ol>
<p>The dataset is one of the most important factors in pretraining. But what exactly do we mean by “data” here?</p>
<p>In this context, it’s textual information gathered from the internet — things like blog posts, articles, books, and discussion sites (Reddit being one of the most popular). All of this text is combined into a huge “corpus,” which developers then clean and process.</p>
<p>It’s not just about having a massive amount of data. Quality matters more than quantity. There’s a saying: garbage in, garbage out. If a model is trained on tons of low-quality text (bad grammar, extreme biases, incoherent scraps from random social media posts), then the model will happily learn to imitate that bad text.</p>
<p>Of course, “high-quality text” doesn’t have a strict definition. But generally, removing bias-heavy content, incorrect information, and extreme or repetitive junk makes the dataset more useful for training.</p>
<ol>
<li>Sources of Data</li>
</ol>
<p>There are a few well-known public datasets often used in LLM training:</p>
<p>CommonCrawl – A massive raw dump of the internet, updated monthly. Very messy and unprocessed.</p>
<p>C4 – Colossal Cleaned Common Crawl, a cleaned-up version of CommonCrawl curated by Google.</p>
<p>The Pile – Curated by EleutherAI, it’s a big mix of sources like arXiv, Wikipedia, GitHub, StackExchange, and more.</p>
<p>For this project, I’ll be using FineWeb, specifically the FineWebEdu subset.</p>
<ol>
<li>What is FineWeb?</li>
</ol>
<p>FineWeb is a large-scale dataset derived from CommonCrawl, but extensively filtered. The result is a 15 trillion token corpus of much higher quality text.</p>
<p><img alt="FineWeb Pipeline" src="../images/fineweb-recipe.png" /></p>
<p>As shown above, the FineWeb team applied several filters:</p>
<p>URL filtering – remove adult/unsafe sites.</p>
<p>Text extraction – clean raw HTML into usable text.</p>
<p>Language filtering – use a fastText classifier to keep only English text with score ≥ 0.65.</p>
<p>LM filtering – use a smaller language model to toss out very low-quality passages.</p>
<p>MinHash deduplication – remove near-duplicate documents.</p>
<p>C4 filters – apply the same cleaning rules used in Google’s C4 dataset.</p>
<p>Custom filters – e.g., require punctuation, remove pages with lots of tiny lines.</p>
<p>PII removal – scrub out personally identifiable information.</p>
<ol>
<li>FineWebEdu Subset</li>
</ol>
<p>FineWebEdu goes even further. It selects content with an educational focus, resulting in two smaller but more useful subsets:</p>
<p>1.3 trillion tokens – very high educational quality.</p>
<p>5.4 trillion tokens – high educational quality.</p>
<p>How was this done? The FineWeb authors actually fine-tuned a LLaMA-3 70B model to act as a text quality rater. The model was trained to assign each passage a score from 0 to 5, where higher means more “educational.”</p>
<p>Threshold = 3 → keep only text scoring ≥ 3 → results in the 1.3T token dataset.</p>
<p>Threshold = 2 → keep only text scoring ≥ 2 → results in the 5.4T token dataset.</p>
<p>So instead of just cleaning mechanically, they used an LLM itself to filter text by “educational quality.”</p>
<p>There’s a great Hugging Face writeup if you want to dive deeper:
<a href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb Blogpost</a></p>
<p>(Note: FineWeb token counts are reported using the GPT-2 tokenizer. More about tokens and tokenization in the next section.)</p>
<ol>
<li>This Project</li>
</ol>
<p>For this project, I’ll be using FineWebEdu as the main pretraining dataset.</p>
<p>A general rule of thumb for dataset sizing: you want a parameter-to-token ratio of at least 1:20 (20 tokens for every model parameter). Once you get to very high ratios (like 1:100), you start seeing diminishing returns.</p>
<h1 id="additional-addons">Additional Addons:!!!!!!!!!!!!!!!!!</h1>
<h1 id="make-sure-to-mention-this-is-trained-on-ascii-only">Make sure to mention this is trained on ascii only!</h1>
<h3 id="dataset-gathering-sharding">Dataset Gathering &amp; Sharding</h3>
<p>The first step is to collect and prepare the dataset. Since this tokenizer will be used to tokenize the <strong>FineWebEdu</strong> dataset we gathered earlier, it would also be used to train the tokenizer (generally, it's better to train the tokenizer on the same distribution as the pretraining data for LLM).<br />
But even then, the raw dataset is too large to work with as a single file, so we break it into smaller <strong>shards</strong>.</p>
<h4 id="why-sharding">Why Sharding?</h4>
<ul>
<li>Large text corpora can easily exceed hundreds of gigabytes.  </li>
<li>Training requires fast streaming of tokens — you don’t want to load the entire dataset into memory.  </li>
<li>By splitting into smaller shards (e.g., ~100MB each), we can load them efficiently and resume from checkpoints if needed.  </li>
</ul>
<h4 id="short-medium-long-buckets">Short / Medium / Long Buckets</h4>
<p>The script doesn’t just shard randomly — it separates text into three “buckets” based on length:
- <strong>Short</strong>: under ~7,500 characters.<br />
- <strong>Medium</strong>: 7,500–12,000 characters.<br />
- <strong>Long</strong>: over 12,000 characters.  </p>
<p>Why? Because training efficiency changes with sequence length. Training on shorter examples first lets the model pick up basic structure faster, while longer sequences come later when it has learned more.  </p>
<h4 id="intentional-leakage">Intentional “Leakage”</h4>
<p>The script also allows some examples to “leak” into bigger buckets. For instance:<br />
- ~15% of short samples are redirected into medium.<br />
- ~10% of short samples are redirected into long.  </p>
<p>This prevents the model from overfitting to only very long text near the end of training. In practice, real-world queries are often much shorter, so keeping a blend of lengths makes the model more robust.  </p>
<h4 id="example-snippet-simplified">Example Snippet (Simplified)</h4>
<p>Here’s a stripped-down version of the gather script:  </p>
<div class="highlight"><pre><span></span><code><span class="k">if</span> <span class="n">ex</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isascii</span><span class="p">()</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="n">max_length</span><span class="p">:</span>
    <span class="c1"># pick bucket based on text length</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="mi">7500</span><span class="p">:</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="s2">&quot;short&quot;</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">ex</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="mi">12000</span><span class="p">:</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="s2">&quot;medium&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="s2">&quot;long&quot;</span>

    <span class="c1"># allow some short → medium/long leakage</span>
    <span class="k">if</span> <span class="n">bucket</span> <span class="o">==</span> <span class="s2">&quot;short&quot;</span> <span class="ow">and</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.25</span><span class="p">:</span>
        <span class="n">bucket</span> <span class="o">=</span> <span class="s2">&quot;medium&quot;</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.6</span> <span class="k">else</span> <span class="s2">&quot;long&quot;</span>

    <span class="n">shard_text</span><span class="p">[</span><span class="n">bucket</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&lt;SOS&gt;</span><span class="si">{</span><span class="n">ex</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&lt;EOS&gt;&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Notice a couple of important details:
- All text is wrapped with <code>&lt;SOS&gt;</code> (start of sequence) and <code>&lt;EOS&gt;</code> (end of sequence).<br />
- This guarantees the tokenizer and model know exactly where an example begins and ends.<br />
- Filtering to ASCII-only ensures consistency and avoids tricky edge cases with multilingual characters (important for compute-constrained projects).  </p>
<p>By the end of this step, the dataset is organized into neatly sharded text files, ready to be fed into the tokenizer training process.</p>
<h2 id="overviewmd">overview.md</h2>
<h1 id="pretraining-overview">Pretraining Overview</h1>
<p>In this pretraining section, we’ll walk through the core parts: what pretraining is for, how the dataset is gathered, the model architecture, and the overall training process.</p>
<h2 id="what-is-pretraining">What is Pretraining?</h2>
<p>Pretraining is the very first step in building a large language model (LLM). Like all deep neural networks, the model’s weights start out completely random (usually sampled from a normal distribution). At this stage, the model is basically useless — if you ask it to generate text, it’ll just spit out noise.</p>
<p>The goal of pretraining is to give the model a general sense of language. It gets exposed to a massive dataset of text and learns patterns like grammar, sentence structure, vocabulary, and even some factual knowledge just from raw co-occurrence. The training objective is next-token prediction — given some text, the model tries to predict the next word (or token).</p>
<p>After enough training, the model learns how to produce text that flows naturally. At this point, if you give it an input sequence, it’ll continue in the way that’s statistically most likely. It’s not yet an “assistant” that follows instructions — it’s more like a text autocomplete engine. That’s where later stages like SFT and RLHF come in.</p>
<h2 id="dataset">Dataset</h2>
<p>For this project, I’m using FineWebEdu from HuggingFace. It’s a large and diverse internet-based dataset that’s been heavily filtered. Filtering steps include things like deduplication, removing boilerplate (like HTML tags from scraped pages), and content filtering to keep it relatively clean. The idea is to make sure the model sees a broad variety of text without too much junk.</p>
<h2 id="model-architecture">Model Architecture</h2>
<p>The base model is a decoder-only transformer, similar to the LLaMA-2 design. It also supports an alternative attention mechanism called MLA (Multi-head Latent Attention), adapted from DeepSeek, though that’s optional in this pipeline.</p>
<h2 id="training-process">Training Process</h2>
<p>Training uses the standard cross-entropy loss with the AdamW optimizer. Since training is iterative, the model gradually improves through many passes of gradient descent — in this case, across tens of billions of tokens.</p>
<p>By the end of pretraining, the model develops a kind of “language sense.” It can generate coherent text, but it’s still very raw. It doesn’t know how to follow instructions or have a chat — those abilities come from the later stages: SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning with Human Feedback).</p>
<h2 id="key-takeaway">Key Takeaway</h2>
<p>Pretraining lays the foundation: it teaches the model how language works at a broad level, but not how to use it in a helpful way. Think of it as teaching a child to read and write before teaching them how to answer questions or follow directions.</p>
<h2 id="training_advancedmd">training_advanced.md</h2>
<h2 id="training_beginnermd">training_beginner.md</h2>
<h2 id="1-introduction-what-are-we-doing">1. Introduction: What Are We Doing?</h2>
<p>Before we dive into training, let’s step back and ask: <strong>what exactly does it mean to “train a language model”?</strong></p>
<p>At its core, the goal is very simple:<br />
<strong>We want the model to learn how to predict the next token in a sequence of text.</strong></p>
<hr />
<h3 id="the-idea-in-plain-english">The Idea in Plain English</h3>
<p>Imagine you are reading a sentence:</p>
<blockquote>
<p>"The cat sat on the ____"</p>
</blockquote>
<p>Even without seeing the last word, your brain automatically guesses it might be <em>“mat”</em>, <em>"couch"</em>, <em>"floor"</em> or any other plausible words. That’s because you’ve learned patterns of how words usually appear together.  </p>
<p>A language model is doing the same thing, but instead of words, it deals with <strong>tokens</strong> (small units of text such as subwords, characters, or whole words depending on the tokenizer).  </p>
<p>During training, the model sees billions of examples of token sequences and learns which token is most likely to come next. Over time, this builds up into a powerful statistical understanding of language.</p>
<hr />
<h3 id="from-text-to-tokens">From Text to Tokens</h3>
<p>Computers can’t directly process raw text like <code>"The cat sat on the mat"</code>. We first need to break it down into numerical form.</p>
<ol>
<li>
<p><strong>Raw Text:</strong><br />
<code>"The cat sat on the mat"</code></p>
</li>
<li>
<p><strong>Tokenized Text (IDs):</strong><br />
<code>[1202, 850, 149, 4211, 769, 1839]</code><br />
   (each number is a token ID from a vocabulary)</p>
</li>
<li>
<p><strong>Input Tensor (batch of sequences):</strong><br />
   <div class="highlight"><pre><span></span><code>tensor([[1202,  850,  149, 4211,  769]])
</code></pre></div></p>
</li>
</ol>
<p>The vocabulary maps every possible token (like “cat”, “sat”, “on”) to a unique integer. The model only sees these integers.</p>
<hr />
<h3 id="how-the-model-thinks">How the Model Thinks</h3>
<p>Now, let’s show the high-level flow of how these tokens are processed:</p>
<pre class="mermaid"><code>flowchart LR
    A[Raw Text] --&gt; B[Tokenizer]
    B --&gt; C[Tokens: IDs]
    C --&gt; D[Transformer Model]
    D --&gt; E[Logits: raw scores]
    E --&gt; F[Softmax: probabilities]
    F --&gt; G[Loss Function]</code></pre>
<p>Let’s break down the important parts:</p>
<ul>
<li>
<p><strong>Logits:</strong> The model outputs a vector of raw scores for each possible token in the vocabulary. For example, if the vocab size is 50,000, the logits are a vector of 50,000 numbers. These are <em>not yet probabilities</em>.</p>
</li>
<li>
<p><strong>Softmax:</strong> We apply the softmax function to logits, which turns them into probabilities (values between 0 and 1 that sum to 1). For example, the model might say:</p>
<ul>
<li>“mat”: 0.72  </li>
<li>“floor”: 0.12  </li>
<li>“couch”: 0.03  </li>
<li>… and so on for every other token.  </li>
</ul>
</li>
<li>
<p><strong>Loss:</strong> The training process compares the predicted probability distribution to the correct answer (the actual next token). It calculates how far off the prediction was. This is the “loss.” The smaller the loss, the better the model is at predicting.</p>
</li>
</ul>
<hr />
<h3 id="why-this-matters">Why This Matters</h3>
<p>This “predict the next token” setup is deceptively simple, but it’s powerful. By learning these probabilities across massive amounts of text, the model starts to capture grammar, facts, reasoning, and even world knowledge — all as a side effect of next-token prediction.</p>
<p>So when we say <em>“train a transformer model,”</em> what we really mean is:<br />
- Give the model tons of sequences of tokens.<br />
- Ask it to guess the next token.<br />
- Measure how wrong it was.<br />
- Adjust the model’s weights to improve its guesses.<br />
- Repeat billions of times.  </p>
<p>That’s the heart of language model training.</p>
<hr />
<h2 id="2-model-configuration">2. Model Configuration</h2>
<p>Before we can train our transformer, we need to decide on all the hyperparameters and settings that control both the model and the training process. These settings are stored in a configuration object, which in our case is implemented using a Python dataclass called <code>TrainingConfig</code>.  </p>
<p>The configuration file may look intimidating at first, since it lists dozens of parameters. But many of them are straightforward once you understand the categories they fall into. Let’s walk through the most important ones.</p>
<hr />
<p>The first group of parameters defines where the data and outputs are stored. For example:  </p>
<ul>
<li><code>dataset_dir</code> tells the program where to find the pre-tokenized dataset files.  </li>
<li><code>tokenizer_path</code> points to the JSON file that contains the trained tokenizer.  </li>
<li><code>ckpt_dir</code> specifies the folder where model checkpoints will be saved during training.  </li>
<li><code>log_file</code> is a simple text file where progress (like loss values) is recorded.  </li>
</ul>
<p>Together, these ensure the training script knows both where to read the data from and where to save its results.  </p>
<hr />
<p>Next, we have the <strong>batch and sequence length parameters</strong>, which directly control how much data the model processes at once.  </p>
<ul>
<li><code>batch_size</code> is the number of sequences per batch. If you set this to 4, then each step processes 4 separate chunks of text in parallel.  </li>
<li><code>max_seq_len</code> is the maximum number of tokens per sequence. For example, if <code>max_seq_len = 2048</code>, then each input sequence is capped at 2048 tokens long. Longer documents must be split into smaller pieces.  </li>
<li><code>tokens_per_update</code> defines how many tokens are processed before the optimizer takes a step. Since this touches upon gradient accumulation, which is outside the scope of this basic explanation, it will be covered in the <code>training_advanced.md</code> file.  </li>
</ul>
<p>These three parameters together determine how much work the model is doing in each training step and how much GPU memory it will consume.</p>
<hr />
<p>Then comes the <strong>model architecture</strong> itself. These parameters define the shape and capacity of the transformer network:  </p>
<ul>
<li><code>n_embd</code> is the embedding dimension, the size of the vector used to represent each token internally. Larger values allow the model to capture richer relationships, but also make it heavier to train.  </li>
<li><code>n_heads</code> sets how many attention heads are used per layer. Each head can focus on different relationships in the sequence, so more heads allow for more diverse patterns.  </li>
<li><code>n_layers</code> is the number of stacked decoder layers. Each layer refines the token representations further, so deeper models are generally more powerful.  </li>
<li><code>multiple_of</code> controls the feedforward layer’s hidden dimension. Instead of choosing an arbitrary number, this ensures the size is a multiple of a fixed value (like 256), which helps optimize matrix multiplications on GPUs.  </li>
<li><code>eps</code> is a tiny value added in normalization layers to avoid division by zero errors. It’s not something you usually tweak, but it is essential for numerical stability.  </li>
<li><code>theta</code> sets the base frequency for Rotary Position Embeddings (RoPE), which are used to encode token positions into the model. Again, you typically leave this at its default.  </li>
<li><code>dropout</code> is a regularization mechanism where some connections are randomly “dropped” during training. For large pretraining, this is often set to <code>0.0</code> because the dataset itself provides enough variety, but in smaller-scale experiments you might increase it to avoid overfitting.  </li>
</ul>
<p>These architecture parameters are the “DNA” of the model. Changing them fundamentally alters the size and behavior of the transformer.</p>
<hr />
<p>Another critical part of the config is the <strong>training schedule</strong>. Training a large language model is not just about choosing an optimizer and running it — we also need to carefully plan how the learning rate evolves over time.  </p>
<ul>
<li><code>warmup_iterations</code> specifies how many steps are used to gradually increase the learning rate at the start of training. This prevents the model from diverging early on.  </li>
<li><code>max_lr</code> is the peak learning rate reached after warmup.  </li>
<li><code>min_lr</code> is the final learning rate at the end of training, typically reached through a cosine decay schedule.  </li>
<li><code>beta1</code> and <code>beta2</code> are parameters of the AdamW optimizer, which control how much past gradients influence the updates.  </li>
<li><code>weight_decay</code> is a form of regularization that prevents weights from growing too large, helping the model generalize better.  </li>
</ul>
<p>Together, these define the “pace” at which the model learns.</p>
<hr />
<p>Finally, we have the <strong>training tokens and evaluation settings</strong>.  </p>
<ul>
<li><code>training_tokens</code> is the total number of tokens the model will see during training. For example, <code>45e9</code> means 45 billion tokens in total.  </li>
<li><code>eval_interval</code> controls how often the model’s progress is evaluated. For instance, every 32 steps the model might generate text and log its loss.  </li>
<li><code>model_gen_multiplier</code> adjusts how frequently sample generations are produced during training.  </li>
</ul>
<p>The config also includes checkpointing settings such as <code>token_ckpt</code> (how often to save the model in terms of tokens processed) and <code>load_ckpt</code> (whether to resume from a previous run).</p>
<hr />
<p>Even though this configuration object looks large, most of its parameters can be grouped into four intuitive categories: <strong>paths</strong>, <strong>batching</strong>, <strong>model architecture</strong>, and <strong>training schedule</strong>. For the beginner doc, you don’t need to memorize every single field — the important thing is to understand what each group does. The rest can be treated as implementation details that you return to once you start experimenting.</p>
<h2 id="3-dataset-batching">3. Dataset &amp; Batching</h2>
<p>In the introduction, we explained that the goal of training is for the model to learn how to predict the next token in a sequence. But how do we actually present this information to the model during training? </p>
<p>This is where the <strong>Dataset Loader</strong> comes in. Its job is to take the large tokenized dataset stored on disk and feed the model with manageable “mini-batches” of tokens at every training step. Without this loader, we would have no practical way to handle billions of tokens, because we cannot load everything into memory or train on an endless stream of raw text.  </p>
<p>When training a language model, we usually start with a massive corpus of text — sometimes hundreds of gigabytes. This raw text has already been <strong>tokenized</strong> and stored in NumPy arrays for efficiency. These files are then fed into the Dataset Loader.</p>
<p>If you tried to feed the entire dataset into the model in one go, three things would immediately go wrong:<br />
1. The model would run out of memory, because GPUs cannot hold billions of tokens at once.<br />
2. Training would be extremely inefficient, since we want to update weights frequently rather than waiting for one giant pass.<br />
3. We would lose the ability to shuffle, divide across GPUs, or checkpoint easily.  </p>
<p>The Dataset Loader solves all of these problems by breaking the token stream into smaller, more manageable pieces. At each step, it delivers a <strong>batch</strong> of sequences — small slices of the dataset that the model can process in parallel.</p>
<hr />
<h3 id="the-structure-of-x-y">The Structure of <code>(x, y)</code></h3>
<p>Each batch returned by the loader consists of two tensors:  </p>
<ul>
<li><code>x</code>: The input sequences of tokens.  </li>
<li><code>y</code>: The same sequences, shifted one position to the right.  </li>
</ul>
<p>This shifting mechanism is what allows the model to learn “next token prediction.”  </p>
<p>Let’s walk through a concrete example. Suppose the dataset contains a chunk the following six tokens:  </p>
<div class="highlight"><pre><span></span><code>Tokens: [1202, 850, 149, 4211, 769, 1839]
</code></pre></div>
<p>If we set <code>batch = 1</code> and <code>seq_len = 5</code>, then the loader will slice the data like this:  </p>
<div class="highlight"><pre><span></span><code>x = [[1202, 850, 149, 4211, 769]]
y = [[ 850, 149, 4211,  769, 1839]]
</code></pre></div>
<p>At first glance, this looks like we are simply training a <strong>bigram model</strong> — for every token in <code>x</code>, we just predict the token in the same position in <code>y</code>. But that’s not really what is happening inside the transformer. The important detail is that the model doesn’t just see the token at position <em>t</em> and try to guess token <em>t+1</em>. Instead, it sees the <strong>entire sequence up to position t</strong>, and from that context, it tries to guess the next token.</p>
<p>So in this case, the training targets look more like this:  </p>
<ul>
<li>Given <code>[1202]</code>, predict <code>850</code>.  </li>
<li>Given <code>[1202, 850]</code>, predict <code>149</code>.  </li>
<li>Given <code>[1202, 850, 149]</code>, predict <code>4211</code>.  </li>
<li>Given <code>[1202, 850, 149, 4211]</code>, predict <code>769</code>.  </li>
<li>Given <code>[1202, 850, 149, 4211, 769]</code>, predict <code>1839</code>.  </li>
</ul>
<p>Notice the subtle difference. A bigram model would only look at one previous token at a time, while the transformer looks at the <strong>entire history of the sequence</strong> and uses self-attention to weigh the importance of different past tokens when predicting the next one. This is what allows it to capture long-range dependencies in language, like subject–verb agreement across many words.</p>
<hr />
<h3 id="code-example-with-toy-data">Code Example with Toy Data</h3>
<p>Here is a small code example using the <code>DatasetLoader</code> class:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">simple_llama.pretraining.dataset_loader</span> <span class="kn">import</span> <span class="n">DatasetLoader</span>

<span class="c1"># Small example</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DatasetLoader</span><span class="p">(</span>
    <span class="n">batch</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>           <span class="c1"># Number of sequences in a batch</span>
    <span class="n">seq_len</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>         <span class="c1"># Number of tokens per sequence</span>
    <span class="n">process_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>    <span class="c1"># Single-process case</span>
    <span class="n">num_processes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dataset_dir</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">,</span> 
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span>
<span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">get_batch</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x:&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y:&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>
<p>Example output (toy data):</p>
<div class="highlight"><pre><span></span><code>x: tensor([[1202,  850,  149, 4211],
           [ 769, 1839, 3521, 4879]])
y: tensor([[ 850,  149, 4211,  769],
           [1839, 3521, 4879, 2035]])
</code></pre></div>
<p>Now you can clearly see how the pairs line up. Each row of <code>x</code> is a sequence of tokens, and each row of <code>y</code> is that same sequence shifted by one token. The model is trained to predict all of those shifts in parallel.</p>
<hr />
<h3 id="why-batching-matters">Why Batching Matters</h3>
<p>The idea of batching deserves special attention. If we only trained on one sequence at a time, the model would make progress, but it would be extremely slow and would not take advantage of GPU acceleration. By grouping multiple sequences together into a batch, we can exploit the GPU’s ability to perform large matrix multiplications efficiently.</p>
<p>Suppose we use:<br />
- <code>batch = 32</code><br />
- <code>seq_len = 2048</code>  </p>
<p>In that case, the model processes <strong>65,536 tokens at every step</strong>. This is a massive increase in efficiency compared to processing a single sequence.
This batching strategy is one of the main reasons why modern transformers can be trained at such large scales. It allows us to feed in huge amounts of data per optimization step, stabilize the gradients, and make much faster progress than would otherwise be possible.</p>
<p>The Dataset Loader is therefore the <strong>bridge</strong> between the massive dataset on disk and the mini-batches that the model actually learns from. It provides structure to the training process, ensuring that at every step, the model sees just enough data to make a meaningful update — and then moves on to the next batch.</p>
<h3 id="inside-the-dataset-loader-how-it-works">Inside the Dataset Loader: How It Works</h3>
<p>When you create a <code>DatasetLoader</code>, you pass in the batch size, sequence length, dataset directory, and a few distributed training arguments:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DatasetLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">process_rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_processes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataset_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param batch: Batch size</span>
<span class="sd">        :param seq_len: Max seq len</span>
<span class="sd">        :param process_rank: Rank of the process that initializes an instance of this class</span>
<span class="sd">        :param num_processes: Total number of processes (World Size)</span>
<span class="sd">        :param dataset_dir: Dataset directory</span>
<span class="sd">        :param device: &quot;cuda&quot; or &quot;cpu&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process_rank</span> <span class="o">=</span> <span class="n">process_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">=</span> <span class="n">num_processes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

        <span class="c1"># Holds all the filepaths</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">filepaths</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dataset_dir</span><span class="p">)])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">file_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filepaths</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_idx</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Current file index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_idx</span> <span class="o">=</span> <span class="n">batch</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">process_rank</span>  <span class="c1"># Current token idx</span>
</code></pre></div>
<p>Here’s what happens under the hood in <code>__init__</code>:</p>
<ol>
<li><strong>Instance Attribute:</strong> It sets the instance attributes using the given arguments  </li>
<li><strong>File discovery:</strong> It scans the dataset directory and gathers all <code>.npy</code> files (each file stores a large array of token IDs).  </li>
<li><strong>Pointers/Tracker Initialization:</strong><br />
        - <strong><code>file_data</code>:</strong> At startup, the loader reads the <em>first</em> <code>.npy</code> file in the dataset directory into memory. This array contains a long sequence of token IDs.<br />
        - <strong><code>file_idx</code>:</strong> A counter that starts at <code>0</code>, meaning we are currently working with the first file in the dataset. As training progresses and one file is exhausted, this index is incremented to load the next file.<br />
        - <strong><code>tok_idx</code>:</strong> A pointer into the current file that tells the loader where to start slicing tokens for the next batch. This is critical because each call to <code>get_batch()</code> must pick up right where the last one left off.<br />
        - <strong>Multi-GPU offset:</strong> If using multiple GPUs (distributed training), each process is assigned a different starting offset for <code>tok_idx</code>. This prevents all GPUs from training on the exact same data, ensuring better utilization of the dataset.  </li>
</ol>
<p>Together, these three trackers (<code>file_data</code>, <code>file_idx</code>, <code>tok_idx</code>) allow the loader to move seamlessly through massive token arrays spread across multiple files, while keeping every batch aligned and avoiding duplication across processes.</p>
<hr />
<h4 id="getting-a-batch">Getting a Batch</h4>
<p>The heart of the class is <code>get_batch()</code>. This is the function called during training to get new <code>(x, y)</code> tensors.</p>
<ol>
<li>
<p><strong>Slice out a chunk of tokens:</strong>  </p>
<div class="highlight"><pre><span></span><code><span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">file_data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tok_idx</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_idx</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div>
<p>Here we grab just enough tokens for a full batch (<code>batch * seq_len</code>) plus one extra, since <code>y</code> is shifted.</p>
</li>
<li>
<p><strong>Reshape into 2D arrays:</strong>  </p>
<div class="highlight"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">)</span>
</code></pre></div>
<p>This step converts the flat token slice into two matrices:<br />
- <code>x</code> for the inputs,<br />
- <code>y</code> for the targets, shifted by one token.</p>
</li>
<li>
<p><strong>Advance the token index:</strong>  </p>
<div class="highlight"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">tok_idx</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="p">)</span>  <span class="c1"># Increment the index counter</span>

<span class="c1"># If we reach the end of file, move on to the next one</span>
<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_idx</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_processes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">file_data</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">file_idx</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">file_idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filepaths</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file_idx</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">file_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filepaths</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">file_idx</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tok_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_rank</span>
</code></pre></div>
<p>After returning a batch, the loader moves its pointer forward. If we reach the end of a file, it automatically loads the next one and update corresponding counters accordingly. </p>
</li>
<li>
<p><strong>Convert to tensors:</strong>  </p>
<div class="highlight"><pre><span></span><code><span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>
<p>The NumPy arrays are cast to <code>torch.long</code> (integer type needed for embeddings) and moved to the correct device (CPU or GPU).</p>
</li>
</ol>
<hr />
<h4 id="why-this-design">Why This Design?</h4>
<p>Overall, the Dataset Loader is designed for training efficiency:</p>
<ul>
<li><strong>Streaming from disk:</strong> It only loads one dataset file at a time, so memory usage stays low.  </li>
<li><strong>Batch alignment:</strong> It guarantees that <code>(x, y)</code> line up perfectly for next-token prediction.  </li>
<li><strong>Distributed training friendly:</strong> The <code>process_rank</code> and <code>num_processes</code> arguments make sure multiple GPUs can work on different slices of the dataset without overlap.  </li>
<li><strong>Scalable:</strong> As long as your dataset is tokenized into <code>.npy</code> files, this loader can handle billions of tokens just as easily as thousands.  </li>
</ul>
<p>One can think of it as a neat wrapper around:<br />
- slicing arrays,<br />
- reshaping them into <code>(batch, seq_len)</code> form,<br />
- shifting by one token, and<br />
- handing them to PyTorch.</p>
<p>This simplicity makes it both easy to understand and powerful enough for large-scale training.</p>
<h2 id="4-learning-rate-scheduler">4. Learning Rate Scheduler</h2>
<p>The learning rate (LR) is one of the most important hyperparameters in training deep neural networks. Too high, and training diverges; too low, and learning stalls. A <strong>scheduler</strong> adjusts the learning rate dynamically during training, instead of keeping it fixed.</p>
<p>This project includes a custom <code>Scheduler</code> class that implements warmup and three different scheduling strategies: <strong>cosine decay</strong>, <strong>linear decay</strong>, and <strong>constant LR</strong>.</p>
<hr />
<h3 id="why-use-a-scheduler">Why Use a Scheduler?</h3>
<p>Schedulers help address two common issues in optimization:</p>
<ul>
<li><strong>Exploding/vanishing gradients</strong> – keeping the LR too high/low throughout training often leads to instability or poor convergence.  </li>
<li><strong>Training dynamics</strong> – a model often benefits from a short <em>warmup</em> phase (slowly ramping LR up), followed by a gradual <em>decay</em> to smaller values.  </li>
<li><strong>Generalization</strong> – decaying the LR near the end of training often improves final accuracy/perplexity.</li>
</ul>
<p>Instead of manually adjusting LR mid-training, a scheduler automates the process.</p>
<hr />
<h3 id="scheduler-implementation">Scheduler Implementation</h3>
<p>The <code>Scheduler</code> class wraps around a PyTorch optimizer. It is initialized with a few key parameters:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Scheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch_optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">schedule</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">min_lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="c1"># schedule ∈ [&quot;cosine&quot;, &quot;linear&quot;, &quot;constant&quot;]</span>
        <span class="c1"># training_steps = total number of steps</span>
        <span class="c1"># warmup_steps = steps spent ramping LR up</span>
        <span class="c1"># max_lr = peak LR</span>
        <span class="c1"># min_lr = final LR (ignored for &quot;constant&quot;)</span>
</code></pre></div>
<ul>
<li><strong>schedule</strong>: strategy ("cosine", "linear", or "constant").  </li>
<li><strong>training_steps</strong>: total steps in training run.  </li>
<li><strong>warmup_steps</strong>: number of warmup steps (linear ramp up).  </li>
<li><strong>max_lr</strong>: highest LR used during training.  </li>
<li><strong>min_lr</strong>: final LR (for decay-based schedules).  </li>
</ul>
<hr />
<p><strong>Warmup</strong></p>
<p>During warmup, LR increases linearly from near zero to <code>max_lr</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">_update_warmup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">current_step</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">return</span> <span class="n">lr</span>
</code></pre></div>
<p>This prevents unstable updates at the beginning of training.</p>
<hr />
<p><strong>Cosine Decay</strong></p>
<p>Cosine decay smoothly lowers the LR from <code>max_lr</code> to <code>min_lr</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">_update_cosine</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">current_step</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_step</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">scale</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">return</span> <span class="n">lr</span>
</code></pre></div>
<p>This schedule is popular in modern LLM training because it decays aggressively at first, then flattens out.</p>
<hr />
<p><strong>Linear Decay</strong></p>
<p>Linear decay reduces LR steadily over time:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">_update_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">current_step</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span> <span class="o">-</span> <span class="p">(</span><span class="n">current_step</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">return</span> <span class="n">lr</span>
</code></pre></div>
<p>Simpler than cosine, but still effective.</p>
<hr />
<p><strong>Constant</strong></p>
<p>Sometimes you may want to keep LR fixed at <code>max_lr</code> (e.g., for debugging).</p>
<div class="highlight"><pre><span></span><code><span class="k">if</span> <span class="n">schedule</span> <span class="o">==</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_lr</span>
</code></pre></div>
<hr />
<p><strong>Step Method</strong></p>
<p>The central logic is in the <code>step</code> method, which updates LR depending on the phase of training:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">current_step</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_steps</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule</span> <span class="o">!=</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_warmup</span><span class="p">(</span><span class="n">current_step</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule</span> <span class="o">==</span> <span class="s2">&quot;cosine&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_cosine</span><span class="p">(</span><span class="n">current_step</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_linear</span><span class="p">(</span><span class="n">current_step</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">schedule</span> <span class="o">==</span> <span class="s2">&quot;constant&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span>
</code></pre></div>
<p>This ensures the correct schedule is applied at every step.</p>
<hr />
<h3 id="visualizing-the-schedules">Visualizing the Schedules</h3>
<p>To make things concrete, below are plots showing how the LR evolves across steps:
(All are 100k total steps, 1k of which is warmup steps, max_lr set to 1e-3 and min_lr set to 1e-4)</p>
<p><strong>Cosine with Warmup:</strong></p>
<p><img alt="Cosine LR" src="../images/lr_cosine.png" /></p>
<p><strong>Linear with Warmup:</strong></p>
<p><img alt="Linear LR" src="../images/lr_linear.png" /></p>
<p><strong>Constant LR:</strong></p>
<p><img alt="Constant LR" src="../images/lr_constant.png" /></p>
<p>You can generate these plots using the included test script in the class (<code>__main__</code> block).</p>
<hr />
<h3 id="summary">Summary</h3>
<ul>
<li><strong>Warmup</strong> prevents instability at the start of training.  </li>
<li><strong>Cosine decay</strong> → smooth, effective, widely used in LLMs.  </li>
<li><strong>Linear decay</strong> → simpler, still works well.  </li>
<li><strong>Constant</strong> → mostly for experiments/debugging.  </li>
</ul>
<p>This custom scheduler is flexible, checkpointable, and provides good control for projects like this.</p>
<h2 id="5-training-loop">5. Training Loop</h2>
<p>Training a large language model (LLM), even a small‑scale one like in this project, comes down to a repeated cycle: take a batch of data, run it through the model, calculate how wrong the predictions are, push the error backwards to update the weights, and repeat. This cycle is what we call the <strong>training loop</strong>. This section will walk in great detail through the core parts of the loop. </p>
<hr />
<h3 id="instantiation">Instantiation</h3>
<p>Before we can train the model, we need to set up all the core building blocks. Once everything is in place, the training loop itself becomes a straightforward repetition of forward pass, loss calculation, backward pass, and optimization.</p>
<hr />
<p><strong>1. Configuration Object</strong></p>
<p>The first thing we need is a configuration object that stores all of our hyperparameters. Instead of scattering values like batch size, learning rate, and number of layers across different files, it’s cleaner to place them in a single object or namespace. This makes the code easier to manage, debug, and extend.  </p>
<p>In this project, it will be the <code>TrainingConfig</code> class, located within the <code>simple_llama/pretraining</code> folder</p>
<div class="highlight"><pre><span></span><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">TrainingConfig</span><span class="p">:</span>
    <span class="c1"># === Paths and Dataset ===</span>
    <span class="n">dataset_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;short&quot;</span><span class="p">)</span>       <span class="c1"># Path to tokenized training data</span>
    <span class="n">tokenizer_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="s2">&quot;bpe_8k.json&quot;</span><span class="p">)</span>          <span class="c1"># Path to tokenizer model</span>
    <span class="n">ckpt_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;pretraining&quot;</span><span class="p">,</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">)</span>   <span class="c1"># Directory to store checkpoints</span>
    <span class="n">log_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">root_path</span><span class="p">(</span><span class="s2">&quot;simple_llama&quot;</span><span class="p">,</span> <span class="s2">&quot;pretraining&quot;</span><span class="p">,</span> <span class="s2">&quot;training_progress.txt&quot;</span><span class="p">)</span>  <span class="c1"># File to log training progress</span>

    <span class="c1"># === Batch &amp; Sequence ===</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>             <span class="c1"># Minibatch size</span>
    <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>         <span class="c1"># Maximum sequence length per sample</span>
    <span class="n">tokens_per_update</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">19</span>  <span class="c1"># ~512K tokens per optimizer update</span>

    <span class="c1"># === Model Architecture ===</span>
    <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>               <span class="c1"># Embedding dimension</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>                <span class="c1"># Number of attention heads</span>
    <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span>               <span class="c1"># Number of transformer layers</span>
    <span class="n">multiple_of</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>           <span class="c1"># Feedforward dim multiple for efficient matmul</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span>                <span class="c1"># Epsilon value to prevent div-by-zero in normalization layers</span>
    <span class="n">theta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span>              <span class="c1"># Theta for RoPE rotation frequency</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>             <span class="c1"># Dropout rate; typically 0.0 for pretraining</span>

    <span class="o">...</span>  <span class="c1"># And many more</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">TrainingConfig</span><span class="p">()</span>
</code></pre></div>
<p>This way, if we want to adjust <code>n_heads</code> or experiment with a different <code>max_lr</code>, it’s a single line change.</p>
<hr />
<p><strong>2. Dataset Loader</strong></p>
<p>Next, instantiate a dataset loader object that is defined, passing in hyperparameters as needed, extracted from the configuration object:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">simple_llama.pretraining.dataset_loader</span> <span class="kn">import</span> <span class="n">DatasetLoader</span>

<span class="n">dataset_loader</span> <span class="o">=</span> <span class="n">DatasetLoader</span><span class="p">(</span><span class="n">batch</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">process_rank</span><span class="o">=</span><span class="n">ddp_rank</span><span class="p">,</span>
                               <span class="n">num_processes</span><span class="o">=</span><span class="n">ddp_world_size</span><span class="p">,</span> <span class="n">dataset_dir</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">dataset_dir</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>
<hr />
<p><strong>3. The Model</strong></p>
<p>Now comes the centerpiece: the transformer model itself. In this project, we’ve implemented <code>LLaMaTransformer</code>, which includes embeddings, attention blocks, feedforward layers, normalization, and output projection.</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">LLaMaTransformer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Here:<br />
- <code>config</code> gives the model hyperparameters.<br />
- <code>tokenizer</code> provides the vocabulary size.<br />
- <code>device="cuda"</code> places the model on GPU.</p>
<p>Initially, the model’s parameters are random. Training gradually adjusts them so that token predictions become more accurate.</p>
<hr />
<p><strong>4. The Loss Function</strong></p>
<p>Next, we define how the model will be judged. For language modeling, the go-to choice is <strong>cross-entropy loss</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</code></pre></div>
<p>Cross-entropy measures how “surprised” the model is by the correct next token.<br />
- If the model assigns high probability → low loss.<br />
- If it assigns low probability → high loss.</p>
<hr />
<p><strong>5. The Optimizer</strong></p>
<p>Finally, we define the optimizer. We use <strong>AdamW</strong>, which is the de facto standard for transformers because it combines Adam’s adaptive gradient updates with weight decay for stability.</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">max_lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_args</span><span class="p">)</span>
</code></pre></div>
<p>This way, every training step will use the optimizer to update the model parameters and the scheduler to adjust the learning rate.</p>
<hr />
<p><strong>6. Learning Rate Scheduler</strong></p>
<p>Before we instantiate the optimizer, we actually define the <strong>learning rate scheduler</strong>. The scheduler controls how the learning rate evolves over time, which is critical for stable training.</p>
<p>We’re using the custom <code>Scheduler</code> class implemented earlier, which supports <em>linear decay</em>, <em>cosine decay</em>, or just a <em>constant</em> learning rate.</p>
<div class="highlight"><pre><span></span><code><span class="n">scheduler</span> <span class="o">=</span> <span class="n">Scheduler</span><span class="p">(</span><span class="n">torch_optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                      <span class="n">schedule</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>
                      <span class="n">training_steps</span><span class="o">=</span><span class="n">optimization_steps</span><span class="p">,</span>
                      <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_iterations</span><span class="p">,</span>
                      <span class="n">max_lr</span><span class="o">=</span><span class="n">max_lr</span><span class="p">,</span>
                      <span class="n">min_lr</span><span class="o">=</span><span class="n">min_lr</span><span class="p">)</span>
</code></pre></div>
<p>At this stage, the <code>torch_optimizer</code> is left as <code>None</code> — we’ll link it once the optimizer is created. This flexibility makes it easy to checkpoint and resume training.</p>
<hr />
<p>At this point, we’ve instantiated:
- The configuration object
- The dataset loader
- The transformer model
- The loss function
- The optimizer
- The learning rate scheduler</p>
<p>All the main components are ready. The next step is to actually run them inside the training loop.</p>
<hr />
<h3 id="the-model-forward-pass">The Model Forward Pass</h3>
<p>We begin with a batch of input tokens, grabbed from the DatasetLoader object via the <code>get_batch()</code> method. Each integer corresponds to a token ID from our vocabulary.  </p>
<p>Let’s say our batch size is <code>B = 4</code>, and the sequence length we train on is <code>T = 16</code>. A batch from the dataset loader might look like:</p>
<div class="highlight"><pre><span></span><code>x.shape = (B, T) = (4, 16)
</code></pre></div>
<p>So <code>x</code> is a 2D tensor of integers. Each row is one training example (one text sequence), and each entry is a token ID.  </p>
<p>When we feed this into the model:</p>
<div class="highlight"><pre><span></span><code><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<p>the transformer runs all of its layers: embedding lookup, multiple decoder blocks, attention, feedforward layers, normalization, and finally a linear projection back to vocabulary size.  </p>
<p>The key here is the shape change:  </p>
<ul>
<li>Input: <code>(B, T)</code> — integers.  </li>
<li>Output: <code>(B, T, C)</code> — floats, where <code>C</code> is the vocabulary size.  </li>
</ul>
<p>Why <code>(B, T, C)</code>? Because for every position in every sequence, the model outputs a vector of size <code>C</code>, which are the raw unnormalized scores for each possible token in the vocabulary. These are called <strong>logits</strong>.</p>
<hr />
<h3 id="the-loss-function">The Loss Function</h3>
<p>Once we have logits, we want to measure how good the predictions are. That is the role of the <strong>loss function</strong>. For language modeling, the standard is <strong>cross entropy loss</strong>.</p>
<p>The goal is simple: the model is asked to predict the next token in the sequence. If the input sequence is <code>[The, cat, sat, on, the]</code>, the correct output is <code>[cat, sat, on, the, mat]</code>. Each token should map to the next token.  </p>
<p>Cross entropy measures how “surprised” the model is by the correct answer. If the model already places high probability on the true next token, the loss is small. If the model thought another token was much more likely, the loss is large.  </p>
<p>In PyTorch, we use:</p>
<div class="highlight"><pre><span></span><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</code></pre></div>
<p>However, <code>CrossEntropyLoss</code> expects inputs of shape <code>(N, C)</code> where <code>N</code> is the number of items and <code>C</code> is the number of classes, and targets of shape <code>(N,)</code>.  </p>
<p>Our logits are <code>(B, T, C)</code> and our targets are <code>(B, T)</code>. So we flatten them:</p>
<div class="highlight"><pre><span></span><code><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">targets</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>
<p>This reshapes:</p>
<ul>
<li><code>logits.view(-1, C)</code> → <code>(B*T, C)</code>  </li>
<li><code>targets.view(-1)</code> → <code>(B*T,)</code>  </li>
</ul>
<p>Effectively, we treat the whole batch as one big list of token predictions.</p>
<p>Mathematically, cross entropy loss is:</p>
<div class="highlight"><pre><span></span><code>L = - (1/N) * Σ log(softmax(logits)[i, y_i])
</code></pre></div>
<p>where <code>y_i</code> is the true class (the correct next token).  </p>
<p>More details will be covered in the Advanced Training page</p>
<hr />
<h3 id="the-backward-pass">The Backward Pass</h3>
<p>Now comes the critical part: telling the model how wrong it was. This is done with:</p>
<div class="highlight"><pre><span></span><code><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div>
<p>This triggers PyTorch’s <strong>autograd engine</strong>, which walks backwards through the computational graph.  </p>
<p>Every tensor operation in PyTorch (matrix multiplies, nonlinearities, normalizations) records how it was computed. During <code>.backward()</code>, PyTorch applies the chain rule of calculus to compute gradients of the loss with respect to every parameter in the model.</p>
<p>So, if our model has parameters θ = {W1, W2, …}, then after <code>loss.backward()</code> we now have stored gradients ∂L/∂W for each parameter. These gradients are stored in each parameter tensor within the <code>.grad</code> attribute, which is a matrix of gradients the shape as the weight matrix. </p>
<p>These gradients tell us: “If you nudge this weight slightly, the loss would go up/down this much.” They are the signals that will guide weight updates.</p>
<hr />
<h3 id="the-optimizer-step">The Optimizer Step</h3>
<p>With gradients calculated, we actually update the weights. This is the job of the optimizer.  </p>
<p>In this project, we use <strong>AdamW</strong>:</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div>
<p>AdamW is a variant of stochastic gradient descent that adapts learning rates per parameter and includes proper weight decay. It’s widely used in training transformers.</p>
<p>The update cycle is:</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># reset gradients to zero</span>

<span class="c1"># Between these two steps, perform forward pass, calculate loss, back propagation</span>

<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>       <span class="c1"># update parameters using gradients</span>
</code></pre></div>
<p>Why zero gradients? Because PyTorch accumulates gradients by default. If we didn’t zero them, gradients from multiple steps would pile up.</p>
<p>So the full cycle is:</p>
<ol>
<li>Zero gradients → prepare for next step.</li>
<li>Forward pass → compute logits and loss.</li>
<li>Loss calculation → use criterion to calculate loss.</li>
<li>Backward pass → compute gradients.  </li>
<li>Optimizer step → update weights.  </li>
</ol>
<hr />
<h3 id="a-minimal-training-loop">A Minimal Training Loop</h3>
<p>Putting everything together:</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="c1"># Get a batch</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset_loader</span><span class="o">.</span><span class="n">get_batch</span><span class="p">()</span>   <span class="c1"># x: (B, T), y: (B, T)</span>

    <span class="c1"># Forward pass</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                   <span class="c1"># (B, T, C)</span>

    <span class="c1"># Compute loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Backward pass</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Granted the actual implementation in <code>simple_llama.pretraining.train</code> file is much more complex, however this is the backbone of training. Every sophisticated training pipeline — from GPT to LLaMA — reduces to these few lines.  </p>
<hr />
<h3 id="evaluation-and-monitoring">Evaluation and Monitoring</h3>
<p>Training is only half the story. We need to know if the model is improving. The simplest way is to track the <strong>training loss</strong>. Over time, as the model sees more data, loss should decrease, which means the model is getting progressively better at predicting the next token, given an input sequence of tokens.</p>
<p>At the very beginning, before the model has learned anything meaningful, predictions are essentially random. In this case, the expected loss can be approximated by the natural logarithm of the vocabulary size, since each token is equally likely under a uniform distribution.  </p>
<p>For our project, the vocabulary size is 8192. So if the predictions were truly uniform, the expected initial loss would be:</p>
<div class="highlight"><pre><span></span><code>ln(8192) ≈ 9.01
</code></pre></div>
<p>However, in practice, most parameters in the model (such as linear layers) are initialized from Gaussian distributions using Kaiming or Xavier initialization. This breaks the perfect uniformity and introduces biases. As a result, the observed loss at the very start of training may be slightly higher than the theoretical value — for example, around 9.2 or 9.3 instead of exactly 9.01.  </p>
<hr />
<p><strong>Why the log of vocab size? (Derivation)</strong></p>
<p>Cross-Entropy Loss (CEL) is essentially a Negative Log Likelihood (NLL) loss. For a dataset of size <code>N</code> with true labels <code>y_i</code> and predicted probabilities <code>p(y_i)</code>:</p>
<div class="highlight"><pre><span></span><code>CEL = - (1/N) * Σ log(p(y_i))
</code></pre></div>
<p>For a single example where the true class is <code>c</code>:</p>
<div class="highlight"><pre><span></span><code>CEL = - log(p(c))
</code></pre></div>
<p>If the model predicts uniformly over all <code>V</code> classes, then <code>p(c) = 1/V</code>. Plugging this in:</p>
<div class="highlight"><pre><span></span><code>CEL = - log(1/V)
    = log(V)
</code></pre></div>
<p>So under uniform predictions, the expected loss equals the log of vocabulary size.</p>
<p>Example:<br />
- <code>V = 8192</code><br />
- <code>CEL = log(8192)</code><br />
- <code>CEL ≈ 9.01</code>  </p>
<p>This is the theoretical baseline for random guessing. In practice, initialization bias may push it to ~9.3 at step 0.</p>
<hr />
<p><strong>Training Dynamics</strong></p>
<p>As training continues, the loss should decrease steadily. For instance, a drop from ~9.3 to ~3 or ~2 means the model is learning meaningful statistical patterns in the data. Lower loss translates directly into the model being less “surprised” when predicting the next token.</p>
<p>Think of it this way:<br />
- At loss ≈ 9, the model is basically clueless, assigning ~1/8192 probability to every token.<br />
- At loss ≈ 3, the model assigns ~1/20 probability to the correct token on average.<br />
- At loss ≈ 1, the model is strongly confident, giving ~1/3 probability to the correct token.</p>
<p>Even at a loss of around 3.0, the probabiliy assignment is at ~1/20. That may sound low if one interpret it as "The model only have a 5% chance of choosing the correct token, for a given sequence"
However that is a bit misleading. In English (or just about all languages) there is natural entropy to it. Vast majority of the time, there are multiple valid answers to a given sequence.  </p>
<p>Taking the previous example, we give the model: <code>[The, cat, sat, on, the]</code> and want it to predict the next token. Our true label should be the token corresponding to the word <code>mat</code> however, in general, there isn't just a single right-wrong answer. 
Words like <code>floor</code>, <code>ground</code>, <code>couch</code> and such are also completely valid. Hence a probability of 1/20 chance choosing the 'correct' token isn't as bad a it may numerically seem to be. </p>
<hr />
<p><strong>Validation?</strong></p>
<p>It’s also common to periodically <strong>evaluate</strong> on a held-out validation set. This prevents overfitting, since training loss always decreases but validation loss may rise if the model memorizes.  </p>
<p>However, in this project, no validation set is used. Why? Because the dataset (50B tokens gathered from FineWebEdu) is completely unique. Training is done in a single epoch — the model will only see each token sequence once. Under this regime, overfitting is theoretically impossible.  </p>
<p>In fact, if a model with ~1B parameters <em>were</em> able to fully overfit on 50B unique tokens, that would be remarkable — it would essentially mean the model is acting as a form of near-lossless compression of the dataset. From that perspective, it might even be considered desirable. But in practice, that's nearly impossible. Here we will only go through one pass using the 50B tokens, simply track training loss as the main signal of progress.</p>
<hr />
<h3 id="a-tiny-generation-example">A Tiny Generation Example</h3>
<p>Even early in training, it’s fun (and useful) to test the model by generating text.  </p>
<p>We take a prompt, tokenize it, and call a generate function:</p>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Once upon a time&quot;</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">))</span>
</code></pre></div>
<p>At the start, the output will be nonsense — the model has learned almost nothing. But as loss decreases, generated samples gradually improve. They start forming grammatical sentences, then coherent paragraphs.</p>
<p>This qualitative check is as important as loss curves, because it directly shows what the model is learning.</p>
<hr />
<h3 id="bringing-it-all-together">Bringing It All Together</h3>
<p>To summarize, each training step does:</p>
<ol>
<li>Take a batch <code>(B, T)</code> of token IDs.  </li>
<li>Run through model → get logits <code>(B, T, C)</code>.  </li>
<li>Compute cross entropy loss with targets <code>(B, T)</code>.  </li>
<li>Backpropagate loss → compute gradients.  </li>
<li>Optimizer updates weights.  </li>
<li>Zero gradients.  </li>
</ol>
<p>This loop runs millions of times. At small scale, it might be just tens of thousands of steps. At large scale (GPT‑3, LLaMA), training can take trillions of tokens.</p>
<p>But the essence is always the same. The beauty of the transformer is that all of this complexity — embeddings, attention, normalization, feedforward layers — reduces down to the training loop you’ve just seen.</p>
<hr />
<h2 id="model_architectureattentionmd">model_architecture\attention.md</h2>
<h1 id="attention">Attention</h1>
<p>Attention is the heart of the transformer. It’s the mechanism that lets the model decide:<br />
<strong>“Which other tokens in the sequence are relevant for predicting the next one?”</strong></p>
<hr />
<h2 id="vanilla-self-attention">Vanilla Self-Attention</h2>
<p>In the vanilla self-attention, we first start out with a given tensor, <code>x</code>, of shape <code>(batch, seq_len, n_embd)</code> as mentioned in the previous <code>embeddings</code> section.</p>
<p>Given that tensor, we compute three tensors, Each of them is a linear projection from the input tensor <code>x</code><br />
- <strong>Query (Q)</strong>: what each token is looking for.<br />
- <strong>Key (K)</strong>: what each token offers.<br />
- <strong>Value (V)</strong>: the information carried by each token.  </p>
<p>This is done by either creating 3 separate linear layers in the constructor for the self-attention class: </p>
<div class="highlight"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>

<span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>  <span class="c1"># This will be used and touched upon later on</span>
</code></pre></div>
<p>Where it can later be invoked to create the Q, K, and V tensors that will be used in attention computation:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Takes an input tensor x of shape (batch, seq_len, n_embd) and linearly project it into another tensor, retaining the shape</span>
<span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  
<span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<p>However a more common method is the merge all of those linear layers into a single one, for more efficient computation:
<div class="highlight"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">)</span>

<span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>
</code></pre></div></p>
<p>Then later on, use that to get: </p>
<div class="highlight"><pre><span></span><code><span class="c1"># Input shape: (batch, seq_len, n_embd), output shape: (batch, seq_len, 3 * n_embd)</span>
<span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Split the tensor along the last dimension </span>
<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<p>Both of those will produce the same result. </p>
<p>At this point, given an input tensor <code>x</code>, we now have 3 separate tensors <code>q</code>, <code>k</code>, and <code>v</code>, all of the shape <code>(batch, seq_len, n_embd)</code>  </p>
<p>Next, we 'expand' the tensor from 3d to 4d, using the <code>n_heads</code> hyperparameter. 
<code>n_heads</code> defines how many 'heads' we want in attention. Further explanation as to what it does will be given below. </p>
<p>We would use the last dimension, <code>n_embd</code> and divide that into <code>(n_heads, head_dim)</code>, based on the formula <code>head_dim = n_embd // n_heads</code>
For example, given <code>n_embd=1024</code>, <code>n_heads=16</code>, then <code>head_dim=1024//16=64</code>, meaning we transform our 1024 embedding dimensions into 16 heads, each head have 64 dimension to work with. 
It is crucial to add an assertion that <code>n_embd % n_heads == 0</code> to make sure it evenly divides. </p>
<p>Given the hyperparameter <code>n_heads</code>, calculate <code>head_dim</code>, then view/reshape the tensor accordingly as follows: 
<div class="highlight"><pre><span></span><code><span class="c1"># (batch, seq_len, n_embd) -&gt; (batch, seq_len, n_heads, head_dim)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
</code></pre></div></p>
<p>Finally, we swap the dimension for <code>seq_len</code> and <code>n_heads</code></p>
<div class="highlight"><pre><span></span><code><span class="c1"># (batch, seq_len, n_heads, head_dim) -&gt; (batch, n_heads, seq_len, head_dim)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div>
<p>Now that our Q, K, V matrices are all of shape <code>(batch, n_heads, seq_len, head_dim)</code>, we apply the self attention formula: </p>
<div class="arithmatex">\[
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}} + Mask \right) V
\]</div>
<p>The first step is to compute <span class="arithmatex">\(<span class="arithmatex">\(QK^\top\)</span>\)</span>
Looking at the shape of the tensors, we will get an output:
<code>(batch, n_heads, seq_len, head_dim) @ (batch, n_heads, head_dim, seq_len) = (batch, n_heads, seq_len, seq_len)</code><br />
which we can denote as <code>attn_scores = QK^T</code>  </p>
<p>Note the <code>(seq_len, seq_len)</code> ending dimensions. That's the reason why people say that attention computation scales quadratically w.r.t the seq_length of the model</p>
<p>Next, we apply element-wise division on the computed matrix, where the divisor is the sqrt of <code>d_k</code>, which from the paper refers to the dimensions of each head (<code>head_dim</code>)</p>
<p><code>attn_scores = attn_scores / math.sqrt(head_dim)</code></p>
<p>The attention scores is normalized by head-dim primarily because of the softmax operation that will immediately take place next.<br />
In short, typically the Q, K, V matrices have unit gaussian distribution, when we apply matrix multiplication, the variance scales by <code>head_dim</code>
However with how softmax works, values that are more than, say, 3 units less than the maximum in the dimension of interest will be heavily squashed to approximately 0. 
In our example scenario, if <code>head_dim=64</code>, that means the std increased from 1 to 8, which would compress the tensor into something similar to a one-hot vector.  </p>
<p>Moving along, after we normalize the attention scores, which doesn't change the shape of the tensor, we would need to apply a triangular mask to <code>attn_scores</code>.  </p>
<p>Typically, LLMs would add in these mask to prevent the model from 'cheating' by looking at future tokens.<br />
Sort of like:<br />
If someone gives you a sentence, 'I love my dog' and asks, 'What is the word after 'my'?'
It's trivial. The answer is 'dog'. However masking prevents that by, as the name suggests, masking out the next token. 
In that same example, the other person would give 'I love my _' and then ask, 'What is the word after 'my'?'</p>
<p>Now in this example, let's use the sentence: "That is a blue dog" tokenized into <code>[That, is, a, blue, dog]</code> (Since there are only 5 tokens, that means <code>seq_len=5</code> in this example)
After going through the embedding layer and above steps, we will reach a tensor of shape <code>(batch, n_heads, 5, 5)</code>
Grabbing one of the (5, 5) matrices arbitrarily might look something like: </p>
<div class="highlight"><pre><span></span><code>&quot;That&quot;   &quot;is&quot;     &quot;a&quot;      &quot;blue&quot;   &quot;dog&quot;
----------------------------------------------
[ 1.93    1.49     0.90    -2.11     0.68 ]   ← &quot;That&quot;
[-1.23   -0.04    -1.60    -0.75    -0.69 ]   ← &quot;is&quot;
[-0.49    0.24    -1.11     0.09    -2.32 ]   ← &quot;a&quot;
[-0.22   -1.38    -0.40     0.80    -0.62 ]   ← &quot;blue&quot;
[-0.59   -0.06    -0.83     0.33    -1.56 ]   ← &quot;dog&quot;
</code></pre></div>
<p>That tells us how much attention each token pays to each other. 
Now these are un-normalized values, so it would be hard to interpret, at least for now. </p>
<p>We then apply the triangular mask that prevents tokens to look ahead. It would be something like: </p>
<div class="highlight"><pre><span></span><code>[  0   -∞   -∞   -∞   -∞ ]
[  0    0   -∞   -∞   -∞ ]
[  0    0    0   -∞   -∞ ]
[  0    0    0    0   -∞ ]
[  0    0    0    0    0 ]
</code></pre></div>
<p>Applying via element-wise addition:
<code>attn_scores = attn_scores + mask</code> </p>
<p>The result would now look like: 
<div class="highlight"><pre><span></span><code>&quot;That&quot;   &quot;is&quot;     &quot;a&quot;      &quot;blue&quot;   &quot;dog&quot;
----------------------------------------------
[  1.93    -∞      -∞      -∞      -∞  ]   ← &quot;That&quot;
[-1.23   -0.04     -∞      -∞      -∞  ]   ← &quot;is&quot;
[-0.49    0.24   -1.11     -∞      -∞  ]   ← &quot;a&quot;
[-0.22   -1.38   -0.40     0.80     -∞  ]   ← &quot;blue&quot;
[-0.59   -0.06   -0.83     0.33   -1.56 ]   ← &quot;dog&quot;
</code></pre></div></p>
<p>we pass it through the softmax function to transform it into something like a probability distribution. </p>
<div class="highlight"><pre><span></span><code><span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&quot;That&quot;   &quot;is&quot;     &quot;a&quot;      &quot;blue&quot;   &quot;dog&quot;
----------------------------------------------
[1.0000   0.0000   0.0000   0.0000   0.0000 ]   ← &quot;That&quot;
[0.2330   0.7670   0.0000   0.0000   0.0000 ]   ← &quot;is&quot;
[0.2759   0.5753   0.1488   0.0000   0.0000 ]   ← &quot;a&quot;
[0.2032   0.0632   0.1699   0.5637   0.0000 ]   ← &quot;blue&quot;
[0.1566   0.2658   0.1236   0.3942   0.0596 ]   ← &quot;dog&quot;
</code></pre></div>
<p>As you can see, starting out with the token corresponding to 'That', it can only pay attention to itself, since it's the first token in the sequence
Next is the word 'is', which splits the attention between 'That' and itself
So on and so forth. This tells the model how much each token attends to each other. </p>
<p>At this point, the <code>attn_weights</code> tensor is still of shape <code>(batch, n_heads, seq_len, seq_len)</code> since both normalization and softmax doesn't change the tensor shape  </p>
<p>Now we process the final steps: </p>
<div class="highlight"><pre><span></span><code><span class="c1"># (batch, n_heads, seq_len, seq_len) @ (batch, n_heads, seq_len, head_dim) = (batch, n_heads, seq_len, head_dim)</span>
<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">v</span>

<span class="c1"># (batch, n_heads, seq_len, head_dim) -&gt; (batch, seq_len, n_heads, head_dim) -&gt; (batch, seq_len, n_embd)</span>
<span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>

<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
</code></pre></div>
<p>The final step here is to first matrix multiply with the <code>v</code> tensor to get the tensor of shape <code>(batch, n_heads, seq_len, head_dim)</code>
We revert the permutation and viewing to get back the original input shape <code>(batch, seq_len, n_embd)</code>
Finally, apply the output projection matrix to <code>attn_output</code> before returning. </p>
<p>So what's the purpose of output matrix? </p>
<p>One can think of it as a way to combine the information that each head learned. 
Recall that when we apply attention, we use multiple heads. Each head process their own 'chunk' of embedding dimensions compeltely separately from each other. 
It's beneficial to allow them to learn their own information, however at the end, we merely concatenate them together. </p>
<p>The final output projection matrix allows the information to get 'aggregated' and combined. </p>
<hr />
<h2 id="in-this-project-implementation">In This Project: Implementation</h2>
<p>In <code>MHSelfAttention</code>, queries, keys, and values are created together, where it's first viewed then chunked along the last dimension:</p>
<div class="highlight"><pre><span></span><code><span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv_linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  
<span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">h_dim</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>Each input token embedding is linearly projected into Q, K, V vectors.  </li>
<li>Shape after splitting: <code>(batch, seq_len, n_heads, h_dim)</code>.  </li>
</ul>
<p>Then, before computing attention, <strong>Rotary Position Embeddings (RoPE)</strong> are applied to Q and K:</p>
<div class="highlight"><pre><span></span><code><span class="n">q</span> <span class="o">=</span> <span class="n">apply_rotary_embeddings</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">freqs_complex</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">apply_rotary_embeddings</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">freqs_complex</span><span class="p">)</span>
</code></pre></div>
<p>Why? Token embeddings alone tell the model what a word is, but not where it appears. Without positional info,  </p>
<ul>
<li>“The cat sat on the mat.”  </li>
<li>“The mat sat on the cat.”  </li>
</ul>
<p>would look identical.</p>
<p>Instead of adding positional vectors (like the original Transformer’s sinusoidal method), RoPE rotates Q and K in the complex plane by an amount proportional to their position. This makes attention directly sensitive to relative distances between tokens.</p>
<p>For our purposes, you can think of RoPE as: “a lightweight operation on Q and K that encodes order, without changing tensor shapes.”</p>
<p>(If you want to dive deeper, check the RoPE paper on arxiv.)</p>
<p>Next, permute the tensors then apply scaled dot-product attention:</p>
<div class="highlight"><pre><span></span><code><span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">h_dim</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">@</span> <span class="n">v</span>
</code></pre></div>
<ul>
<li>The causal mask ensures each token can only attend to past tokens (left-to-right).  </li>
<li>Softmax converts similarity scores into weights.  </li>
<li>Weighted sum with V produces the final attended representation.</li>
</ul>
<p>Finally, results from all heads are concatenated and passed through a linear projection to mix information.</p>
<hr />
<h2 id="notes">Notes</h2>
<ol>
<li>
<p><strong>Why not Multi-Query Attention?</strong><br />
   The original LLaMA-2 paper uses <em>multi-query attention</em> (MQA), where all heads share the same K and V but have separate Q.<br />
   This greatly reduces KV-cache memory usage, which is important for scaling to very large models and efficient inference.<br />
   For this project, memory pressure from KV-cache isn’t a bottleneck, so standard multi-head attention is simpler and sufficient.</p>
</li>
<li>
<p><strong>What about DeepSeek’s MLA?</strong><br />
   This project includes an optional implementation of <strong>Multi-Head Latent Attention (MLA)</strong>, which is a refinement that reduces KV-cache memory even further while keeping multiple latent spaces.<br />
   It’s more efficient than MQA, but again — KV-cache isn’t the limiting factor here.<br />
   Since the focus is educational clarity, SimpleLLaMA sticks with classic multi-head attention.</p>
</li>
</ol>
<h2 id="model_architecturedecoder_blockmd">model_architecture\decoder_block.md</h2>
<h1 id="transformer-decoder-block">Transformer Decoder Block</h1>
<p>The <strong>decoder block</strong> is the fundamental building unit of the transformer.<br />
Each block combines <strong>attention</strong>, <strong>feedforward networks</strong>, <strong>normalization</strong>, and <strong>residual connections</strong> into a repeatable structure.</p>
<hr />
<h2 id="structure-of-a-decoder-block">Structure of a Decoder Block</h2>
<p>A decoder block has two main parts:</p>
<ol>
<li><strong>Multi-Head Self-Attention (MHA)</strong> → lets tokens exchange information.  </li>
<li><strong>Feedforward Network (FFN)</strong> → transforms the attended features into richer representations.  </li>
</ol>
<p>Surrounding these are:<br />
- <strong>RMSNorm</strong> → stabilizes training by normalizing activations.<br />
- <strong>Residual Connections</strong> → ensure information from earlier layers isn’t lost.  </p>
<p>The primary block flow is:</p>
<div class="highlight"><pre><span></span><code>Input → Norm → Attention → Residual → Norm → Feedforward → Residual → Output
</code></pre></div>
<p>This <strong>“pre-norm” setup</strong> (normalize before each sub-layer) is known to improve stability in deep transformers.</p>
<hr />
<h2 id="example-walkthrough">Example Walkthrough</h2>
<p>Let’s step through what happens inside one decoder block.<br />
Suppose we have an input tensor <code>x</code> of shape <code>(batch, seq_len, n_embd)</code>.</p>
<h3 id="1-first-normalization">1. First Normalization</h3>
<p><div class="highlight"><pre><span></span><code><span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
- RMSNorm is applied to <code>x</code>.<br />
- This ensures the activations are scaled to a stable range before entering attention.<br />
- Unlike LayerNorm, RMSNorm does not recenter the mean — it only rescales variance.  </p>
<h3 id="2-multi-head-self-attention">2. Multi-Head Self-Attention</h3>
<p><div class="highlight"><pre><span></span><code><span class="n">attn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">freqs_complex</span><span class="p">)</span>
</code></pre></div>
- Each token produces <strong>query, key, and value</strong> vectors.<br />
- Rotary Position Embeddings (RoPE) are applied to Q and K to inject positional info.<br />
- Attention computes how strongly each token attends to others in the sequence.<br />
- The output has the same shape as the input: <code>(batch, seq_len, n_embd)</code>.</p>
<h3 id="3-first-residual-connection">3. First Residual Connection</h3>
<p><div class="highlight"><pre><span></span><code><span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">attn_out</span>
</code></pre></div>
- Here we add the original input <code>x</code> back to the attention output.<br />
- This is called a <strong>residual connection</strong> (or skip connection).  </p>
<p>Why is this important?<br />
- Imagine stacking dozens of layers. Without skip connections, the network could "forget" the original signal after being transformed multiple times.<br />
- By adding <code>x</code> back, we preserve the original information while also giving the model access to the new transformed features from attention.<br />
- During backpropagation, residuals also help gradients flow more smoothly, preventing vanishing or exploding gradients.<br />
- In practice, you can think of it as: <em>the model learns adjustments (deltas) on top of the original input, instead of rewriting it from scratch every time.</em>  </p>
<h3 id="4-second-normalization">4. Second Normalization</h3>
<p><div class="highlight"><pre><span></span><code><span class="n">h_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</code></pre></div>
- Again, normalize before the next sublayer.<br />
- This keeps the values stable before passing into the FFN.  </p>
<h3 id="5-feedforward-network">5. Feedforward Network</h3>
<p><div class="highlight"><pre><span></span><code><span class="n">ffn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffwd</span><span class="p">(</span><span class="n">h_norm</span><span class="p">)</span>
</code></pre></div>
- Input passes through a SwiGLU feedforward network (as described in detail in the FFN doc).<br />
- Adds nonlinearity and transformation capacity.<br />
- Output shape: <code>(batch, seq_len, n_embd)</code>.</p>
<h3 id="6-second-residual-connection">6. Second Residual Connection</h3>
<p><div class="highlight"><pre><span></span><code><span class="n">out</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">ffn_out</span>
</code></pre></div>
- Again, the skip connection ensures that the block doesn’t overwrite the information coming from the attention stage.<br />
- Instead, it layers on additional transformations from the FFN.<br />
- By the time you stack many decoder blocks, each one is contributing refinements while keeping the original context intact.<br />
- This makes the network much more robust and trainable.  </p>
<p>Final output shape: <code>(batch, seq_len, n_embd)</code>.</p>
<hr />
<h2 id="in-this-project">In This Project</h2>
<ul>
<li><strong>Attention type</strong>: defaults to standard multi-head self-attention, with optional MLA for efficiency.  </li>
<li><strong>Normalization</strong>: RMSNorm used everywhere (simpler than LayerNorm, but empirically stable).  </li>
<li><strong>Activation</strong>: SiLU-based feedforward (SwiGLU).  </li>
<li><strong>Dropout</strong>: applied after projections, mainly used during fine-tuning (SFT/RLHF).  </li>
<li><strong>Residuals</strong>: used after both the attention and FFN sublayers.  </li>
</ul>
<p>Together, these form the repeating backbone of the SimpleLLaMA model.<br />
By stacking many of these blocks, the network can build increasingly complex representations of text sequences.</p>
<h2 id="model_architectureembeddingsmd">model_architecture\embeddings.md</h2>
<h1 id="embeddings">Embeddings</h1>
<p>Embeddings are the first step in turning discrete tokens (integers from tokenization) into continuous vectors for a neural network to process.</p>
<hr />
<h2 id="token-embeddings">Token Embeddings</h2>
<p>After tokenization, each word or subword is represented by an integer ID. But LLMs don’t work directly with these integers. Instead, we map each token ID into a <strong>dense vector of fixed size</strong> (the embedding dimension).</p>
<p>In PyTorch, this is done with an <code>nn.Embedding</code> layer. In the <code>LLaMaTransformer</code> class from this project, you’ll see:</p>
<div class="highlight"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">(),</span> <span class="n">n_embd</span><span class="p">)</span>
</code></pre></div>
<p>(Often times the embedding dimensionality of the model, <code>n_embd</code> in this case, is referred to under different names. Common ones include <code>embedding_dim</code>, <code>d_model</code>, and <code>hidden_size</code>)</p>
<p>Here, an embedding layer is created, which serves as a lookup table for the model.<br />
It takes in two primary values, <code>vocab_size</code> and <code>n_embd</code> and create a matrix of shape <code>(vocab_size, n_embd)</code><br />
Each row corresponds to a token ID and is a trainable vector. For example:</p>
<ul>
<li>Token "I" → 73 → [0.12, -0.44, 1.05, ...]</li>
<li>Token "an" → 256 → [1.33, 0.05, -0.72, ...]</li>
</ul>
<p>Both will be map to unique vectors, all of which will be of length <code>n_embd</code>
At initialization, these vectors are random. During training, the model adjusts them so that it learns semantic relationship between tokens.  </p>
<p><code>n_embd</code> is a crucial hyperparameter when creating a LLM. It essentially gives the model the flexibility of how much 'semantics' each token can hold.   </p>
<p>For example, say the word <code>man</code> and <code>woman</code> can be represented by a single token, <code>1098</code> and <code>1290</code> respectively. 
Passing those through the embedding layer, the model will grab the vector at row index of <code>1098</code> to represent that as man, and row index <code>1290</code> for woman</p>
<p>Their vectors will differ, but both have shape <code>(n_embd,)</code>.
You can think of each dimension in this vector space as encoding some abstract feature the model has learned. For example, one combination of dimensions might capture gender-like differences (man vs. woman), while another might capture whether something is an animate being or an object.<br />
However this is just a simplified way of explanation. In reality, these dimension are polysemantic and is much more complex.
(Should include explanation that each value is a dimension as well?)</p>
<p>Once we convert our list of tokens into a list of vectors, we can proceed with passing that to the Decoder Block.</p>
<hr />
<h2 id="embeddings-in-this-project">Embeddings in This Project</h2>
<ul>
<li><strong>Embedding dimension</strong> (<code>n_embd</code>) is configurable (e.g., 768, 1024, or higher).  </li>
<li><strong>RoPE</strong> is used for positional encoding by default, following LLaMA.  </li>
<li><strong>Initialization</strong>: embeddings start random and are updated through backpropagation.  </li>
<li><strong>Tied weights</strong>: this project experimented with tying embeddings to the final output projection layer (a trick used in some models). But in practice, training became unstable, so it was disabled.</li>
</ul>
<hr />
<h2 id="walkthrough-example">Walkthrough Example</h2>
<p>Let’s walk through a toy example with the sentence:  </p>
<p><strong>“I love dogs”</strong>  </p>
<p><strong>Step 1. Tokenization</strong>  </p>
<p>Using an arbitrary <em>word-level</em> tokenizer, each word is mapped to an integer ID:  </p>
<ul>
<li><code>"I"</code> → 73  </li>
<li><code>"love"</code> → 786  </li>
<li><code>"dogs"</code> → 2934  </li>
</ul>
<p>So the input sequence is:  </p>
<div class="highlight"><pre><span></span><code>[73, 786, 2934]
</code></pre></div>
<hr />
<p><strong>Step 2. Embedding Matrix</strong>  </p>
<p>When we create an <code>nn.Embedding(vocab_size, embedding_dim)</code> layer, it internally builds a big <strong>lookup table</strong> (a matrix).  </p>
<ul>
<li>Shape = <code>(vocab_size, embedding_dim)</code>  </li>
<li>Each row index corresponds to a token ID.  </li>
<li>Each row contains a vector of length <code>embedding_dim</code>.  </li>
</ul>
<p>For this example, let’s set <code>embedding_dim = 8</code>. That means every token ID will be mapped to an <strong>8-dimensional vector</strong>.  </p>
<p>A (very small) portion of the embedding matrix might look like this at initialization (values are random floats):  </p>
<div class="highlight"><pre><span></span><code>0:     [ 0.11, -0.07,  0.45,  0.02, -0.33,  0.19, -0.48,  0.05]
1:     [-0.21,  0.34, -0.11, -0.08,  0.27, -0.39,  0.17, -0.43]
2:     [ 0.09,  0.13,  0.28, -0.47, -0.36,  0.22,  0.41, -0.18]
3:     [-0.15,  0.54,  0.28,  0.12, -0.41, -0.41, -0.53,  0.44]
4:     [ 0.12,  0.25, -0.58,  0.56,  0.4,  -0.35, -0.38, -0.38]
5:     [-0.23,  0.03, -0.08, -0.25,  0.13, -0.43, -0.25, -0.16]
...
73:    [ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27]
...
786:   [-0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04]
...
2934:  [ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33]
...
</code></pre></div>
<hr />
<p><strong>Step 3. Lookup</strong>  </p>
<p>Now, to embed our sentence <code>[73, 786, 2934]</code>, the embedding layer simply <strong>selects the rows</strong> at those indices:  </p>
<ul>
<li>Token ID <strong>73 (“I”)</strong> → <code>[ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27 ]</code>  </li>
<li>Token ID <strong>786 (“love”)</strong> → <code>[ -0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04 ]</code>  </li>
<li>Token ID <strong>2934 (“dogs”)</strong> → <code>[ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33 ]</code>  </li>
</ul>
<hr />
<p><strong>Step 4. Output Tensor</strong>  </p>
<p>Stacking them together, the embedding layer outputs a tensor:  </p>
<div class="highlight"><pre><span></span><code>[
  [ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27 ],   # &quot;I&quot;
  [ -0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04 ], # &quot;love&quot;
  [ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33 ]   # &quot;dogs&quot;
]
</code></pre></div>
<p>Shape = <code>(3, 8)</code> → 3 tokens, each represented by an 8-dimensional vector.</p>
<p>Essentially, given an input 1d tensor of tokens, which the number of tokens is often referred to as <code>(seq_len,)</code>,
we transform it into a tensor of shape <code>(seq_len, n_embd)</code> </p>
<p>In this example, it is <code>(3, 8)</code></p>
<p>This is the format that gets passed on to the Decoder Block. </p>
<p>A very important note is that there's almost always a third dimension, called a Batch dimension. 
This allows parallel processing, which makes training much faster. 
Batch dimension is always the very first dimension, so the shape of output tensor is <code>(batch, seq_len, n_embd)</code>
In this case, since we only have a single example sentence, batch dimension value would be 1, which is</p>
<p><code>(1, 3, 8)</code></p>
<h2 id="model_architectureend_to_endmd">model_architecture\end_to_end.md</h2>
<h1 id="end-to-end-walkthrough">End-to-End Walkthrough</h1>
<p>Now at this point, most of the aspects of the architecture and pipeline have been covered in detail.<br />
This final page will be used to <strong>tie everything together</strong> and give a more thorough, step-by-step overview of how all the parts interact to form a working large language model.  </p>
<hr />
<h2 id="1-from-raw-text-to-tokens">1. From Raw Text to Tokens</h2>
<p>We first start out with a <strong>massive corpus of text</strong> — think in the scale of hundreds of gigabytes upwards, containing billions or even trillions of tokens.<br />
This corpus is gathered from sources like books, Wikipedia, academic papers, code repositories, and curated parts of the internet.  </p>
<p>But raw text isn’t useful to the model. The model only works with numbers, so the very first step is to <strong>tokenize</strong> this text using a pretrained tokenizer.  </p>
<p>For example, suppose we have the sentence:  </p>
<div class="highlight"><pre><span></span><code>&quot;The quick brown fox jumps over the lazy dog.&quot;
</code></pre></div>
<p>The tokenizer will break this into smaller units (subwords or characters depending on the algorithm) and then convert each into an integer ID.  </p>
<p>That means something like:  </p>
<div class="highlight"><pre><span></span><code>[&quot;The&quot;, &quot; quick&quot;, &quot; brown&quot;, &quot; fox&quot;, &quot; jumps&quot;, &quot; over&quot;, &quot; the&quot;, &quot; lazy&quot;, &quot; dog&quot;, &quot;.&quot;]
→ [1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209]
</code></pre></div>
<p>Now the sentence is represented as a sequence of integers.<br />
This is the form that the neural network can actually process.  </p>
<hr />
<h2 id="2-batching-and-shaping-the-data">2. Batching and Shaping the Data</h2>
<p>Instead of feeding one sentence at a time, training uses <strong>mini-batches</strong> to process many sequences in parallel.<br />
This is crucial for efficiency on GPUs/TPUs.  </p>
<p>Suppose we have a long stream of tokens like:  </p>
<div class="highlight"><pre><span></span><code>[1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209, 954, 4461, 3546, 206, 4401, ...]
</code></pre></div>
<p>If we set:</p>
<ul>
<li><code>batch_size = 2</code>  </li>
<li><code>seq_len = 6</code>  </li>
</ul>
<p>We would take the first <code>batch_size * seq_len = 12</code> tokens and reshape them into a <code>(batch, seq_len)</code> tensor:  </p>
<div class="highlight"><pre><span></span><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1202</span><span class="p">,</span> <span class="mi">850</span><span class="p">,</span> <span class="mi">149</span><span class="p">,</span> <span class="mi">4211</span><span class="p">,</span> <span class="mi">769</span><span class="p">,</span> <span class="mi">1839</span><span class="p">,</span> <span class="mi">3521</span><span class="p">,</span> <span class="mi">4879</span><span class="p">,</span> <span class="mi">2035</span><span class="p">,</span> <span class="mi">1209</span><span class="p">,</span> <span class="mi">954</span><span class="p">,</span> <span class="mi">4461</span><span class="p">,</span> <span class="mi">3546</span><span class="p">,</span> <span class="mi">206</span><span class="p">,</span> <span class="mi">4401</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

<span class="c1"># When printed, input_tokens would look like:</span>
<span class="c1"># tensor([[1202,  850,  149, 4211,  769, 1839],</span>
<span class="c1">#         [3521, 4879, 2035, 1209,  954, 4461]])</span>
</code></pre></div>
<p>This reshaped tensor is now ready for the model.  </p>
<p>In realistic training runs, values are much larger, e.g.:<br />
- <code>batch_size = 32</code> (process 32 sequences in parallel)<br />
- <code>seq_len = 2048</code> (each sequence is 2048 tokens long)  </p>
<p>So the model processes a tensor of shape <code>(32, 2048)</code> in one forward pass.  </p>
<hr />
<h2 id="3-passing-through-the-transformer-model">3. Passing Through the Transformer Model</h2>
<p>Next, this tensor of token IDs is passed into the <strong>transformer model</strong>.<br />
The model is composed of the architecture we previously touched upon in detail: embeddings, attention, feedforward networks, normalization, and residual connections, all stacked together into many decoder blocks.  </p>
<p>Here is the structure of the <code>LLaMaTransformer</code> class that uses all the previous building blocks:  </p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">LLaMaTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">config</span><span class="p">:</span> <span class="nb">any</span><span class="p">,</span>
                 <span class="n">tokenizer</span><span class="p">:</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">,</span>
                 <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># === Unpack config ===</span>
        <span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_seq_len</span>
        <span class="n">n_embd</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span>
        <span class="n">n_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="n">n_layers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_layers</span>
        <span class="n">multiple_of</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">multiple_of</span>
        <span class="c1"># And many more, will omit for documentation</span>

        <span class="k">assert</span> <span class="n">n_embd</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;n_embd (</span><span class="si">{</span><span class="n">n_embd</span><span class="si">}</span><span class="s2">) % n_heads (</span><span class="si">{</span><span class="n">n_heads</span><span class="si">}</span><span class="s2">) must equal 0!&quot;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_heads</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">qk_rope_head_dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;head_dim must be even for RoPE!&quot;</span>


        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">(),</span> <span class="n">n_embd</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">DecoderBlock</span><span class="p">(</span><span class="n">n_embd</span><span class="o">=</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">multiple_of</span><span class="o">=</span><span class="n">multiple_of</span><span class="p">,</span> <span class="n">use_mla</span><span class="o">=</span><span class="n">use_mla</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">(),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">h_dim</span> <span class="o">=</span> <span class="n">qk_rope_head_dim</span> <span class="k">if</span> <span class="n">use_mla</span> <span class="k">else</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">freq_complex</span> <span class="o">=</span> <span class="n">precompute_theta_pos_frequencies</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># (batch, seq_len) -&gt; (batch, seq_len, n_embd)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Each token ID mapped to vector</span>
        <span class="n">freqs_complex</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freq_complex</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]</span>

        <span class="c1"># Pass through all Decoder Blocks</span>
        <span class="k">for</span> <span class="n">dec_block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">:</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">dec_block</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">freqs_complex</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_linear</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>  <span class="c1"># (batch, seq_len, n_embd) -&gt; (batch, seq_len, vocab_size)</span>
</code></pre></div>
<hr />
<h2 id="4-step-by-step-inside-the-model">4. Step-by-Step Inside the Model</h2>
<h3 id="41-embedding-layer">4.1 Embedding Layer</h3>
<ul>
<li>Input: <code>(batch, seq_len)</code> of integers.  </li>
<li>Each integer token is mapped into a dense vector of length <code>n_embd</code>.  </li>
<li>Output: <code>(batch, seq_len, n_embd)</code>  </li>
</ul>
<p>This is the <strong>semantic representation</strong> of tokens: instead of just IDs, we now have vectors that carry meaning and relationships.  </p>
<h3 id="42-positional-information">4.2 Positional Information</h3>
<p>We then fetch <code>freqs_complex</code>, which stores precomputed values for <strong>Rotary Position Embeddings (RoPE)</strong>.<br />
RoPE encodes the positions of tokens into the attention mechanism, so the model knows order (e.g., difference between “dog bites man” and “man bites dog”).  </p>
<h3 id="43-decoder-blocks">4.3 Decoder Blocks</h3>
<p>The embedded tensor is then passed into the first <strong>Decoder Block</strong>.<br />
Each block applies:<br />
- <strong>RMSNorm</strong> → normalizes values for stability.<br />
- <strong>Multi-Head Attention</strong> → lets each token attend to others in the sequence.<br />
- <strong>Feedforward Network (SwiGLU)</strong> → adds nonlinear transformation capacity.<br />
- <strong>Residual Connections</strong> → add the original input back to preserve information.  </p>
<p>The internal computations change the <strong>contents</strong> of the tensor but not its shape: it remains <code>(batch, seq_len, n_embd)</code>.  </p>
<p>The output of block 1 is passed into block 2, then block 3, and so on, until it passes through all <code>n_layers</code>.  </p>
<h3 id="44-final-normalization">4.4 Final Normalization</h3>
<p>After the last decoder block, the tensor goes through one final <strong>RMSNorm layer</strong>.<br />
This ensures the distribution of activations is stable before the final projection.  </p>
<h3 id="45-final-linear-layer">4.5 Final Linear Layer</h3>
<p>Finally, we project each hidden vector of size <code>n_embd</code> into a vector the size of the vocabulary.<br />
- Shape: <code>(batch, seq_len, n_embd) → (batch, seq_len, vocab_size)</code>  </p>
<p>This gives <strong>logits</strong> — raw scores for each token in the vocabulary.  </p>
<hr />
<h2 id="5-from-logits-to-predictions">5. From Logits to Predictions</h2>
<p>The logits are not yet probabilities. To turn them into probabilities, we apply <strong>softmax</strong> across the vocabulary dimension:  </p>
<div class="highlight"><pre><span></span><code>probabilities = softmax(logits, dim=-1)
</code></pre></div>
<p>Now, for each position in the sequence, we get a probability distribution over the vocabulary.<br />
For example:  </p>
<div class="highlight"><pre><span></span><code>At position 6, &quot;fox&quot; might have 0.72 probability, &quot;dog&quot; 0.05, &quot;cat&quot; 0.02, ...
</code></pre></div>
<p>During training, we compare these probabilities against the actual next token using <strong>cross-entropy loss</strong>.<br />
This loss guides backpropagation, which updates the model weights via gradient descent.  </p>
<hr />
<h2 id="6-training-loop-connection">6. Training Loop Connection</h2>
<p>To summarize the <strong>training loop</strong> connection:  </p>
<ol>
<li>Start with batch of token sequences.  </li>
<li>Forward pass through embeddings → decoder blocks → normalization → linear layer.  </li>
<li>Produce logits <code>(batch, seq_len, vocab_size)</code>.  </li>
<li>Apply softmax to get probabilities.  </li>
<li>Compute loss vs. ground truth next tokens.  </li>
<li>Backpropagate gradients.  </li>
<li>Update weights with optimizer (AdamW, etc.).  </li>
<li>Repeat across billions of tokens until the model converges.  </li>
</ol>
<p>Over time, the model gradually learns grammar, facts, and semantics purely by predicting the next token.  </p>
<hr />
<h2 id="key-takeaway_1">Key Takeaway</h2>
<p>This end-to-end flow shows how everything connects:<br />
- <strong>Tokenization</strong> converts raw text into IDs.<br />
- <strong>Embeddings + RoPE</strong> give meaning and order.<br />
- <strong>Decoder Blocks</strong> repeatedly transform and refine the representations.<br />
- <strong>Final Linear Layer + Softmax</strong> produce predictions over the vocabulary.<br />
- <strong>Loss and Optimization</strong> allow the model to learn from its mistakes.  </p>
<p>By stacking these stages together, and scaling up with billions of tokens and parameters, we arrive at a large language model capable of generating coherent and context-aware text.  </p>
<h2 id="model_architecturefeedforwardmd">model_architecture\feedforward.md</h2>
<h1 id="feedforward-networks-ffn-in-transformers">Feedforward Networks (FFN) in Transformers</h1>
<p>When people first learn about Transformers, the attention mechanism usually takes the spotlight.<br />
But the <strong>feedforward network (FFN)</strong> is equally important — in fact, it often accounts for the majority of parameters in the model.  </p>
<hr />
<h2 id="why-do-we-need-feedforward-layers">Why Do We Need Feedforward Layers?</h2>
<p>Attention layers are powerful, but they are still fundamentally <strong>linear operations</strong> (matrix multiplications, weighted sums).<br />
A stack of only linear layers would remain a linear model, which cannot approximate complex, nonlinear functions.  </p>
<p>The feedforward network adds <strong>nonlinearity</strong> and <strong>capacity to transform information</strong>.<br />
It allows the model to map representations into a higher-dimensional space, apply nonlinear activation, and then project back down.  </p>
<p>In practice, every Transformer block has the structure:  </p>
<div class="highlight"><pre><span></span><code>Input → [Attention] → [Feedforward] → Output
</code></pre></div>
<p>Both attention and FFN are wrapped with normalization and residual connections.</p>
<hr />
<h2 id="vanilla-transformer-ffn">Vanilla Transformer FFN</h2>
<p>In the original Transformer paper (Vaswani et al., 2017), the FFN was defined as:  </p>
<div class="highlight"><pre><span></span><code>FFN(x) = max(0, xW1 + b1)W2 + b2
</code></pre></div>
<ul>
<li>Two linear layers with a ReLU in between.  </li>
<li>The hidden dimension is usually set to <strong>4× the embedding dimension</strong>, then projected back down.  </li>
</ul>
<p>For example, if embedding size = 1024, the FFN hidden size = 4096.  </p>
<p>This “expand and contract” pattern gives the model strong nonlinear mixing power, because the network has a wide layer to mix features, then projects it back to the model’s working dimension.  </p>
<h3 id="implementation-example">Implementation Example</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</code></pre></div>
<p>Let’s walk through this carefully:<br />
1. Input tensor <code>x</code> has shape <code>(batch, seq_len, n_embd)</code>.<br />
2. First layer projects it into <code>(batch, seq_len, 4 * n_embd)</code>.<br />
3. Apply ReLU to introduce non-linearity (important because without it, stacking linear layers would still just be linear).<br />
4. Second layer projects it back down to <code>(batch, seq_len, n_embd)</code>.  </p>
<p>So while the shape going into and out of the FFN is the same, the hidden computation in between allows the network to express far richer functions.</p>
<hr />
<h2 id="llama-style-ffn-swiglu">LLaMA-Style FFN (SwiGLU)</h2>
<p>The LLaMA architecture introduced a key modification: instead of the plain ReLU-based FFN, it uses a <strong>SwiGLU activation</strong> (SiLU-Gated Linear Unit).  </p>
<p>Here’s how it looks in code:  </p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">multiple_of</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># Authors of LLaMa used 2/3 of 4*n_embd</span>
        <span class="c1"># Round hidden_dim up to a nicer multiple for efficient GPU utilization</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">multiple_of</span> <span class="o">*</span> <span class="p">((</span><span class="n">hidden_dim</span> <span class="o">+</span> <span class="n">multiple_of</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">multiple_of</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># SwiGLU block</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
</code></pre></div>
<h3 id="breaking-it-down-step-by-step">Breaking It Down Step by Step</h3>
<ul>
<li>In the vanilla FFN we had <strong>two linear layers</strong>.  </li>
<li>In LLaMA’s FFN we now have <strong>three linear layers</strong>. Why? To introduce a <strong>gating mechanism</strong>.  </li>
</ul>
<p>The formula looks like this:  </p>
<div class="arithmatex">\[
h = \text{SiLU}(W_1x) \odot (W_3x)
\]</div>
<p>where <code>⊙</code> is elementwise (Hadamard) multiplication.  </p>
<ul>
<li><code>W1x</code>: main transformation path.  </li>
<li>Apply SiLU activation (smooth, like ReLU but differentiable everywhere).  </li>
<li><code>W3x</code>: produces a second transformed version of <code>x</code>, used as a <strong>gate</strong>.  </li>
<li>Multiply them elementwise: the gate decides how much of each hidden unit passes through.  </li>
</ul>
<p>This means:<br />
- If a value in <code>W3x</code> is near 0 → the signal from <code>W1x</code> gets suppressed.<br />
- If large → it amplifies the signal.<br />
- If negative → it can flip the sign of the signal.  </p>
<p>So the gating function lets the model regulate the flow of information more flexibly than a simple ReLU cutoff.</p>
<h3 id="parameter-balancing">Parameter Balancing</h3>
<p>But wait — adding a third projection layer means more parameters, right?<br />
Yes, but the authors balanced this by shrinking the hidden dimension.  </p>
<ul>
<li>Vanilla FFN parameters: about <code>8 * n_embd²</code>.<br />
  (from <code>(n_embd × 4n_embd) + (4n_embd × n_embd)</code>).  </li>
<li>LLaMA FFN: uses hidden_dim = <code>int(4 * n_embd * 2/3)</code>.<br />
  With three projections (<code>w1</code>, <code>w2</code>, <code>w3</code>), total ≈ <code>8 * n_embd²</code>.  </li>
</ul>
<p>So both end up with roughly the same parameter budget, but LLaMA gets more expressive power via gating.  </p>
<h3 id="shapes-in-action">Shapes in Action</h3>
<ul>
<li>Input: <code>(batch, seq_len, n_embd)</code>  </li>
<li>After <code>w1</code> and <code>w3</code>: <code>(batch, seq_len, hidden_dim)</code>  </li>
<li>After SiLU + elementwise product: <code>(batch, seq_len, hidden_dim)</code>  </li>
<li>After <code>w2</code>: <code>(batch, seq_len, n_embd)</code>  </li>
</ul>
<p>Input and output shapes match the original — only the internal transformation is richer.  </p>
<h3 id="multiple-of-trick">Multiple-of Trick</h3>
<p>The <code>multiple_of</code> hyperparameter ensures <code>hidden_dim</code> is divisible by a convenient number (like 64, 128, 256).<br />
This is purely for GPU efficiency: matrix multiplications run faster on dimensions aligned to powers of two.  </p>
<p>For example:<br />
- Without adjustment: <code>n_embd=1024</code> → hidden_dim = 2730.<br />
- With <code>multiple_of=128</code>: hidden_dim becomes 2816, which is far more efficient for hardware.  </p>
<h3 id="dropout">Dropout</h3>
<p>At the end, dropout is applied. This isn’t heavily used during large-scale pretraining, but it becomes important in fine-tuning (SFT, RLHF) to regularize and prevent overfitting when datasets are smaller.  </p>
<hr />
<h2 id="summary_1">Summary</h2>
<ul>
<li>Feedforward layers provide the <strong>nonlinear expressiveness</strong> that attention alone cannot.  </li>
<li>Vanilla Transformer FFN: <strong>Linear → ReLU → Linear</strong>, with a 4× expansion.  </li>
<li>LLaMA FFN: <strong>SwiGLU-style</strong> (SiLU + gating with a third linear layer).  </li>
<li>Gating allows richer feature modulation (suppress, amplify, flip).  </li>
<li>Parameter count is balanced to stay ~<code>8 * n_embd²</code>.  </li>
<li>Input/output shapes stay the same, but the internal computation is more expressive.  </li>
<li>Most of a Transformer’s parameters live in these FFNs — they are just as crucial as attention.  </li>
</ul>
<h2 id="model_architecturenormalizationmd">model_architecture\normalization.md</h2>
<h1 id="normalization">Normalization</h1>
<p>When training deep neural networks, one of the recurring problems is that activations can either <strong>explode</strong> (grow without bound) or <strong>vanish</strong> (shrink toward zero) as they pass through many layers. This makes optimization unstable and slows down learning.  </p>
<p>Another issue is <strong>internal covariate shift</strong> — the idea that the distribution of activations keeps changing as each layer is updated during training, which makes it harder for later layers to adapt.  </p>
<p>Internal Covariate Shift can be thought of as follows: </p>
<p>Imagine you’re a student preparing for math exams.  </p>
<ul>
<li>On the first day, you get an <strong>Algebra I exam</strong>. You do your best, turn it in, get feedback from the teacher, and feel ready to improve for the next one.  </li>
<li>Based on the feedback, you adjust your study strategy and expect another Algebra test.  </li>
<li>But the next exam you receive is suddenly <strong>Calculus</strong> — a totally different level of difficulty. All the preparation you just did no longer matches what you’re being tested on.  </li>
</ul>
<p>This is what happens in deep neural networks without normalization.<br />
Each hidden layer is like a student preparing for its next “exam” (the next round of inputs). After every weight update, the <strong>distribution of outputs from the previous layer shifts</strong>. That means the “exam” the next layer sees can suddenly look very different than what it was trained on.  </p>
<p>As the network gets deeper, these unexpected shifts <strong>compound</strong>, making training unstable. Layers spend more time re-adapting to constantly changing input distributions rather than actually learning useful features.</p>
<p><strong>Normalization layers</strong> (like BatchNorm, LayerNorm, etc.) fix this problem.<br />
They act like a teacher who ensures that every new exam stays at the same Algebra level — same general difficulty, same type of questions — just slightly adjusted each time. This consistency allows each layer to steadily improve rather than getting thrown off by wild distribution shifts.</p>
<p>In short:<br />
- <em>Without normalization</em>: “I prepared for Algebra, but got Calculus.”<br />
- <em>With normalization</em>: “I keep getting Algebra, just with different numbers.”  </p>
<p>Normalization layers stabilize the distribution of activations, keep gradients more predictable, and generally allow networks to train deeper and faster.</p>
<hr />
<h2 id="layer-normalization-layernorm">Layer Normalization (LayerNorm)</h2>
<p>In Transformers (like the original paper “Attention Is All You Need”), the normalization method of choice was <strong>Layer Normalization (LayerNorm)</strong>.</p>
<p>How it works:<br />
- Given a tensor <code>x</code> of shape <code>(batch, seq_len, n_embd)</code>, LayerNorm normalizes <strong>across the embedding dimension</strong> for each token.<br />
- For each token vector, it computes the mean and variance across its <code>n_embd</code> values.<br />
- The normalized vector is then scaled and shifted by learnable parameters (<code>gamma</code> and <code>beta</code>).  </p>
<p>Mathematically:</p>
<div class="highlight"><pre><span></span><code>LayerNorm(x) = gamma * (x - mean(x)) / sqrt(var(x) + eps) + beta
</code></pre></div>
<p>Where:<br />
- <code>mean(x)</code> and <code>var(x)</code> are computed over the last dimension (<code>hidden_dim</code>).<br />
- <code>gamma</code> and <code>beta</code> are learnable parameters that allow the model to "undo" normalization if needed.<br />
- <code>eps</code> is a small constant to prevent division by zero.  </p>
<p>Effectively, LayerNorm <strong>re-centers</strong> (subtracts the mean) and <strong>re-scales</strong> (divides by standard deviation). This ensures each token’s hidden vector has roughly zero mean and unit variance before being rescaled.</p>
<p>LayerNorm is still widely used in most Transformer implementations, but it comes with a computational cost: subtracting means, computing variances, and performing square roots for every vector.</p>
<hr />
<h2 id="root-mean-square-normalization-rmsnorm">Root Mean Square Normalization (RMSNorm)</h2>
<p>LLaMA and some later models (including this implementation) instead use <strong>RMSNorm</strong>, a simpler but surprisingly effective alternative.</p>
<p>The key idea:<br />
Research showed that <em>re-centering</em> (subtracting the mean) is less important than <em>re-scaling</em> (fixing the variance). In other words, what really stabilizes activations is making sure their magnitude (energy) doesn’t blow up, not whether they’re mean-centered.</p>
<p>So RMSNorm skips the mean subtraction entirely.</p>
<p>Mathematically:</p>
<div class="highlight"><pre><span></span><code>RMSNorm(x) = (x / RMS(x)) * weight
</code></pre></div>
<p>Where:<br />
- <code>RMS(x) = sqrt(mean(x^2))</code><br />
- <code>weight</code> is a learnable scaling vector (similar to <code>gamma</code> in LayerNorm).<br />
- No <code>beta</code>, since there’s no re-centering.  </p>
<p>This implementation of <code>RMSNorm</code> shows this clearly:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">RMSNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_embd</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<p>Step by step:<br />
1. Square all elements (<code>x.pow(2)</code>).<br />
2. Take the mean across the last dimension (<code>mean(-1, keepdim=True)</code>).<br />
3. Add a tiny epsilon for numerical stability.<br />
4. Take the reciprocal square root (<code>rsqrt</code>).<br />
5. Multiply back with the original <code>x</code> → now the activations are normalized to unit RMS.<br />
6. Finally, scale by a learnable weight vector (one parameter per hidden dimension).  </p>
<p>This achieves the stabilization effect of LayerNorm but with slightly fewer operations.</p>
<hr />
<h2 id="why-rmsnorm">Why RMSNorm?</h2>
<p>So why is RMSNorm used in models like LLaMA (and this project)?</p>
<ol>
<li>
<p><strong>Efficiency</strong><br />
   RMSNorm is simpler than LayerNorm. It removes the mean subtraction, which slightly reduces compute and memory usage — especially important when running at very large scales.</p>
</li>
<li>
<p><strong>Empirical stability</strong><br />
   Experiments (<a href="https://arxiv.org/pdf/1910.07467">see the RMSNorm paper</a>) showed that mean-centering didn’t improve stability much. The key factor was scaling by the variance (or root mean square).</p>
</li>
<li>
<p><strong>Better fit for Transformers</strong><br />
   Since Transformers already have residual connections and other stabilizing tricks, skipping the mean step doesn’t hurt — and in fact, models trained with RMSNorm often match or exceed performance of LayerNorm.</p>
</li>
</ol>
<hr />
<h2 id="putting-it-together">Putting It Together</h2>
<p>In this project, RMSNorm appears inside the <strong>Decoder Block</strong> in two places:</p>
<div class="highlight"><pre><span></span><code><span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">freqs_complex</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffwd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
</code></pre></div>
<p>Here, <code>norm1</code> and <code>norm2</code> are both <code>RMSNorm</code> layers. They normalize activations before passing them into attention and feedforward sublayers. This is known as a <strong>Pre-Norm Transformer</strong> design (normalize before the sublayer, then add the residual). Pre-Norm improves gradient flow compared to the original Post-Norm design.</p>
<hr />
<h2 id="summary_2">Summary</h2>
<ul>
<li>Normalization stabilizes deep networks, preventing exploding/vanishing activations.  </li>
<li>Transformers originally used <strong>LayerNorm</strong>, which re-centers and re-scales each hidden vector.  </li>
<li><strong>RMSNorm</strong> drops the mean subtraction, keeping only the re-scaling step.  </li>
<li>Despite being simpler, RMSNorm works just as well (sometimes better) and is used in LLaMA and your implementation.  </li>
<li>In SimpleLLaMA, RMSNorm ensures stable training across all decoder blocks while keeping the implementation lightweight.  </li>
</ul>
<h2 id="model_architectureoverviewmd">model_architecture\overview.md</h2>
<h1 id="model-architecture-overview">Model Architecture Overview</h1>
<p>At the heart of SimpleLLaMA is the <strong>transformer architecture</strong> — the same family of models that power GPT, LLaMA, and DeepSeek.<br />
Transformers are flexible neural networks designed to process sequences of data (like text), and they’ve become the standard for large language models.</p>
<hr />
<h2 id="big-picture">Big Picture</h2>
<p>When you give the model a prompt, like:</p>
<p><code>"The cat sat on the mat"</code></p>
<p>the pipeline looks like this:<br />
~<br />
1. <strong>Tokenization</strong> – Text is broken into tokens (e.g., "The", " cat", " sat", " on", " the", " mat").<br />
2. <strong>Embeddings</strong> – Each token is turned into a dense vector representation.<br />
3. <strong>Transformer Blocks</strong> – A stack of repeated layers where the real learning happens:<br />
   - <strong>Attention</strong> → figures out relationships between tokens.<br />
   - <strong>Feedforward</strong> → transforms and mixes the information.<br />
   - <strong>Residuals + Normalization</strong> → stabilize and speed up training.<br />
4. <strong>Output Projection</strong> – Final layer maps hidden states back to the vocabulary.<br />
5. <strong>Softmax</strong> – Converts raw scores into probabilities for the next token.  </p>
<p><img alt="GPT VS LLaMA Architecture" src="../../images/llama_architecture.png" /></p>
<p>Should add a reference to Umar! </p>
<hr />
<h2 id="decoder-only-design">Decoder-Only Design</h2>
<p>This project uses a <strong>decoder-only transformer</strong>, which means:<br />
- The model only predicts the <em>next</em> token given all tokens before it.<br />
- It’s autoregressive: it generates text left to right, one token at a time.<br />
- This design is perfect for language modeling, where the task is “predict what comes next.”</p>
<hr />
<h2 id="why-transformers">Why Transformers?</h2>
<p>Transformers replaced older sequence models (RNNs, LSTMs) because:
- They scale much better with data and compute.<br />
- Attention allows the model to directly connect distant tokens (e.g., the start and end of a paragraph).<br />
- Parallelization makes them efficient to train on GPUs.</p>
<hr />
<p>In the following sections, we’ll break the model down into its core components:<br />
- <strong>Embeddings</strong> – how tokens are represented as vectors.<br />
- <strong>Attention</strong> – how the model connects words together.<br />
- <strong>Layer Block</strong> – the repeating unit of the transformer.<br />
- <strong>Output</strong> – how predictions are made.  </p>
<h2 id="tokenizationalgorithmsmd">tokenization\algorithms.md</h2>
<h2 id="training-a-tokenizer">Training a Tokenizer</h2>
<p>A tokenizer isn’t just a simple rule that splits text on spaces or punctuation — it’s actually trained on a large text corpus, very much like the LLM itself is trained on text.<br />
The idea is to learn how to break down words into pieces in a way that balances efficiency and flexibility.</p>
<p>There are different algorithms to do this, but the two most common are:</p>
<ul>
<li><strong>BPE (Byte Pair Encoding)</strong></li>
<li><strong>SentencePiece (often using Unigram LM)</strong></li>
</ul>
<p>In this section, we’ll focus on BPE because it’s easier to walk through step by step and is widely used in models like GPT-2.</p>
<hr />
<h2 id="example-bpe-in-action">Example: BPE in Action</h2>
<p>Let’s say our toy text corpus is:</p>
<p>"I love my banana bandana"</p>
<p>We’ll show how BPE gradually learns to compress repeated character pairs into new tokens.</p>
<hr />
<h3 id="step-1-characters-bytes">Step 1. Characters → Bytes</h3>
<p>Everything starts with raw characters. BPE first converts each character into its ASCII/UTF-8 byte value.</p>
<div class="highlight"><pre><span></span><code><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;I love my banana bandana&quot;</span>
<span class="k">for</span> <span class="n">character</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="n">character</span><span class="p">))</span>
</code></pre></div>
<p>Resulting sequence of numbers:</p>
<p><code>[73, 32, 108, 111, 118, 101, 32, 109, 121, 32, 98, 97, 110, 97, 110, 97, 32, 98, 97, 110, 100, 97, 110, 97]</code></p>
<p>Here:<br />
"I" -&gt; 73<br />
" " -&gt; 32<br />
"l" -&gt; 108<br />
"o" -&gt; 111<br />
"v" -&gt; 118<br />
"e" -&gt; 101<br />
...  </p>
<p>So now the sentence is just a list of numbers.</p>
<hr />
<h3 id="step-2-count-frequent-pairs">Step 2. Count frequent pairs</h3>
<p>BPE works by repeatedly finding the <strong>most common adjacent pair of symbols</strong> and merging it into a new symbol.<br />
We start with a sliding window of size 2 and count all pairs in the byte list.</p>
<div class="highlight"><pre><span></span><code><span class="n">count</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">byte_repr</span> <span class="o">=</span> <span class="p">[</span><span class="mi">73</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">108</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="mi">118</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">109</span><span class="p">,</span> <span class="mi">121</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> 
             <span class="mi">98</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">98</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">97</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">byte_repr</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">byte_repr</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">sorted_keys</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">count</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">sorted_keys</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> : </span><span class="si">{</span><span class="n">count</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<p>The most frequent pair is:</p>
<p>(97, 110) : 4</p>
<p>This corresponds to the two characters "a" (97) and "n" (110), which together form "an".<br />
Looking at our sentence, “banana bandana,” it makes sense: "an" repeats a lot.</p>
<hr />
<h3 id="step-3-merge-and-replace">Step 3. Merge and replace</h3>
<p>We now assign a new token ID for "an", say 256.<br />
Then we replace all occurrences of the pair (97, 110) with this new symbol:</p>
<p><code>[73, 32, 108, 111, 118, 101, 32, 109, 121, 32, 98, 256, 256, 97, 32, 98, 256, 100, 256, 97]</code></p>
<p>The list is shorter — we compressed the repeated "an" pairs.</p>
<hr />
<h3 id="step-4-repeat-the-process">Step 4. Repeat the process</h3>
<p>We do the same thing again: scan for the most frequent pair, merge, and replace.  </p>
<p>Now the top pairs would be:<br />
- (32, 98) : 2<br />
- (98, 256) : 2<br />
- (256, 97) : 2  </p>
<p>Let’s pick <code>(32, 98)</code> first. Remember, 32 is the ASCII code for a space <code>" "</code>, and 98 is <code>"b"</code>. Together they represent <code>" b"</code>.<br />
We can merge them into a new token ID, say <code>257</code>, so now the tokenizer knows <code>" b"</code> is a single symbol.  </p>
<p>If we run the process again, the frequent pairs update. Now we see pairs like:<br />
- (257, 256) : 2<br />
- (256, 97) : 2  </p>
<p>This means <code>" b"</code> (257) is often followed by <code>"an"</code> (256), which together form <code>" ban"</code>.<br />
We can merge <code>(257, 256)</code> into <code>258</code>, representing <code>" ban"</code>.  </p>
<p>Each time we merge, the sequence gets shorter and starts capturing bigger chunks of meaning. By repeating this again and again, we eventually build up tokens for frequent pieces of words, until we reach the target vocabulary size.</p>
<hr />
<h3 id="step-5-maintain-reverse-mapping">Step 5. Maintain reverse mapping</h3>
<p>For decoding back into text, the tokenizer keeps track of a reverse dictionary that remembers what each new token stands for.</p>
<p>For example:</p>
<p>73  -&gt; "I"<br />
32  -&gt; " "<br />
108 -&gt; "l"<br />
111 -&gt; "o"<br />
118 -&gt; "v"<br />
101 -&gt; "e"<br />
...<br />
256 -&gt; "an"<br />
257 -&gt; " b"<br />
258 -&gt; " ban"  </p>
<p>When we want to reconstruct text from tokens, the tokenizer looks up these mappings.</p>
<hr />
<h3 id="final-tokenization">Final Tokenization</h3>
<p>With this toy tokenizer, the original sentence:</p>
<p>"I love my banana bandana"</p>
<p>might end up tokenized as:</p>
<p>["I", " ", "l", "o", "v", "e", " ", "m", "y", " ban", "an", "a", " ban", "d", "an", "a"]</p>
<p>It’s not very compressed at this tiny scale, but notice how frequent chunks like "an" and " ban" became their own tokens.  </p>
<hr />
<h2 id="recap">Recap</h2>
<p>BPE builds a tokenizer by:<br />
1. Starting with characters as symbols.<br />
2. Repeatedly finding the most frequent adjacent pair.<br />
3. Merging that pair into a new token.<br />
4. Updating the text and repeating until the vocabulary reaches a set size.  </p>
<p>The result is a <strong>subword tokenizer</strong> that’s much more efficient than character-level and word-level tokenization, which will be discussed in the next section.<br />
This balance is why BPE (or related methods like Unigram LM) became the standard in modern LLMs.</p>
<h2 id="tokenizationexamplesmd">tokenization\examples.md</h2>
<h2 id="tokenization-examples">Tokenization Examples</h2>
<p>In this section, we’ll look at some more concrete examples of how tokenization works in practice, and why subword tokenization ends up being the most effective choice for LLMs.</p>
<hr />
<h3 id="character-level-tokenization">Character-Level Tokenization</h3>
<p>Example sentence:<br />
"Hello, how are you doing today?"</p>
<p>At the character level, this becomes:<br />
["H","e","l","l","o",","," ","h","o","w"," ","a","r","e"," ","y","o","u"," ","d","o","i","n","g"," ","t","o","d","a","y","?"]</p>
<p>That’s 31 tokens for a short sentence!  </p>
<p><strong>Pros:</strong><br />
- Very simple, no training required.<br />
- Small vocabulary (if limited to ASCII, just 128 possible tokens).  </p>
<p><strong>Cons:</strong><br />
- Extremely inefficient: every character, space, and punctuation becomes a token.<br />
- Long sequences mean the model uses much more compute to learn the given text.<br />
- Context windows get consumed very quickly. Imagine trying to process an entire book one character at a time — it would explode into millions of tokens.<br />
- Doesn’t scale well to languages with large character sets (Chinese, Korean) where the vocabulary would still be huge and sequences still long.</p>
<hr />
<h3 id="word-level-tokenization">Word-Level Tokenization</h3>
<p>If we instead split on words, we get:<br />
["Hello", "how", "are", "you", "doing", "today", "?"]</p>
<p>Now it’s only 7 tokens — much shorter than the character-level case.  </p>
<p><strong>Pros:</strong><br />
- Far fewer tokens per sentence.<br />
- Faster training since each token carries more semantic meaning.<br />
- Intuitively matches how humans think about words.  </p>
<p><strong>Cons:</strong><br />
- Vocabulary explosion: English alone has 100,000+ words, and that number grows quickly once you consider inflections, misspellings, slang, and technical jargon.<br />
- Out-of-Vocabulary (OOV) errors: any word not seen during training cannot be encoded at inference. For example, if the word “quantumbotics” appears but wasn’t in the training data, the tokenizer simply has no way to represent it.<br />
- Poor handling of morphologically rich languages. For example, Turkish or Finnish can produce many forms of a single root word, each of which would need its own token.<br />
- Non-space-delimited languages (like Chinese or Japanese) are a nightmare for word-level tokenization since there’s no clear way to split words. Tokenizing character-by-character just reverts back to the inefficiency of character-level methods.  </p>
<hr />
<h3 id="subword-level-tokenization">Subword-Level Tokenization</h3>
<p>Subword tokenization finds a balance between the two.<br />
The same sentence could become something like:<br />
["Hello", ",", " how", " are", " you", " do", "ing", " today", "?"]</p>
<p>Now the length is around 9 tokens — not too long, not too short.  </p>
<p><strong>Pros:</strong><br />
- Vocabulary size is controllable (usually 30k–100k tokens).<br />
- Can represent any word, even unseen ones, by splitting into smaller known pieces.<br />
- Captures common roots, prefixes, and suffixes (“walk”, “walking”, “walked” share “walk”).<br />
- Much more efficient than character-level while avoiding the fragility of word-level.<br />
- Works across languages, including ones without spaces.  </p>
<p><strong>Cons:</strong><br />
- More complex to train than character- or word-level.<br />
- Some awkward splits still happen (rare words may be broken down oddly).<br />
- Token boundaries aren’t always human-intuitive.  </p>
<hr />
<h3 id="why-subword-wins">Why Subword Wins</h3>
<p>Character-level forces the model to learn spelling and structure from scratch, while word-level makes the vocabulary too large and brittle. Subword-level combines the strengths of both:<br />
- Compression is strong enough for efficient training.<br />
- Vocabulary is manageable.<br />
- Handles unseen or rare words gracefully.  </p>
<p>This is why nearly every modern LLM — GPT, LLaMA, DeepSeek, etc. — uses a subword tokenizer (BPE, Unigram, or a variant) at its core.</p>
<hr />
<h3 id="final-notes-about-bpe">Final Notes About BPE</h3>
<p>BPE is a simple but highly efficient algorithm for tokenization, but a few additional caveats to be aware of:</p>
<ol>
<li>
<p><strong>Handling non-ASCII characters.</strong><br />
   In multilingual settings, non-ASCII characters may cause problems. If the model predicts invalid continuation tokens, you can end up with “garbled” outputs (invalid byte artifacts).</p>
</li>
<li>
<p><strong>No notion of words.</strong><br />
   BPE works purely on frequency of symbol pairs. It doesn’t know about word boundaries, so sometimes splits can be unintuitive. For example:<br />
   "unhappy" → "un", "hap", "py"<br />
   While unlikely for very common words, this does illustrate the lack of semantic awareness.</p>
</li>
<li>
<p><strong>Inefficient for rare words.</strong><br />
   Since BPE prioritizes frequent patterns, rare words often get split into many tokens.<br />
   Example: "Mississippi" → "Mi", "ssi", "ssi", "ppi" (4 tokens for 1 word).<br />
   Not catastrophic, but inefficient.</p>
</li>
<li>
<p><strong>Whitespace quirks.</strong><br />
   Spaces often get “glued” to words (" dog" instead of "dog"). This is by design, but can look odd.</p>
</li>
<li>
<p><strong>Static once trained.</strong><br />
   Once a BPE tokenizer is trained, its vocabulary is fixed. If new words become common later (say a new slang term or product name), they won’t be merged automatically.</p>
</li>
</ol>
<hr />
<p>For this project, these limitations are minor — but it’s good to be aware of them. Overall, BPE still provides an excellent trade-off and is much better suited for LLMs than raw character- or word-level tokenization.</p>
<h2 id="tokenizationoverviewmd">tokenization\overview.md</h2>
<h2 id="what-is-tokenization">What is Tokenization?</h2>
<p>Now that we have a text corpus to work with, the next step is tokenization. Tokenization is the process of converting text into numerical values, since models can only work with numbers as input. Before a model can learn any patterns in language, it needs the text expressed in a consistent numerical form. Tokenization is the bridge between raw text and the numerical world that neural networks actually understand.</p>
<h2 id="the-big-picture">The Big Picture</h2>
<p>Take the example sentence:</p>
<p><code>"Hawaii is great for vacationing."</code></p>
<p>This same sentence can be broken down in different ways depending on the tokenization approach:</p>
<ul>
<li><strong>Character-level:</strong> <code>["H","a","w","a","i","i"," ","i","s"," ","g","r","e","a","t"," ","f","o","r"," ","v","a","c","a","t","i","o","n","i","n","g","."]</code></li>
<li><strong>Word-level:</strong> <code>["Hawaii","is","great","for","vacationing","."]</code></li>
<li><strong>Subword-level:</strong> <code>["Ha","wa","ii ","is ","great ","for ","vac","ation","ing","."]</code></li>
</ul>
<p>Notice how some words remain whole in the subword case (<code>"is"</code>, <code>"great"</code>, <code>"for"</code>), while others like <code>"Hawaii"</code> and <code>"vacationing"</code> are broken into pieces. Subword tokenization hits the middle ground between characters and words — efficient compression without losing flexibility for new or unusual words.</p>
<h2 id="why-subword-tokenization">Why Subword Tokenization?</h2>
<p>Character-level tokenization is too inefficient (long sequences), while word-level tokenization has problems with exploding vocabulary size and out-of-vocabulary words. Subword tokenization avoids both: it keeps vocabulary manageable while still handling unseen words by splitting them into smaller known pieces. That balance is why modern LLMs almost always use subword tokenization.</p>
<p>So how do we actually decide the splits? That’s where the tokenizer itself comes in. In the next section, we’ll walk through how tokenizers are trained, starting with the most common approach: Byte Pair Encoding (BPE).</p>
<h2 id="tokenizationprojectmd">tokenization\project.md</h2>
<h2 id="tokenization-in-simplellama">Tokenization in SimpleLLaMA</h2>
<p>Now that the dataset has been gathered and sharded, the next step is to actually train a tokenizer and use it to encode the data into numerical form.<br />
This section walks through how this was done in SimpleLLaMA.</p>
<hr />
<h3 id="training-the-tokenizer">Training the Tokenizer</h3>
<p>We use a <strong>ByteLevel BPE</strong> tokenizer for this project, provided by the <code>tokenizers</code> library</p>
<h4 id="special-tokens">Special Tokens</h4>
<p>On top of the usual vocabulary, we add custom tokens to support training:  </p>
<ul>
<li><code>&lt;SOS&gt;</code> : Start of sequence  </li>
<li><code>&lt;EOS&gt;</code> : End of sequence  </li>
<li><code>&lt;PAD&gt;</code> : Padding (for batching sequences of different lengths)  </li>
<li><code>&lt;UNK&gt;</code> : Unknown token (fallback for anything not in vocab)  </li>
<li><code>&lt;SOU&gt;</code> / <code>&lt;EOU&gt;</code> : Mark user messages in dialogue datasets  </li>
<li><code>&lt;SOA&gt;</code> / <code>&lt;EOA&gt;</code> : Mark assistant messages  </li>
<li><code>&lt;SOT&gt;</code> / <code>&lt;EOT&gt;</code> : Mark templates or system prompts  </li>
</ul>
<p>These tokens are crucial for supervised fine-tuning and RLHF stages later on, where we want the model to clearly distinguish between different roles and contexts.</p>
<h4 id="example-script">Example Script</h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">trainers</span>
<span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">ByteLevel</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">BPE</span><span class="p">())</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">ByteLevel</span><span class="p">(</span><span class="n">add_prefix_space</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&lt;SOS&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;EOS&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;PAD&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;UNK&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;SOU&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;EOU&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;SOA&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;EOA&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;SOT&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&lt;EOT&gt;&quot;</span><span class="p">]</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">8192</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span> <span class="n">min_frequency</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;short_0001.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;short_0002.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;short_0003.txt&quot;</span><span class="p">]</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;bpe_8k.json&quot;</span><span class="p">)</span>
</code></pre></div>
<p>Note the <code>vocab_size=8192</code> part. This is a value that we can adjust as needed. A vocabulary size of <code>8192</code> means that we undergo compression until we have 8192 mapping in our dict. 
One can think of it as a balancer: If vocab size is set too low (e.g. 256), BPE will collaspe into character level tokenization. However if vocab size is too large, like 1 million, it will converge towards something like a word level tokenizer. 
Generally, BPE tokenizers have vocabulary size of 32k+, however since we are dealing with ascii only dataset, 8192 works fine. </p>
<h4 id="key-design-choices">Key Design Choices</h4>
<ul>
<li><strong>ByteLevel PreTokenizer</strong> → Ensures consistent handling of whitespace. For example, <code>" dog"</code> and <code>"dog"</code> are treated as distinct.  </li>
<li><strong>add_prefix_space=True</strong> → Preserves leading spaces, which otherwise could get lost.  </li>
<li><strong>Vocabulary size = 8192</strong> → Small enough for efficient training, large enough to capture common words and subwords.  </li>
<li><strong>min_frequency=16</strong> → Rare patterns are ignored, preventing the vocabulary from bloating with noise.  </li>
</ul>
<hr />
<h3 id="encoding-the-dataset">Encoding the Dataset</h3>
<p>Once the tokenizer is trained, the next step is to encode the raw text into token IDs. This step converts every <code>.txt</code> shard that was previously gathered from <strong>FineWebEdu</strong> into a <code>.npy</code> file of integers.  </p>
<p>Why? Because:
- Encoding once upfront is faster than re-tokenizing on-the-fly.<br />
- <code>.npy</code> arrays are lightweight and can be memory-mapped during training.<br />
- Smaller storage space on system (~4:1 compression ratio, where assuming 1 byte is needed for each character, assuming 2 bytes per token, reduce storage to 50%)  </p>
<h4 id="example-script_1">Example Script</h4>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="o">,</span> <span class="nn">os</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s2">&quot;bpe_8k.json&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">=</span> <span class="s2">&quot;&lt;UNK&gt;&quot;</span>  <span class="c1"># Set unknown token</span>

<span class="n">src_dir</span><span class="p">,</span> <span class="n">dst_dir</span> <span class="o">=</span> <span class="s2">&quot;short_1800&quot;</span><span class="p">,</span> <span class="s2">&quot;short_1800_tokens&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">dst_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">src_dir</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.txt&quot;</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">src_dir</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dst_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.txt&#39;</span><span class="p">,</span><span class="s1">&#39;.npy&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
</code></pre></div>
<h4 id="notes_1">Notes</h4>
<ul>
<li>We set <code>unk_token = "&lt;UNK&gt;"</code> as a fallback. In practice, almost all text will map cleanly since we used ByteLevel.  </li>
<li>Tokens are stored as <code>uint16</code> because our vocabulary size is &lt; 2**16. This is more memory-efficient than <code>int32</code> or <code>int64</code>.  </li>
<li>For a 10,000 character text file, this typically compresses down to ~2,500 tokens, depending on content.  </li>
</ul>
<hr />
<p>By the end of this stage, the dataset has gone from raw text → clean shards → token IDs, all ready to be fed into the pretraining pipeline.
For the remainder of this documentation/tutorial, I will show tokenization on word level for simplicity. </p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["content.math"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>