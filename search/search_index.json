{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SimpleLLaMA: A Sandbox for Understanding LLM Training","text":"<p>SimpleLLaMA is an educational project that reproduces the training pipeline of a modern large language model (LLM) at a smaller, manageable scale. Unlike black-box APIs, this project provides a transparent, step-by-step view into pretraining, supervised fine-tuning, alignment, and evaluation.</p> <pre><code>flowchart LR\n    A[Dataset Collection] --&gt; B[Pretraining]\n    B --&gt; C[Supervised Fine-Tuning]\n    C --&gt; D[RLHF / Alignment]\n    D --&gt; E[Evaluation &amp; Benchmarking]\n    E --&gt; F[Inference Pipeline / Demo]</code></pre>"},{"location":"#sections","title":"Sections","text":"<ul> <li>Pretraining</li> <li>Supervised Fine-Tuning</li> <li>Reinforcement Learning / Alignment</li> <li>Miscellaneous (Benchmarking, Inference, Notes)</li> </ul> <p>Disclaimer- This documentation is created with the best of my abilities. Further proof reads were done manually, followed by review of a LLM to verify accuracy. That said, there many still be some slightly inaccuracies present. </p>"},{"location":"concat/","title":"Concat","text":""},{"location":"concat/#datasetmd","title":"dataset.md","text":""},{"location":"concat/#dataset-preparation","title":"Dataset Preparation","text":"<ol> <li>Why the Dataset Matters</li> </ol> <p>The dataset is one of the most important factors in pretraining. But what exactly do we mean by \u201cdata\u201d here?</p> <p>In this context, it\u2019s textual information gathered from the internet \u2014 things like blog posts, articles, books, and discussion sites (Reddit being one of the most popular). All of this text is combined into a huge \u201ccorpus,\u201d which developers then clean and process.</p> <p>It\u2019s not just about having a massive amount of data. Quality matters more than quantity. There\u2019s a saying: garbage in, garbage out. If a model is trained on tons of low-quality text (bad grammar, extreme biases, incoherent scraps from random social media posts), then the model will happily learn to imitate that bad text.</p> <p>Of course, \u201chigh-quality text\u201d doesn\u2019t have a strict definition. But generally, removing bias-heavy content, incorrect information, and extreme or repetitive junk makes the dataset more useful for training.</p> <ol> <li>Sources of Data</li> </ol> <p>There are a few well-known public datasets often used in LLM training:</p> <p>CommonCrawl \u2013 A massive raw dump of the internet, updated monthly. Very messy and unprocessed.</p> <p>C4 \u2013 Colossal Cleaned Common Crawl, a cleaned-up version of CommonCrawl curated by Google.</p> <p>The Pile \u2013 Curated by EleutherAI, it\u2019s a big mix of sources like arXiv, Wikipedia, GitHub, StackExchange, and more.</p> <p>For this project, I\u2019ll be using FineWeb, specifically the FineWebEdu subset.</p> <ol> <li>What is FineWeb?</li> </ol> <p>FineWeb is a large-scale dataset derived from CommonCrawl, but extensively filtered. The result is a 15 trillion token corpus of much higher quality text.</p> <p></p> <p>As shown above, the FineWeb team applied several filters:</p> <p>URL filtering \u2013 remove adult/unsafe sites.</p> <p>Text extraction \u2013 clean raw HTML into usable text.</p> <p>Language filtering \u2013 use a fastText classifier to keep only English text with score \u2265 0.65.</p> <p>LM filtering \u2013 use a smaller language model to toss out very low-quality passages.</p> <p>MinHash deduplication \u2013 remove near-duplicate documents.</p> <p>C4 filters \u2013 apply the same cleaning rules used in Google\u2019s C4 dataset.</p> <p>Custom filters \u2013 e.g., require punctuation, remove pages with lots of tiny lines.</p> <p>PII removal \u2013 scrub out personally identifiable information.</p> <ol> <li>FineWebEdu Subset</li> </ol> <p>FineWebEdu goes even further. It selects content with an educational focus, resulting in two smaller but more useful subsets:</p> <p>1.3 trillion tokens \u2013 very high educational quality.</p> <p>5.4 trillion tokens \u2013 high educational quality.</p> <p>How was this done? The FineWeb authors actually fine-tuned a LLaMA-3 70B model to act as a text quality rater. The model was trained to assign each passage a score from 0 to 5, where higher means more \u201ceducational.\u201d</p> <p>Threshold = 3 \u2192 keep only text scoring \u2265 3 \u2192 results in the 1.3T token dataset.</p> <p>Threshold = 2 \u2192 keep only text scoring \u2265 2 \u2192 results in the 5.4T token dataset.</p> <p>So instead of just cleaning mechanically, they used an LLM itself to filter text by \u201ceducational quality.\u201d</p> <p>There\u2019s a great Hugging Face writeup if you want to dive deeper: FineWeb Blogpost</p> <p>(Note: FineWeb token counts are reported using the GPT-2 tokenizer. More about tokens and tokenization in the next section.)</p> <ol> <li>This Project</li> </ol> <p>For this project, I\u2019ll be using FineWebEdu as the main pretraining dataset.</p> <p>A general rule of thumb for dataset sizing: you want a parameter-to-token ratio of at least 1:20 (20 tokens for every model parameter). Once you get to very high ratios (like 1:100), you start seeing diminishing returns.</p>"},{"location":"concat/#additional-addons","title":"Additional Addons:!!!!!!!!!!!!!!!!!","text":""},{"location":"concat/#make-sure-to-mention-this-is-trained-on-ascii-only","title":"Make sure to mention this is trained on ascii only!","text":""},{"location":"concat/#dataset-gathering-sharding","title":"Dataset Gathering &amp; Sharding","text":"<p>The first step is to collect and prepare the dataset. Since this tokenizer will be used to tokenize the FineWebEdu dataset we gathered earlier, it would also be used to train the tokenizer (generally, it's better to train the tokenizer on the same distribution as the pretraining data for LLM). But even then, the raw dataset is too large to work with as a single file, so we break it into smaller shards.</p>"},{"location":"concat/#why-sharding","title":"Why Sharding?","text":"<ul> <li>Large text corpora can easily exceed hundreds of gigabytes.  </li> <li>Training requires fast streaming of tokens \u2014 you don\u2019t want to load the entire dataset into memory.  </li> <li>By splitting into smaller shards (e.g., ~100MB each), we can load them efficiently and resume from checkpoints if needed.  </li> </ul>"},{"location":"concat/#short-medium-long-buckets","title":"Short / Medium / Long Buckets","text":"<p>The script doesn\u2019t just shard randomly \u2014 it separates text into three \u201cbuckets\u201d based on length: - Short: under ~7,500 characters. - Medium: 7,500\u201312,000 characters. - Long: over 12,000 characters.  </p> <p>Why? Because training efficiency changes with sequence length. Training on shorter examples first lets the model pick up basic structure faster, while longer sequences come later when it has learned more.  </p>"},{"location":"concat/#intentional-leakage","title":"Intentional \u201cLeakage\u201d","text":"<p>The script also allows some examples to \u201cleak\u201d into bigger buckets. For instance: - ~15% of short samples are redirected into medium. - ~10% of short samples are redirected into long.  </p> <p>This prevents the model from overfitting to only very long text near the end of training. In practice, real-world queries are often much shorter, so keeping a blend of lengths makes the model more robust.  </p>"},{"location":"concat/#example-snippet-simplified","title":"Example Snippet (Simplified)","text":"<p>Here\u2019s a stripped-down version of the gather script:  </p> <pre><code>if ex[\"text\"].isascii() and len(ex[\"text\"]) &lt;= max_length:\n    # pick bucket based on text length\n    if len(ex[\"text\"]) &lt;= 7500:\n        bucket = \"short\"\n    elif len(ex[\"text\"]) &lt;= 12000:\n        bucket = \"medium\"\n    else:\n        bucket = \"long\"\n\n    # allow some short \u2192 medium/long leakage\n    if bucket == \"short\" and random.random() &lt; 0.25:\n        bucket = \"medium\" if random.random() &lt; 0.6 else \"long\"\n\n    shard_text[bucket].append(f\"&lt;SOS&gt;{ex['text']}&lt;EOS&gt;\")\n</code></pre> <p>Notice a couple of important details: - All text is wrapped with <code>&lt;SOS&gt;</code> (start of sequence) and <code>&lt;EOS&gt;</code> (end of sequence). - This guarantees the tokenizer and model know exactly where an example begins and ends. - Filtering to ASCII-only ensures consistency and avoids tricky edge cases with multilingual characters (important for compute-constrained projects).  </p> <p>By the end of this step, the dataset is organized into neatly sharded text files, ready to be fed into the tokenizer training process.</p>"},{"location":"concat/#overviewmd","title":"overview.md","text":""},{"location":"concat/#pretraining-overview","title":"Pretraining Overview","text":"<p>In this pretraining section, we\u2019ll walk through the core parts: what pretraining is for, how the dataset is gathered, the model architecture, and the overall training process.</p>"},{"location":"concat/#what-is-pretraining","title":"What is Pretraining?","text":"<p>Pretraining is the very first step in building a large language model (LLM). Like all deep neural networks, the model\u2019s weights start out completely random (usually sampled from a normal distribution). At this stage, the model is basically useless \u2014 if you ask it to generate text, it\u2019ll just spit out noise.</p> <p>The goal of pretraining is to give the model a general sense of language. It gets exposed to a massive dataset of text and learns patterns like grammar, sentence structure, vocabulary, and even some factual knowledge just from raw co-occurrence. The training objective is next-token prediction \u2014 given some text, the model tries to predict the next word (or token).</p> <p>After enough training, the model learns how to produce text that flows naturally. At this point, if you give it an input sequence, it\u2019ll continue in the way that\u2019s statistically most likely. It\u2019s not yet an \u201cassistant\u201d that follows instructions \u2014 it\u2019s more like a text autocomplete engine. That\u2019s where later stages like SFT and RLHF come in.</p>"},{"location":"concat/#dataset","title":"Dataset","text":"<p>For this project, I\u2019m using FineWebEdu from HuggingFace. It\u2019s a large and diverse internet-based dataset that\u2019s been heavily filtered. Filtering steps include things like deduplication, removing boilerplate (like HTML tags from scraped pages), and content filtering to keep it relatively clean. The idea is to make sure the model sees a broad variety of text without too much junk.</p>"},{"location":"concat/#model-architecture","title":"Model Architecture","text":"<p>The base model is a decoder-only transformer, similar to the LLaMA-2 design. It also supports an alternative attention mechanism called MLA (Multi-head Latent Attention), adapted from DeepSeek, though that\u2019s optional in this pipeline.</p>"},{"location":"concat/#training-process","title":"Training Process","text":"<p>Training uses the standard cross-entropy loss with the AdamW optimizer. Since training is iterative, the model gradually improves through many passes of gradient descent \u2014 in this case, across tens of billions of tokens.</p> <p>By the end of pretraining, the model develops a kind of \u201clanguage sense.\u201d It can generate coherent text, but it\u2019s still very raw. It doesn\u2019t know how to follow instructions or have a chat \u2014 those abilities come from the later stages: SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning with Human Feedback).</p>"},{"location":"concat/#key-takeaway","title":"Key Takeaway","text":"<p>Pretraining lays the foundation: it teaches the model how language works at a broad level, but not how to use it in a helpful way. Think of it as teaching a child to read and write before teaching them how to answer questions or follow directions.</p>"},{"location":"concat/#training_advancedmd","title":"training_advanced.md","text":""},{"location":"concat/#training_beginnermd","title":"training_beginner.md","text":""},{"location":"concat/#1-introduction-what-are-we-doing","title":"1. Introduction: What Are We Doing?","text":"<p>Before we dive into training, let\u2019s step back and ask: what exactly does it mean to \u201ctrain a language model\u201d?</p> <p>At its core, the goal is very simple: We want the model to learn how to predict the next token in a sequence of text.</p>"},{"location":"concat/#the-idea-in-plain-english","title":"The Idea in Plain English","text":"<p>Imagine you are reading a sentence:</p> <p>\"The cat sat on the ____\"</p> <p>Even without seeing the last word, your brain automatically guesses it might be \u201cmat\u201d, \"couch\", \"floor\" or any other plausible words. That\u2019s because you\u2019ve learned patterns of how words usually appear together.  </p> <p>A language model is doing the same thing, but instead of words, it deals with tokens (small units of text such as subwords, characters, or whole words depending on the tokenizer).  </p> <p>During training, the model sees billions of examples of token sequences and learns which token is most likely to come next. Over time, this builds up into a powerful statistical understanding of language.</p>"},{"location":"concat/#from-text-to-tokens","title":"From Text to Tokens","text":"<p>Computers can\u2019t directly process raw text like <code>\"The cat sat on the mat\"</code>. We first need to break it down into numerical form.</p> <ol> <li> <p>Raw Text: <code>\"The cat sat on the mat\"</code></p> </li> <li> <p>Tokenized Text (IDs): <code>[1202, 850, 149, 4211, 769, 1839]</code>    (each number is a token ID from a vocabulary)</p> </li> <li> <p>Input Tensor (batch of sequences): <pre><code>tensor([[1202,  850,  149, 4211,  769]])\n</code></pre></p> </li> </ol> <p>The vocabulary maps every possible token (like \u201ccat\u201d, \u201csat\u201d, \u201con\u201d) to a unique integer. The model only sees these integers.</p>"},{"location":"concat/#how-the-model-thinks","title":"How the Model Thinks","text":"<p>Now, let\u2019s show the high-level flow of how these tokens are processed:</p> <pre><code>flowchart LR\n    A[Raw Text] --&gt; B[Tokenizer]\n    B --&gt; C[Tokens: IDs]\n    C --&gt; D[Transformer Model]\n    D --&gt; E[Logits: raw scores]\n    E --&gt; F[Softmax: probabilities]\n    F --&gt; G[Loss Function]</code></pre> <p>Let\u2019s break down the important parts:</p> <ul> <li> <p>Logits: The model outputs a vector of raw scores for each possible token in the vocabulary. For example, if the vocab size is 50,000, the logits are a vector of 50,000 numbers. These are not yet probabilities.</p> </li> <li> <p>Softmax: We apply the softmax function to logits, which turns them into probabilities (values between 0 and 1 that sum to 1). For example, the model might say:</p> <ul> <li>\u201cmat\u201d: 0.72  </li> <li>\u201cfloor\u201d: 0.12  </li> <li>\u201ccouch\u201d: 0.03  </li> <li>\u2026 and so on for every other token.  </li> </ul> </li> <li> <p>Loss: The training process compares the predicted probability distribution to the correct answer (the actual next token). It calculates how far off the prediction was. This is the \u201closs.\u201d The smaller the loss, the better the model is at predicting.</p> </li> </ul>"},{"location":"concat/#why-this-matters","title":"Why This Matters","text":"<p>This \u201cpredict the next token\u201d setup is deceptively simple, but it\u2019s powerful. By learning these probabilities across massive amounts of text, the model starts to capture grammar, facts, reasoning, and even world knowledge \u2014 all as a side effect of next-token prediction.</p> <p>So when we say \u201ctrain a transformer model,\u201d what we really mean is: - Give the model tons of sequences of tokens. - Ask it to guess the next token. - Measure how wrong it was. - Adjust the model\u2019s weights to improve its guesses. - Repeat billions of times.  </p> <p>That\u2019s the heart of language model training.</p>"},{"location":"concat/#2-model-configuration","title":"2. Model Configuration","text":"<p>Before we can train our transformer, we need to decide on all the hyperparameters and settings that control both the model and the training process. These settings are stored in a configuration object, which in our case is implemented using a Python dataclass called <code>TrainingConfig</code>.  </p> <p>The configuration file may look intimidating at first, since it lists dozens of parameters. But many of them are straightforward once you understand the categories they fall into. Let\u2019s walk through the most important ones.</p> <p>The first group of parameters defines where the data and outputs are stored. For example:  </p> <ul> <li><code>dataset_dir</code> tells the program where to find the pre-tokenized dataset files.  </li> <li><code>tokenizer_path</code> points to the JSON file that contains the trained tokenizer.  </li> <li><code>ckpt_dir</code> specifies the folder where model checkpoints will be saved during training.  </li> <li><code>log_file</code> is a simple text file where progress (like loss values) is recorded.  </li> </ul> <p>Together, these ensure the training script knows both where to read the data from and where to save its results.  </p> <p>Next, we have the batch and sequence length parameters, which directly control how much data the model processes at once.  </p> <ul> <li><code>batch_size</code> is the number of sequences per batch. If you set this to 4, then each step processes 4 separate chunks of text in parallel.  </li> <li><code>max_seq_len</code> is the maximum number of tokens per sequence. For example, if <code>max_seq_len = 2048</code>, then each input sequence is capped at 2048 tokens long. Longer documents must be split into smaller pieces.  </li> <li><code>tokens_per_update</code> defines how many tokens are processed before the optimizer takes a step. Since this touches upon gradient accumulation, which is outside the scope of this basic explanation, it will be covered in the <code>training_advanced.md</code> file.  </li> </ul> <p>These three parameters together determine how much work the model is doing in each training step and how much GPU memory it will consume.</p> <p>Then comes the model architecture itself. These parameters define the shape and capacity of the transformer network:  </p> <ul> <li><code>n_embd</code> is the embedding dimension, the size of the vector used to represent each token internally. Larger values allow the model to capture richer relationships, but also make it heavier to train.  </li> <li><code>n_heads</code> sets how many attention heads are used per layer. Each head can focus on different relationships in the sequence, so more heads allow for more diverse patterns.  </li> <li><code>n_layers</code> is the number of stacked decoder layers. Each layer refines the token representations further, so deeper models are generally more powerful.  </li> <li><code>multiple_of</code> controls the feedforward layer\u2019s hidden dimension. Instead of choosing an arbitrary number, this ensures the size is a multiple of a fixed value (like 256), which helps optimize matrix multiplications on GPUs.  </li> <li><code>eps</code> is a tiny value added in normalization layers to avoid division by zero errors. It\u2019s not something you usually tweak, but it is essential for numerical stability.  </li> <li><code>theta</code> sets the base frequency for Rotary Position Embeddings (RoPE), which are used to encode token positions into the model. Again, you typically leave this at its default.  </li> <li><code>dropout</code> is a regularization mechanism where some connections are randomly \u201cdropped\u201d during training. For large pretraining, this is often set to <code>0.0</code> because the dataset itself provides enough variety, but in smaller-scale experiments you might increase it to avoid overfitting.  </li> </ul> <p>These architecture parameters are the \u201cDNA\u201d of the model. Changing them fundamentally alters the size and behavior of the transformer.</p> <p>Another critical part of the config is the training schedule. Training a large language model is not just about choosing an optimizer and running it \u2014 we also need to carefully plan how the learning rate evolves over time.  </p> <ul> <li><code>warmup_iterations</code> specifies how many steps are used to gradually increase the learning rate at the start of training. This prevents the model from diverging early on.  </li> <li><code>max_lr</code> is the peak learning rate reached after warmup.  </li> <li><code>min_lr</code> is the final learning rate at the end of training, typically reached through a cosine decay schedule.  </li> <li><code>beta1</code> and <code>beta2</code> are parameters of the AdamW optimizer, which control how much past gradients influence the updates.  </li> <li><code>weight_decay</code> is a form of regularization that prevents weights from growing too large, helping the model generalize better.  </li> </ul> <p>Together, these define the \u201cpace\u201d at which the model learns.</p> <p>Finally, we have the training tokens and evaluation settings.  </p> <ul> <li><code>training_tokens</code> is the total number of tokens the model will see during training. For example, <code>45e9</code> means 45 billion tokens in total.  </li> <li><code>eval_interval</code> controls how often the model\u2019s progress is evaluated. For instance, every 32 steps the model might generate text and log its loss.  </li> <li><code>model_gen_multiplier</code> adjusts how frequently sample generations are produced during training.  </li> </ul> <p>The config also includes checkpointing settings such as <code>token_ckpt</code> (how often to save the model in terms of tokens processed) and <code>load_ckpt</code> (whether to resume from a previous run).</p> <p>Even though this configuration object looks large, most of its parameters can be grouped into four intuitive categories: paths, batching, model architecture, and training schedule. For the beginner doc, you don\u2019t need to memorize every single field \u2014 the important thing is to understand what each group does. The rest can be treated as implementation details that you return to once you start experimenting.</p>"},{"location":"concat/#3-dataset-batching","title":"3. Dataset &amp; Batching","text":"<p>In the introduction, we explained that the goal of training is for the model to learn how to predict the next token in a sequence. But how do we actually present this information to the model during training? </p> <p>This is where the Dataset Loader comes in. Its job is to take the large tokenized dataset stored on disk and feed the model with manageable \u201cmini-batches\u201d of tokens at every training step. Without this loader, we would have no practical way to handle billions of tokens, because we cannot load everything into memory or train on an endless stream of raw text.  </p> <p>When training a language model, we usually start with a massive corpus of text \u2014 sometimes hundreds of gigabytes. This raw text has already been tokenized and stored in NumPy arrays for efficiency. These files are then fed into the Dataset Loader.</p> <p>If you tried to feed the entire dataset into the model in one go, three things would immediately go wrong: 1. The model would run out of memory, because GPUs cannot hold billions of tokens at once. 2. Training would be extremely inefficient, since we want to update weights frequently rather than waiting for one giant pass. 3. We would lose the ability to shuffle, divide across GPUs, or checkpoint easily.  </p> <p>The Dataset Loader solves all of these problems by breaking the token stream into smaller, more manageable pieces. At each step, it delivers a batch of sequences \u2014 small slices of the dataset that the model can process in parallel.</p>"},{"location":"concat/#the-structure-of-x-y","title":"The Structure of <code>(x, y)</code>","text":"<p>Each batch returned by the loader consists of two tensors:  </p> <ul> <li><code>x</code>: The input sequences of tokens.  </li> <li><code>y</code>: The same sequences, shifted one position to the right.  </li> </ul> <p>This shifting mechanism is what allows the model to learn \u201cnext token prediction.\u201d  </p> <p>Let\u2019s walk through a concrete example. Suppose the dataset contains a chunk the following six tokens:  </p> <pre><code>Tokens: [1202, 850, 149, 4211, 769, 1839]\n</code></pre> <p>If we set <code>batch = 1</code> and <code>seq_len = 5</code>, then the loader will slice the data like this:  </p> <pre><code>x = [[1202, 850, 149, 4211, 769]]\ny = [[ 850, 149, 4211,  769, 1839]]\n</code></pre> <p>At first glance, this looks like we are simply training a bigram model \u2014 for every token in <code>x</code>, we just predict the token in the same position in <code>y</code>. But that\u2019s not really what is happening inside the transformer. The important detail is that the model doesn\u2019t just see the token at position t and try to guess token t+1. Instead, it sees the entire sequence up to position t, and from that context, it tries to guess the next token.</p> <p>So in this case, the training targets look more like this:  </p> <ul> <li>Given <code>[1202]</code>, predict <code>850</code>.  </li> <li>Given <code>[1202, 850]</code>, predict <code>149</code>.  </li> <li>Given <code>[1202, 850, 149]</code>, predict <code>4211</code>.  </li> <li>Given <code>[1202, 850, 149, 4211]</code>, predict <code>769</code>.  </li> <li>Given <code>[1202, 850, 149, 4211, 769]</code>, predict <code>1839</code>.  </li> </ul> <p>Notice the subtle difference. A bigram model would only look at one previous token at a time, while the transformer looks at the entire history of the sequence and uses self-attention to weigh the importance of different past tokens when predicting the next one. This is what allows it to capture long-range dependencies in language, like subject\u2013verb agreement across many words.</p>"},{"location":"concat/#code-example-with-toy-data","title":"Code Example with Toy Data","text":"<p>Here is a small code example using the <code>DatasetLoader</code> class:</p> <pre><code>from simple_llama.pretraining.dataset_loader import DatasetLoader\n\n# Small example\nloader = DatasetLoader(\n    batch=2,           # Number of sequences in a batch\n    seq_len=4,         # Number of tokens per sequence\n    process_rank=0,    # Single-process case\n    num_processes=1,\n    dataset_dir=\"data\", \n    device=\"cpu\"\n)\n\nx, y = loader.get_batch()\nprint(\"x:\", x)\nprint(\"y:\", y)\n</code></pre> <p>Example output (toy data):</p> <pre><code>x: tensor([[1202,  850,  149, 4211],\n           [ 769, 1839, 3521, 4879]])\ny: tensor([[ 850,  149, 4211,  769],\n           [1839, 3521, 4879, 2035]])\n</code></pre> <p>Now you can clearly see how the pairs line up. Each row of <code>x</code> is a sequence of tokens, and each row of <code>y</code> is that same sequence shifted by one token. The model is trained to predict all of those shifts in parallel.</p>"},{"location":"concat/#why-batching-matters","title":"Why Batching Matters","text":"<p>The idea of batching deserves special attention. If we only trained on one sequence at a time, the model would make progress, but it would be extremely slow and would not take advantage of GPU acceleration. By grouping multiple sequences together into a batch, we can exploit the GPU\u2019s ability to perform large matrix multiplications efficiently.</p> <p>Suppose we use: - <code>batch = 32</code> - <code>seq_len = 2048</code> </p> <p>In that case, the model processes 65,536 tokens at every step. This is a massive increase in efficiency compared to processing a single sequence. This batching strategy is one of the main reasons why modern transformers can be trained at such large scales. It allows us to feed in huge amounts of data per optimization step, stabilize the gradients, and make much faster progress than would otherwise be possible.</p> <p>The Dataset Loader is therefore the bridge between the massive dataset on disk and the mini-batches that the model actually learns from. It provides structure to the training process, ensuring that at every step, the model sees just enough data to make a meaningful update \u2014 and then moves on to the next batch.</p>"},{"location":"concat/#inside-the-dataset-loader-how-it-works","title":"Inside the Dataset Loader: How It Works","text":"<p>When you create a <code>DatasetLoader</code>, you pass in the batch size, sequence length, dataset directory, and a few distributed training arguments:</p> <pre><code>class DatasetLoader:\n    def __init__(self, batch: int, seq_len: int, process_rank: int, num_processes: int, dataset_dir: str, device: str):\n        \"\"\"\n        :param batch: Batch size\n        :param seq_len: Max seq len\n        :param process_rank: Rank of the process that initializes an instance of this class\n        :param num_processes: Total number of processes (World Size)\n        :param dataset_dir: Dataset directory\n        :param device: \"cuda\" or \"cpu\"\n        \"\"\"\n\n        self.batch = batch\n        self.seq_len = seq_len\n        self.process_rank = process_rank\n        self.num_processes = num_processes\n        self.device = device\n\n        # Holds all the filepaths\n        self.filepaths = sorted([os.path.join(dataset_dir, p) for p in os.listdir(dataset_dir)])\n\n        self.file_data = np.load(self.filepaths[0])\n        self.file_idx = 0  # Current file index\n        self.tok_idx = batch * seq_len * process_rank  # Current token idx\n</code></pre> <p>Here\u2019s what happens under the hood in <code>__init__</code>:</p> <ol> <li>Instance Attribute: It sets the instance attributes using the given arguments  </li> <li>File discovery: It scans the dataset directory and gathers all <code>.npy</code> files (each file stores a large array of token IDs).  </li> <li>Pointers/Tracker Initialization:         - <code>file_data</code>: At startup, the loader reads the first <code>.npy</code> file in the dataset directory into memory. This array contains a long sequence of token IDs.         - <code>file_idx</code>: A counter that starts at <code>0</code>, meaning we are currently working with the first file in the dataset. As training progresses and one file is exhausted, this index is incremented to load the next file.         - <code>tok_idx</code>: A pointer into the current file that tells the loader where to start slicing tokens for the next batch. This is critical because each call to <code>get_batch()</code> must pick up right where the last one left off.         - Multi-GPU offset: If using multiple GPUs (distributed training), each process is assigned a different starting offset for <code>tok_idx</code>. This prevents all GPUs from training on the exact same data, ensuring better utilization of the dataset.  </li> </ol> <p>Together, these three trackers (<code>file_data</code>, <code>file_idx</code>, <code>tok_idx</code>) allow the loader to move seamlessly through massive token arrays spread across multiple files, while keeping every batch aligned and avoiding duplication across processes.</p>"},{"location":"concat/#getting-a-batch","title":"Getting a Batch","text":"<p>The heart of the class is <code>get_batch()</code>. This is the function called during training to get new <code>(x, y)</code> tensors.</p> <ol> <li> <p>Slice out a chunk of tokens: </p> <pre><code>batch = self.file_data[self.tok_idx : self.tok_idx + (self.batch * self.seq_len) + 1]\n</code></pre> <p>Here we grab just enough tokens for a full batch (<code>batch * seq_len</code>) plus one extra, since <code>y</code> is shifted.</p> </li> <li> <p>Reshape into 2D arrays: </p> <pre><code>x = batch[:-1].reshape(self.batch, self.seq_len)\ny = batch[1:].reshape(self.batch, self.seq_len)\n</code></pre> <p>This step converts the flat token slice into two matrices: - <code>x</code> for the inputs, - <code>y</code> for the targets, shifted by one token.</p> </li> <li> <p>Advance the token index: </p> <pre><code>self.tok_idx += (self.batch * self.seq_len * self.num_processes)  # Increment the index counter\n\n# If we reach the end of file, move on to the next one\nif self.tok_idx + (self.batch * self.seq_len * self.num_processes) + 1 &gt;= len(self.file_data):\n    self.file_idx += 1\n    if self.file_idx &gt;= len(self.filepaths):\n        self.file_idx = 0\n\n    self.file_data = np.load(self.filepaths[self.file_idx])\n    self.tok_idx = self.batch * self.seq_len * self.process_rank\n</code></pre> <p>After returning a batch, the loader moves its pointer forward. If we reach the end of a file, it automatically loads the next one and update corresponding counters accordingly. </p> </li> <li> <p>Convert to tensors: </p> <pre><code>return torch.from_numpy(x.astype(np.int32)).long().to(self.device), torch.from_numpy(y.astype(np.int32)).long().to(self.device)\n</code></pre> <p>The NumPy arrays are cast to <code>torch.long</code> (integer type needed for embeddings) and moved to the correct device (CPU or GPU).</p> </li> </ol>"},{"location":"concat/#why-this-design","title":"Why This Design?","text":"<p>Overall, the Dataset Loader is designed for training efficiency:</p> <ul> <li>Streaming from disk: It only loads one dataset file at a time, so memory usage stays low.  </li> <li>Batch alignment: It guarantees that <code>(x, y)</code> line up perfectly for next-token prediction.  </li> <li>Distributed training friendly: The <code>process_rank</code> and <code>num_processes</code> arguments make sure multiple GPUs can work on different slices of the dataset without overlap.  </li> <li>Scalable: As long as your dataset is tokenized into <code>.npy</code> files, this loader can handle billions of tokens just as easily as thousands.  </li> </ul> <p>One can think of it as a neat wrapper around: - slicing arrays, - reshaping them into <code>(batch, seq_len)</code> form, - shifting by one token, and - handing them to PyTorch.</p> <p>This simplicity makes it both easy to understand and powerful enough for large-scale training.</p>"},{"location":"concat/#4-learning-rate-scheduler","title":"4. Learning Rate Scheduler","text":"<p>The learning rate (LR) is one of the most important hyperparameters in training deep neural networks. Too high, and training diverges; too low, and learning stalls. A scheduler adjusts the learning rate dynamically during training, instead of keeping it fixed.</p> <p>This project includes a custom <code>Scheduler</code> class that implements warmup and three different scheduling strategies: cosine decay, linear decay, and constant LR.</p>"},{"location":"concat/#why-use-a-scheduler","title":"Why Use a Scheduler?","text":"<p>Schedulers help address two common issues in optimization:</p> <ul> <li>Exploding/vanishing gradients \u2013 keeping the LR too high/low throughout training often leads to instability or poor convergence.  </li> <li>Training dynamics \u2013 a model often benefits from a short warmup phase (slowly ramping LR up), followed by a gradual decay to smaller values.  </li> <li>Generalization \u2013 decaying the LR near the end of training often improves final accuracy/perplexity.</li> </ul> <p>Instead of manually adjusting LR mid-training, a scheduler automates the process.</p>"},{"location":"concat/#scheduler-implementation","title":"Scheduler Implementation","text":"<p>The <code>Scheduler</code> class wraps around a PyTorch optimizer. It is initialized with a few key parameters:</p> <pre><code>class Scheduler:\n    def __init__(self, torch_optimizer: Optimizer, schedule: str, training_steps: int,\n                 warmup_steps: int, max_lr: float, min_lr: float):\n        # schedule \u2208 [\"cosine\", \"linear\", \"constant\"]\n        # training_steps = total number of steps\n        # warmup_steps = steps spent ramping LR up\n        # max_lr = peak LR\n        # min_lr = final LR (ignored for \"constant\")\n</code></pre> <ul> <li>schedule: strategy (\"cosine\", \"linear\", or \"constant\").  </li> <li>training_steps: total steps in training run.  </li> <li>warmup_steps: number of warmup steps (linear ramp up).  </li> <li>max_lr: highest LR used during training.  </li> <li>min_lr: final LR (for decay-based schedules).  </li> </ul> <p>Warmup</p> <p>During warmup, LR increases linearly from near zero to <code>max_lr</code>:</p> <pre><code>def _update_warmup(self, current_step: int):\n    lr = (max(1, current_step) / self.warmup_steps) * self.max_lr\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n</code></pre> <p>This prevents unstable updates at the beginning of training.</p> <p>Cosine Decay</p> <p>Cosine decay smoothly lowers the LR from <code>max_lr</code> to <code>min_lr</code>:</p> <pre><code>def _update_cosine(self, current_step: int):\n    current_step -= self.warmup_steps\n    scale = (current_step / self.decay_steps) * math.pi\n    lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + math.cos(scale))\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n</code></pre> <p>This schedule is popular in modern LLM training because it decays aggressively at first, then flattens out.</p> <p>Linear Decay</p> <p>Linear decay reduces LR steadily over time:</p> <pre><code>def _update_linear(self, current_step: int):\n    current_step -= self.warmup_steps\n    lr = self.max_lr - (current_step / self.decay_steps) * (self.max_lr - self.min_lr)\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n</code></pre> <p>Simpler than cosine, but still effective.</p> <p>Constant</p> <p>Sometimes you may want to keep LR fixed at <code>max_lr</code> (e.g., for debugging).</p> <pre><code>if schedule == \"constant\":\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = max_lr\n</code></pre> <p>Step Method</p> <p>The central logic is in the <code>step</code> method, which updates LR depending on the phase of training:</p> <pre><code>def step(self, current_step: int):\n    if current_step &lt; self.warmup_steps and self.schedule != \"constant\":\n        self.current_lr = self._update_warmup(current_step)\n        return\n\n    if self.schedule == \"cosine\":\n        self.current_lr = self._update_cosine(current_step)\n    elif self.schedule == \"linear\":\n        self.current_lr = self._update_linear(current_step)\n    elif self.schedule == \"constant\":\n        self.current_lr = self.max_lr\n</code></pre> <p>This ensures the correct schedule is applied at every step.</p>"},{"location":"concat/#visualizing-the-schedules","title":"Visualizing the Schedules","text":"<p>To make things concrete, below are plots showing how the LR evolves across steps: (All are 100k total steps, 1k of which is warmup steps, max_lr set to 1e-3 and min_lr set to 1e-4)</p> <p>Cosine with Warmup:</p> <p></p> <p>Linear with Warmup:</p> <p></p> <p>Constant LR:</p> <p></p> <p>You can generate these plots using the included test script in the class (<code>__main__</code> block).</p>"},{"location":"concat/#summary","title":"Summary","text":"<ul> <li>Warmup prevents instability at the start of training.  </li> <li>Cosine decay \u2192 smooth, effective, widely used in LLMs.  </li> <li>Linear decay \u2192 simpler, still works well.  </li> <li>Constant \u2192 mostly for experiments/debugging.  </li> </ul> <p>This custom scheduler is flexible, checkpointable, and provides good control for projects like this.</p>"},{"location":"concat/#5-training-loop","title":"5. Training Loop","text":"<p>Training a large language model (LLM), even a small\u2011scale one like in this project, comes down to a repeated cycle: take a batch of data, run it through the model, calculate how wrong the predictions are, push the error backwards to update the weights, and repeat. This cycle is what we call the training loop. This section will walk in great detail through the core parts of the loop. </p>"},{"location":"concat/#instantiation","title":"Instantiation","text":"<p>Before we can train the model, we need to set up all the core building blocks. Once everything is in place, the training loop itself becomes a straightforward repetition of forward pass, loss calculation, backward pass, and optimization.</p> <p>1. Configuration Object</p> <p>The first thing we need is a configuration object that stores all of our hyperparameters. Instead of scattering values like batch size, learning rate, and number of layers across different files, it\u2019s cleaner to place them in a single object or namespace. This makes the code easier to manage, debug, and extend.  </p> <p>In this project, it will be the <code>TrainingConfig</code> class, located within the <code>simple_llama/pretraining</code> folder</p> <pre><code>@dataclass\nclass TrainingConfig:\n    # === Paths and Dataset ===\n    dataset_dir: str = root_path(\"simple_llama\", \"dataset\", \"short\")       # Path to tokenized training data\n    tokenizer_path: str = root_path(\"simple_llama\", \"dataset\", \"bpe_8k.json\")          # Path to tokenizer model\n    ckpt_dir: str = root_path(\"simple_llama\", \"pretraining\", \"checkpoints\")   # Directory to store checkpoints\n    log_file: str = root_path(\"simple_llama\", \"pretraining\", \"training_progress.txt\")  # File to log training progress\n\n    # === Batch &amp; Sequence ===\n    batch_size: int = 4             # Minibatch size\n    max_seq_len: int = 2048         # Maximum sequence length per sample\n    tokens_per_update: int = 2**19  # ~512K tokens per optimizer update\n\n    # === Model Architecture ===\n    n_embd: int = 2048               # Embedding dimension\n    n_heads: int = 32                # Number of attention heads\n    n_layers: int = 24               # Number of transformer layers\n    multiple_of: int = 256           # Feedforward dim multiple for efficient matmul\n    eps: float = 1e-5                # Epsilon value to prevent div-by-zero in normalization layers\n    theta: int = 10_000              # Theta for RoPE rotation frequency\n    dropout: float = 0.0             # Dropout rate; typically 0.0 for pretraining\n\n    ...  # And many more\n\nconfig = TrainingConfig()\n</code></pre> <p>This way, if we want to adjust <code>n_heads</code> or experiment with a different <code>max_lr</code>, it\u2019s a single line change.</p> <p>2. Dataset Loader</p> <p>Next, instantiate a dataset loader object that is defined, passing in hyperparameters as needed, extracted from the configuration object:</p> <pre><code>from simple_llama.pretraining.dataset_loader import DatasetLoader\n\ndataset_loader = DatasetLoader(batch=batch_size, seq_len=max_seq_len, process_rank=ddp_rank,\n                               num_processes=ddp_world_size, dataset_dir=config.dataset_dir, device=device)\n</code></pre> <p>3. The Model</p> <p>Now comes the centerpiece: the transformer model itself. In this project, we\u2019ve implemented <code>LLaMaTransformer</code>, which includes embeddings, attention blocks, feedforward layers, normalization, and output projection.</p> <pre><code>model = LLaMaTransformer(config, tokenizer, device=\"cuda\")\n</code></pre> <p>Here: - <code>config</code> gives the model hyperparameters. - <code>tokenizer</code> provides the vocabulary size. - <code>device=\"cuda\"</code> places the model on GPU.</p> <p>Initially, the model\u2019s parameters are random. Training gradually adjusts them so that token predictions become more accurate.</p> <p>4. The Loss Function</p> <p>Next, we define how the model will be judged. For language modeling, the go-to choice is cross-entropy loss:</p> <pre><code>criterion = torch.nn.CrossEntropyLoss()\n</code></pre> <p>Cross-entropy measures how \u201csurprised\u201d the model is by the correct next token. - If the model assigns high probability \u2192 low loss. - If it assigns low probability \u2192 high loss.</p> <p>5. The Optimizer</p> <p>Finally, we define the optimizer. We use AdamW, which is the de facto standard for transformers because it combines Adam\u2019s adaptive gradient updates with weight decay for stability.</p> <pre><code>optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(beta1, beta2), weight_decay=weight_decay, **extra_args)\n</code></pre> <p>This way, every training step will use the optimizer to update the model parameters and the scheduler to adjust the learning rate.</p> <p>6. Learning Rate Scheduler</p> <p>Before we instantiate the optimizer, we actually define the learning rate scheduler. The scheduler controls how the learning rate evolves over time, which is critical for stable training.</p> <p>We\u2019re using the custom <code>Scheduler</code> class implemented earlier, which supports linear decay, cosine decay, or just a constant learning rate.</p> <pre><code>scheduler = Scheduler(torch_optimizer=optimizer,\n                      schedule=\"cosine\",\n                      training_steps=optimization_steps,\n                      warmup_steps=warmup_iterations,\n                      max_lr=max_lr,\n                      min_lr=min_lr)\n</code></pre> <p>At this stage, the <code>torch_optimizer</code> is left as <code>None</code> \u2014 we\u2019ll link it once the optimizer is created. This flexibility makes it easy to checkpoint and resume training.</p> <p>At this point, we\u2019ve instantiated: - The configuration object - The dataset loader - The transformer model - The loss function - The optimizer - The learning rate scheduler</p> <p>All the main components are ready. The next step is to actually run them inside the training loop.</p>"},{"location":"concat/#the-model-forward-pass","title":"The Model Forward Pass","text":"<p>We begin with a batch of input tokens, grabbed from the DatasetLoader object via the <code>get_batch()</code> method. Each integer corresponds to a token ID from our vocabulary.  </p> <p>Let\u2019s say our batch size is <code>B = 4</code>, and the sequence length we train on is <code>T = 16</code>. A batch from the dataset loader might look like:</p> <pre><code>x.shape = (B, T) = (4, 16)\n</code></pre> <p>So <code>x</code> is a 2D tensor of integers. Each row is one training example (one text sequence), and each entry is a token ID.  </p> <p>When we feed this into the model:</p> <pre><code>logits = model(x)\n</code></pre> <p>the transformer runs all of its layers: embedding lookup, multiple decoder blocks, attention, feedforward layers, normalization, and finally a linear projection back to vocabulary size.  </p> <p>The key here is the shape change:  </p> <ul> <li>Input: <code>(B, T)</code> \u2014 integers.  </li> <li>Output: <code>(B, T, C)</code> \u2014 floats, where <code>C</code> is the vocabulary size.  </li> </ul> <p>Why <code>(B, T, C)</code>? Because for every position in every sequence, the model outputs a vector of size <code>C</code>, which are the raw unnormalized scores for each possible token in the vocabulary. These are called logits.</p>"},{"location":"concat/#the-loss-function","title":"The Loss Function","text":"<p>Once we have logits, we want to measure how good the predictions are. That is the role of the loss function. For language modeling, the standard is cross entropy loss.</p> <p>The goal is simple: the model is asked to predict the next token in the sequence. If the input sequence is <code>[The, cat, sat, on, the]</code>, the correct output is <code>[cat, sat, on, the, mat]</code>. Each token should map to the next token.  </p> <p>Cross entropy measures how \u201csurprised\u201d the model is by the correct answer. If the model already places high probability on the true next token, the loss is small. If the model thought another token was much more likely, the loss is large.  </p> <p>In PyTorch, we use:</p> <pre><code>criterion = nn.CrossEntropyLoss()\n</code></pre> <p>However, <code>CrossEntropyLoss</code> expects inputs of shape <code>(N, C)</code> where <code>N</code> is the number of items and <code>C</code> is the number of classes, and targets of shape <code>(N,)</code>.  </p> <p>Our logits are <code>(B, T, C)</code> and our targets are <code>(B, T)</code>. So we flatten them:</p> <pre><code>loss = criterion(logits.view(-1, C), targets.view(-1))\n</code></pre> <p>This reshapes:</p> <ul> <li><code>logits.view(-1, C)</code> \u2192 <code>(B*T, C)</code> </li> <li><code>targets.view(-1)</code> \u2192 <code>(B*T,)</code> </li> </ul> <p>Effectively, we treat the whole batch as one big list of token predictions.</p> <p>Mathematically, cross entropy loss is:</p> <pre><code>L = - (1/N) * \u03a3 log(softmax(logits)[i, y_i])\n</code></pre> <p>where <code>y_i</code> is the true class (the correct next token).  </p> <p>More details will be covered in the Advanced Training page</p>"},{"location":"concat/#the-backward-pass","title":"The Backward Pass","text":"<p>Now comes the critical part: telling the model how wrong it was. This is done with:</p> <pre><code>loss.backward()\n</code></pre> <p>This triggers PyTorch\u2019s autograd engine, which walks backwards through the computational graph.  </p> <p>Every tensor operation in PyTorch (matrix multiplies, nonlinearities, normalizations) records how it was computed. During <code>.backward()</code>, PyTorch applies the chain rule of calculus to compute gradients of the loss with respect to every parameter in the model.</p> <p>So, if our model has parameters \u03b8 = {W1, W2, \u2026}, then after <code>loss.backward()</code> we now have stored gradients \u2202L/\u2202W for each parameter. These gradients are stored in each parameter tensor within the <code>.grad</code> attribute, which is a matrix of gradients the shape as the weight matrix. </p> <p>These gradients tell us: \u201cIf you nudge this weight slightly, the loss would go up/down this much.\u201d They are the signals that will guide weight updates.</p>"},{"location":"concat/#the-optimizer-step","title":"The Optimizer Step","text":"<p>With gradients calculated, we actually update the weights. This is the job of the optimizer.  </p> <p>In this project, we use AdamW:</p> <pre><code>optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n</code></pre> <p>AdamW is a variant of stochastic gradient descent that adapts learning rates per parameter and includes proper weight decay. It\u2019s widely used in training transformers.</p> <p>The update cycle is:</p> <pre><code>optimizer.zero_grad()  # reset gradients to zero\n\n# Between these two steps, perform forward pass, calculate loss, back propagation\n\noptimizer.step()       # update parameters using gradients\n</code></pre> <p>Why zero gradients? Because PyTorch accumulates gradients by default. If we didn\u2019t zero them, gradients from multiple steps would pile up.</p> <p>So the full cycle is:</p> <ol> <li>Zero gradients \u2192 prepare for next step.</li> <li>Forward pass \u2192 compute logits and loss.</li> <li>Loss calculation \u2192 use criterion to calculate loss.</li> <li>Backward pass \u2192 compute gradients.  </li> <li>Optimizer step \u2192 update weights.  </li> </ol>"},{"location":"concat/#a-minimal-training-loop","title":"A Minimal Training Loop","text":"<p>Putting everything together:</p> <pre><code>for step in range(num_steps):\n    # Get a batch\n    x, y = dataset_loader.get_batch()   # x: (B, T), y: (B, T)\n\n    # Forward pass\n    logits = model(x)                   # (B, T, C)\n\n    # Compute loss\n    loss = criterion(logits.view(-1, C), y.view(-1))\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Update\n    optimizer.step()\n\n    if step % 100 == 0:\n        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n</code></pre> <p>Granted the actual implementation in <code>simple_llama.pretraining.train</code> file is much more complex, however this is the backbone of training. Every sophisticated training pipeline \u2014 from GPT to LLaMA \u2014 reduces to these few lines.  </p>"},{"location":"concat/#evaluation-and-monitoring","title":"Evaluation and Monitoring","text":"<p>Training is only half the story. We need to know if the model is improving. The simplest way is to track the training loss. Over time, as the model sees more data, loss should decrease, which means the model is getting progressively better at predicting the next token, given an input sequence of tokens.</p> <p>At the very beginning, before the model has learned anything meaningful, predictions are essentially random. In this case, the expected loss can be approximated by the natural logarithm of the vocabulary size, since each token is equally likely under a uniform distribution.  </p> <p>For our project, the vocabulary size is 8192. So if the predictions were truly uniform, the expected initial loss would be:</p> <pre><code>ln(8192) \u2248 9.01\n</code></pre> <p>However, in practice, most parameters in the model (such as linear layers) are initialized from Gaussian distributions using Kaiming or Xavier initialization. This breaks the perfect uniformity and introduces biases. As a result, the observed loss at the very start of training may be slightly higher than the theoretical value \u2014 for example, around 9.2 or 9.3 instead of exactly 9.01.  </p> <p>Why the log of vocab size? (Derivation)</p> <p>Cross-Entropy Loss (CEL) is essentially a Negative Log Likelihood (NLL) loss. For a dataset of size <code>N</code> with true labels <code>y_i</code> and predicted probabilities <code>p(y_i)</code>:</p> <pre><code>CEL = - (1/N) * \u03a3 log(p(y_i))\n</code></pre> <p>For a single example where the true class is <code>c</code>:</p> <pre><code>CEL = - log(p(c))\n</code></pre> <p>If the model predicts uniformly over all <code>V</code> classes, then <code>p(c) = 1/V</code>. Plugging this in:</p> <pre><code>CEL = - log(1/V)\n    = log(V)\n</code></pre> <p>So under uniform predictions, the expected loss equals the log of vocabulary size.</p> <p>Example: - <code>V = 8192</code> - <code>CEL = log(8192)</code> - <code>CEL \u2248 9.01</code> </p> <p>This is the theoretical baseline for random guessing. In practice, initialization bias may push it to ~9.3 at step 0.</p> <p>Training Dynamics</p> <p>As training continues, the loss should decrease steadily. For instance, a drop from ~9.3 to ~3 or ~2 means the model is learning meaningful statistical patterns in the data. Lower loss translates directly into the model being less \u201csurprised\u201d when predicting the next token.</p> <p>Think of it this way: - At loss \u2248 9, the model is basically clueless, assigning ~1/8192 probability to every token. - At loss \u2248 3, the model assigns ~1/20 probability to the correct token on average. - At loss \u2248 1, the model is strongly confident, giving ~1/3 probability to the correct token.</p> <p>Even at a loss of around 3.0, the probabiliy assignment is at ~1/20. That may sound low if one interpret it as \"The model only have a 5% chance of choosing the correct token, for a given sequence\" However that is a bit misleading. In English (or just about all languages) there is natural entropy to it. Vast majority of the time, there are multiple valid answers to a given sequence.  </p> <p>Taking the previous example, we give the model: <code>[The, cat, sat, on, the]</code> and want it to predict the next token. Our true label should be the token corresponding to the word <code>mat</code> however, in general, there isn't just a single right-wrong answer.  Words like <code>floor</code>, <code>ground</code>, <code>couch</code> and such are also completely valid. Hence a probability of 1/20 chance choosing the 'correct' token isn't as bad a it may numerically seem to be. </p> <p>Validation?</p> <p>It\u2019s also common to periodically evaluate on a held-out validation set. This prevents overfitting, since training loss always decreases but validation loss may rise if the model memorizes.  </p> <p>However, in this project, no validation set is used. Why? Because the dataset (50B tokens gathered from FineWebEdu) is completely unique. Training is done in a single epoch \u2014 the model will only see each token sequence once. Under this regime, overfitting is theoretically impossible.  </p> <p>In fact, if a model with ~1B parameters were able to fully overfit on 50B unique tokens, that would be remarkable \u2014 it would essentially mean the model is acting as a form of near-lossless compression of the dataset. From that perspective, it might even be considered desirable. But in practice, that's nearly impossible. Here we will only go through one pass using the 50B tokens, simply track training loss as the main signal of progress.</p>"},{"location":"concat/#a-tiny-generation-example","title":"A Tiny Generation Example","text":"<p>Even early in training, it\u2019s fun (and useful) to test the model by generating text.  </p> <p>We take a prompt, tokenize it, and call a generate function:</p> <pre><code>print(model.generate(\"Once upon a time\", max_new_tokens=20))\n</code></pre> <p>At the start, the output will be nonsense \u2014 the model has learned almost nothing. But as loss decreases, generated samples gradually improve. They start forming grammatical sentences, then coherent paragraphs.</p> <p>This qualitative check is as important as loss curves, because it directly shows what the model is learning.</p>"},{"location":"concat/#bringing-it-all-together","title":"Bringing It All Together","text":"<p>To summarize, each training step does:</p> <ol> <li>Take a batch <code>(B, T)</code> of token IDs.  </li> <li>Run through model \u2192 get logits <code>(B, T, C)</code>.  </li> <li>Compute cross entropy loss with targets <code>(B, T)</code>.  </li> <li>Backpropagate loss \u2192 compute gradients.  </li> <li>Optimizer updates weights.  </li> <li>Zero gradients.  </li> </ol> <p>This loop runs millions of times. At small scale, it might be just tens of thousands of steps. At large scale (GPT\u20113, LLaMA), training can take trillions of tokens.</p> <p>But the essence is always the same. The beauty of the transformer is that all of this complexity \u2014 embeddings, attention, normalization, feedforward layers \u2014 reduces down to the training loop you\u2019ve just seen.</p>"},{"location":"concat/#model_architectureattentionmd","title":"model_architecture\\attention.md","text":""},{"location":"concat/#attention","title":"Attention","text":"<p>Attention is the heart of the transformer. It\u2019s the mechanism that lets the model decide: \u201cWhich other tokens in the sequence are relevant for predicting the next one?\u201d</p>"},{"location":"concat/#vanilla-self-attention","title":"Vanilla Self-Attention","text":"<p>In the vanilla self-attention, we first start out with a given tensor, <code>x</code>, of shape <code>(batch, seq_len, n_embd)</code> as mentioned in the previous <code>embeddings</code> section.</p> <p>Given that tensor, we compute three tensors, Each of them is a linear projection from the input tensor <code>x</code> - Query (Q): what each token is looking for. - Key (K): what each token offers. - Value (V): the information carried by each token.  </p> <p>This is done by either creating 3 separate linear layers in the constructor for the self-attention class: </p> <pre><code>self.q_proj = nn.Linear(n_embd, n_embd)\nself.k_proj = nn.Linear(n_embd, n_embd)\nself.v_proj = nn.Linear(n_embd, n_embd)\n\nself.o_proj = nn.Linear(n_embd, n_embd)  # This will be used and touched upon later on\n</code></pre> <p>Where it can later be invoked to create the Q, K, and V tensors that will be used in attention computation:</p> <pre><code># Takes an input tensor x of shape (batch, seq_len, n_embd) and linearly project it into another tensor, retaining the shape\nq = self.q_proj(x)  \nk = self.k_proj(x)\nv = self.v_proj(x)\n</code></pre> <p>However a more common method is the merge all of those linear layers into a single one, for more efficient computation: <pre><code>self.qkv_proj = nn.Linear(n_embd, 3 * n_embd)\n\nself.o_proj = nn.Linear(n_embd, n_embd)\n</code></pre></p> <p>Then later on, use that to get: </p> <pre><code># Input shape: (batch, seq_len, n_embd), output shape: (batch, seq_len, 3 * n_embd)\nqkv = self.qkv_proj(x)\n\n# Split the tensor along the last dimension \nq, k, v = qkv.chunk(3, dim=-1)\n</code></pre> <p>Both of those will produce the same result. </p> <p>At this point, given an input tensor <code>x</code>, we now have 3 separate tensors <code>q</code>, <code>k</code>, and <code>v</code>, all of the shape <code>(batch, seq_len, n_embd)</code> </p> <p>Next, we 'expand' the tensor from 3d to 4d, using the <code>n_heads</code> hyperparameter.  <code>n_heads</code> defines how many 'heads' we want in attention. Further explanation as to what it does will be given below. </p> <p>We would use the last dimension, <code>n_embd</code> and divide that into <code>(n_heads, head_dim)</code>, based on the formula <code>head_dim = n_embd // n_heads</code> For example, given <code>n_embd=1024</code>, <code>n_heads=16</code>, then <code>head_dim=1024//16=64</code>, meaning we transform our 1024 embedding dimensions into 16 heads, each head have 64 dimension to work with.  It is crucial to add an assertion that <code>n_embd % n_heads == 0</code> to make sure it evenly divides. </p> <p>Given the hyperparameter <code>n_heads</code>, calculate <code>head_dim</code>, then view/reshape the tensor accordingly as follows:  <pre><code># (batch, seq_len, n_embd) -&gt; (batch, seq_len, n_heads, head_dim)\nq = q.view(batch, seq_len, n_heads, head_dim)\nk = k.view(batch, seq_len, n_heads, head_dim)\nv = v.view(batch, seq_len, n_heads, head_dim)\n</code></pre></p> <p>Finally, we swap the dimension for <code>seq_len</code> and <code>n_heads</code></p> <pre><code># (batch, seq_len, n_heads, head_dim) -&gt; (batch, n_heads, seq_len, head_dim)\nq = q.permute(0, 2, 1, 3)\nk = k.permute(0, 2, 1, 3)\nv = v.permute(0, 2, 1, 3)\n</code></pre> <p>Now that our Q, K, V matrices are all of shape <code>(batch, n_heads, seq_len, head_dim)</code>, we apply the self attention formula: </p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + Mask \\right) V \\] <p>The first step is to compute \\(\\(QK^\\top\\)\\) Looking at the shape of the tensors, we will get an output: <code>(batch, n_heads, seq_len, head_dim) @ (batch, n_heads, head_dim, seq_len) = (batch, n_heads, seq_len, seq_len)</code> which we can denote as <code>attn_scores = QK^T</code> </p> <p>Note the <code>(seq_len, seq_len)</code> ending dimensions. That's the reason why people say that attention computation scales quadratically w.r.t the seq_length of the model</p> <p>Next, we apply element-wise division on the computed matrix, where the divisor is the sqrt of <code>d_k</code>, which from the paper refers to the dimensions of each head (<code>head_dim</code>)</p> <p><code>attn_scores = attn_scores / math.sqrt(head_dim)</code></p> <p>The attention scores is normalized by head-dim primarily because of the softmax operation that will immediately take place next. In short, typically the Q, K, V matrices have unit gaussian distribution, when we apply matrix multiplication, the variance scales by <code>head_dim</code> However with how softmax works, values that are more than, say, 3 units less than the maximum in the dimension of interest will be heavily squashed to approximately 0.  In our example scenario, if <code>head_dim=64</code>, that means the std increased from 1 to 8, which would compress the tensor into something similar to a one-hot vector.  </p> <p>Moving along, after we normalize the attention scores, which doesn't change the shape of the tensor, we would need to apply a triangular mask to <code>attn_scores</code>.  </p> <p>Typically, LLMs would add in these mask to prevent the model from 'cheating' by looking at future tokens. Sort of like: If someone gives you a sentence, 'I love my dog' and asks, 'What is the word after 'my'?' It's trivial. The answer is 'dog'. However masking prevents that by, as the name suggests, masking out the next token.  In that same example, the other person would give 'I love my _' and then ask, 'What is the word after 'my'?'</p> <p>Now in this example, let's use the sentence: \"That is a blue dog\" tokenized into <code>[That, is, a, blue, dog]</code> (Since there are only 5 tokens, that means <code>seq_len=5</code> in this example) After going through the embedding layer and above steps, we will reach a tensor of shape <code>(batch, n_heads, 5, 5)</code> Grabbing one of the (5, 5) matrices arbitrarily might look something like: </p> <pre><code>\"That\"   \"is\"     \"a\"      \"blue\"   \"dog\"\n----------------------------------------------\n[ 1.93    1.49     0.90    -2.11     0.68 ]   \u2190 \"That\"\n[-1.23   -0.04    -1.60    -0.75    -0.69 ]   \u2190 \"is\"\n[-0.49    0.24    -1.11     0.09    -2.32 ]   \u2190 \"a\"\n[-0.22   -1.38    -0.40     0.80    -0.62 ]   \u2190 \"blue\"\n[-0.59   -0.06    -0.83     0.33    -1.56 ]   \u2190 \"dog\"\n</code></pre> <p>That tells us how much attention each token pays to each other.  Now these are un-normalized values, so it would be hard to interpret, at least for now. </p> <p>We then apply the triangular mask that prevents tokens to look ahead. It would be something like: </p> <pre><code>[  0   -\u221e   -\u221e   -\u221e   -\u221e ]\n[  0    0   -\u221e   -\u221e   -\u221e ]\n[  0    0    0   -\u221e   -\u221e ]\n[  0    0    0    0   -\u221e ]\n[  0    0    0    0    0 ]\n</code></pre> <p>Applying via element-wise addition: <code>attn_scores = attn_scores + mask</code> </p> <p>The result would now look like:  <pre><code>\"That\"   \"is\"     \"a\"      \"blue\"   \"dog\"\n----------------------------------------------\n[  1.93    -\u221e      -\u221e      -\u221e      -\u221e  ]   \u2190 \"That\"\n[-1.23   -0.04     -\u221e      -\u221e      -\u221e  ]   \u2190 \"is\"\n[-0.49    0.24   -1.11     -\u221e      -\u221e  ]   \u2190 \"a\"\n[-0.22   -1.38   -0.40     0.80     -\u221e  ]   \u2190 \"blue\"\n[-0.59   -0.06   -0.83     0.33   -1.56 ]   \u2190 \"dog\"\n</code></pre></p> <p>we pass it through the softmax function to transform it into something like a probability distribution. </p> <pre><code>attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n</code></pre> <pre><code>\"That\"   \"is\"     \"a\"      \"blue\"   \"dog\"\n----------------------------------------------\n[1.0000   0.0000   0.0000   0.0000   0.0000 ]   \u2190 \"That\"\n[0.2330   0.7670   0.0000   0.0000   0.0000 ]   \u2190 \"is\"\n[0.2759   0.5753   0.1488   0.0000   0.0000 ]   \u2190 \"a\"\n[0.2032   0.0632   0.1699   0.5637   0.0000 ]   \u2190 \"blue\"\n[0.1566   0.2658   0.1236   0.3942   0.0596 ]   \u2190 \"dog\"\n</code></pre> <p>As you can see, starting out with the token corresponding to 'That', it can only pay attention to itself, since it's the first token in the sequence Next is the word 'is', which splits the attention between 'That' and itself So on and so forth. This tells the model how much each token attends to each other. </p> <p>At this point, the <code>attn_weights</code> tensor is still of shape <code>(batch, n_heads, seq_len, seq_len)</code> since both normalization and softmax doesn't change the tensor shape  </p> <p>Now we process the final steps: </p> <pre><code># (batch, n_heads, seq_len, seq_len) @ (batch, n_heads, seq_len, head_dim) = (batch, n_heads, seq_len, head_dim)\nattn_output = attn_weights @ v\n\n# (batch, n_heads, seq_len, head_dim) -&gt; (batch, seq_len, n_heads, head_dim) -&gt; (batch, seq_len, n_embd)\nattn_output = attn_output.permute(0, 2, 1, 3).view(batch, seq_len, n_embd)\n\nreturn self.o_proj(attn_output)\n</code></pre> <p>The final step here is to first matrix multiply with the <code>v</code> tensor to get the tensor of shape <code>(batch, n_heads, seq_len, head_dim)</code> We revert the permutation and viewing to get back the original input shape <code>(batch, seq_len, n_embd)</code> Finally, apply the output projection matrix to <code>attn_output</code> before returning. </p> <p>So what's the purpose of output matrix? </p> <p>One can think of it as a way to combine the information that each head learned.  Recall that when we apply attention, we use multiple heads. Each head process their own 'chunk' of embedding dimensions compeltely separately from each other.  It's beneficial to allow them to learn their own information, however at the end, we merely concatenate them together. </p> <p>The final output projection matrix allows the information to get 'aggregated' and combined. </p>"},{"location":"concat/#in-this-project-implementation","title":"In This Project: Implementation","text":"<p>In <code>MHSelfAttention</code>, queries, keys, and values are created together, where it's first viewed then chunked along the last dimension:</p> <pre><code>qkv = self.qkv_linear(x)  \nq, k, v = qkv.view(batch, seq_len, n_heads, 3 * h_dim).chunk(3, dim=-1)\n</code></pre> <ul> <li>Each input token embedding is linearly projected into Q, K, V vectors.  </li> <li>Shape after splitting: <code>(batch, seq_len, n_heads, h_dim)</code>.  </li> </ul> <p>Then, before computing attention, Rotary Position Embeddings (RoPE) are applied to Q and K:</p> <pre><code>q = apply_rotary_embeddings(q, freqs_complex)\nk = apply_rotary_embeddings(k, freqs_complex)\n</code></pre> <p>Why? Token embeddings alone tell the model what a word is, but not where it appears. Without positional info,  </p> <ul> <li>\u201cThe cat sat on the mat.\u201d  </li> <li>\u201cThe mat sat on the cat.\u201d  </li> </ul> <p>would look identical.</p> <p>Instead of adding positional vectors (like the original Transformer\u2019s sinusoidal method), RoPE rotates Q and K in the complex plane by an amount proportional to their position. This makes attention directly sensitive to relative distances between tokens.</p> <p>For our purposes, you can think of RoPE as: \u201ca lightweight operation on Q and K that encodes order, without changing tensor shapes.\u201d</p> <p>(If you want to dive deeper, check the RoPE paper on arxiv.)</p> <p>Next, permute the tensors then apply scaled dot-product attention:</p> <pre><code>q = q.permute(0, 2, 1, 3)\nk = k.permute(0, 2, 1, 3)\nv = v.permute(0, 2, 1, 3)\n\nscores = q @ k.transpose(-2, -1) / sqrt(h_dim)\nmask = torch.tril(torch.ones((seq_len, seq_len), device=self.device))\nscores = scores.masked_fill(~mask, float('-inf'))\nweights = F.softmax(scores, dim=-1)\nout = weights @ v\n</code></pre> <ul> <li>The causal mask ensures each token can only attend to past tokens (left-to-right).  </li> <li>Softmax converts similarity scores into weights.  </li> <li>Weighted sum with V produces the final attended representation.</li> </ul> <p>Finally, results from all heads are concatenated and passed through a linear projection to mix information.</p>"},{"location":"concat/#notes","title":"Notes","text":"<ol> <li> <p>Why not Multi-Query Attention?    The original LLaMA-2 paper uses multi-query attention (MQA), where all heads share the same K and V but have separate Q.    This greatly reduces KV-cache memory usage, which is important for scaling to very large models and efficient inference.    For this project, memory pressure from KV-cache isn\u2019t a bottleneck, so standard multi-head attention is simpler and sufficient.</p> </li> <li> <p>What about DeepSeek\u2019s MLA?    This project includes an optional implementation of Multi-Head Latent Attention (MLA), which is a refinement that reduces KV-cache memory even further while keeping multiple latent spaces.    It\u2019s more efficient than MQA, but again \u2014 KV-cache isn\u2019t the limiting factor here.    Since the focus is educational clarity, SimpleLLaMA sticks with classic multi-head attention.</p> </li> </ol>"},{"location":"concat/#model_architecturedecoder_blockmd","title":"model_architecture\\decoder_block.md","text":""},{"location":"concat/#transformer-decoder-block","title":"Transformer Decoder Block","text":"<p>The decoder block is the fundamental building unit of the transformer. Each block combines attention, feedforward networks, normalization, and residual connections into a repeatable structure.</p>"},{"location":"concat/#structure-of-a-decoder-block","title":"Structure of a Decoder Block","text":"<p>A decoder block has two main parts:</p> <ol> <li>Multi-Head Self-Attention (MHA) \u2192 lets tokens exchange information.  </li> <li>Feedforward Network (FFN) \u2192 transforms the attended features into richer representations.  </li> </ol> <p>Surrounding these are: - RMSNorm \u2192 stabilizes training by normalizing activations. - Residual Connections \u2192 ensure information from earlier layers isn\u2019t lost.  </p> <p>The primary block flow is:</p> <pre><code>Input \u2192 Norm \u2192 Attention \u2192 Residual \u2192 Norm \u2192 Feedforward \u2192 Residual \u2192 Output\n</code></pre> <p>This \u201cpre-norm\u201d setup (normalize before each sub-layer) is known to improve stability in deep transformers.</p>"},{"location":"concat/#example-walkthrough","title":"Example Walkthrough","text":"<p>Let\u2019s step through what happens inside one decoder block. Suppose we have an input tensor <code>x</code> of shape <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"concat/#1-first-normalization","title":"1. First Normalization","text":"<p><pre><code>h = self.norm1(x)\n</code></pre> - RMSNorm is applied to <code>x</code>. - This ensures the activations are scaled to a stable range before entering attention. - Unlike LayerNorm, RMSNorm does not recenter the mean \u2014 it only rescales variance.  </p>"},{"location":"concat/#2-multi-head-self-attention","title":"2. Multi-Head Self-Attention","text":"<p><pre><code>attn_out = self.attention(h, freqs_complex)\n</code></pre> - Each token produces query, key, and value vectors. - Rotary Position Embeddings (RoPE) are applied to Q and K to inject positional info. - Attention computes how strongly each token attends to others in the sequence. - The output has the same shape as the input: <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"concat/#3-first-residual-connection","title":"3. First Residual Connection","text":"<p><pre><code>h = x + attn_out\n</code></pre> - Here we add the original input <code>x</code> back to the attention output. - This is called a residual connection (or skip connection).  </p> <p>Why is this important? - Imagine stacking dozens of layers. Without skip connections, the network could \"forget\" the original signal after being transformed multiple times. - By adding <code>x</code> back, we preserve the original information while also giving the model access to the new transformed features from attention. - During backpropagation, residuals also help gradients flow more smoothly, preventing vanishing or exploding gradients. - In practice, you can think of it as: the model learns adjustments (deltas) on top of the original input, instead of rewriting it from scratch every time. </p>"},{"location":"concat/#4-second-normalization","title":"4. Second Normalization","text":"<p><pre><code>h_norm = self.norm2(h)\n</code></pre> - Again, normalize before the next sublayer. - This keeps the values stable before passing into the FFN.  </p>"},{"location":"concat/#5-feedforward-network","title":"5. Feedforward Network","text":"<p><pre><code>ffn_out = self.ffwd(h_norm)\n</code></pre> - Input passes through a SwiGLU feedforward network (as described in detail in the FFN doc). - Adds nonlinearity and transformation capacity. - Output shape: <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"concat/#6-second-residual-connection","title":"6. Second Residual Connection","text":"<p><pre><code>out = h + ffn_out\n</code></pre> - Again, the skip connection ensures that the block doesn\u2019t overwrite the information coming from the attention stage. - Instead, it layers on additional transformations from the FFN. - By the time you stack many decoder blocks, each one is contributing refinements while keeping the original context intact. - This makes the network much more robust and trainable.  </p> <p>Final output shape: <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"concat/#in-this-project","title":"In This Project","text":"<ul> <li>Attention type: defaults to standard multi-head self-attention, with optional MLA for efficiency.  </li> <li>Normalization: RMSNorm used everywhere (simpler than LayerNorm, but empirically stable).  </li> <li>Activation: SiLU-based feedforward (SwiGLU).  </li> <li>Dropout: applied after projections, mainly used during fine-tuning (SFT/RLHF).  </li> <li>Residuals: used after both the attention and FFN sublayers.  </li> </ul> <p>Together, these form the repeating backbone of the SimpleLLaMA model. By stacking many of these blocks, the network can build increasingly complex representations of text sequences.</p>"},{"location":"concat/#model_architectureembeddingsmd","title":"model_architecture\\embeddings.md","text":""},{"location":"concat/#embeddings","title":"Embeddings","text":"<p>Embeddings are the first step in turning discrete tokens (integers from tokenization) into continuous vectors for a neural network to process.</p>"},{"location":"concat/#token-embeddings","title":"Token Embeddings","text":"<p>After tokenization, each word or subword is represented by an integer ID. But LLMs don\u2019t work directly with these integers. Instead, we map each token ID into a dense vector of fixed size (the embedding dimension).</p> <p>In PyTorch, this is done with an <code>nn.Embedding</code> layer. In the <code>LLaMaTransformer</code> class from this project, you\u2019ll see:</p> <pre><code>self.embeddings = nn.Embedding(tokenizer.get_vocab_size(), n_embd)\n</code></pre> <p>(Often times the embedding dimensionality of the model, <code>n_embd</code> in this case, is referred to under different names. Common ones include <code>embedding_dim</code>, <code>d_model</code>, and <code>hidden_size</code>)</p> <p>Here, an embedding layer is created, which serves as a lookup table for the model. It takes in two primary values, <code>vocab_size</code> and <code>n_embd</code> and create a matrix of shape <code>(vocab_size, n_embd)</code> Each row corresponds to a token ID and is a trainable vector. For example:</p> <ul> <li>Token \"I\" \u2192 73 \u2192 [0.12, -0.44, 1.05, ...]</li> <li>Token \"an\" \u2192 256 \u2192 [1.33, 0.05, -0.72, ...]</li> </ul> <p>Both will be map to unique vectors, all of which will be of length <code>n_embd</code> At initialization, these vectors are random. During training, the model adjusts them so that it learns semantic relationship between tokens.  </p> <p><code>n_embd</code> is a crucial hyperparameter when creating a LLM. It essentially gives the model the flexibility of how much 'semantics' each token can hold.   </p> <p>For example, say the word <code>man</code> and <code>woman</code> can be represented by a single token, <code>1098</code> and <code>1290</code> respectively.  Passing those through the embedding layer, the model will grab the vector at row index of <code>1098</code> to represent that as man, and row index <code>1290</code> for woman</p> <p>Their vectors will differ, but both have shape <code>(n_embd,)</code>. You can think of each dimension in this vector space as encoding some abstract feature the model has learned. For example, one combination of dimensions might capture gender-like differences (man vs. woman), while another might capture whether something is an animate being or an object. However this is just a simplified way of explanation. In reality, these dimension are polysemantic and is much more complex. (Should include explanation that each value is a dimension as well?)</p> <p>Once we convert our list of tokens into a list of vectors, we can proceed with passing that to the Decoder Block.</p>"},{"location":"concat/#embeddings-in-this-project","title":"Embeddings in This Project","text":"<ul> <li>Embedding dimension (<code>n_embd</code>) is configurable (e.g., 768, 1024, or higher).  </li> <li>RoPE is used for positional encoding by default, following LLaMA.  </li> <li>Initialization: embeddings start random and are updated through backpropagation.  </li> <li>Tied weights: this project experimented with tying embeddings to the final output projection layer (a trick used in some models). But in practice, training became unstable, so it was disabled.</li> </ul>"},{"location":"concat/#walkthrough-example","title":"Walkthrough Example","text":"<p>Let\u2019s walk through a toy example with the sentence:  </p> <p>\u201cI love dogs\u201d </p> <p>Step 1. Tokenization </p> <p>Using an arbitrary word-level tokenizer, each word is mapped to an integer ID:  </p> <ul> <li><code>\"I\"</code> \u2192 73  </li> <li><code>\"love\"</code> \u2192 786  </li> <li><code>\"dogs\"</code> \u2192 2934  </li> </ul> <p>So the input sequence is:  </p> <pre><code>[73, 786, 2934]\n</code></pre> <p>Step 2. Embedding Matrix </p> <p>When we create an <code>nn.Embedding(vocab_size, embedding_dim)</code> layer, it internally builds a big lookup table (a matrix).  </p> <ul> <li>Shape = <code>(vocab_size, embedding_dim)</code> </li> <li>Each row index corresponds to a token ID.  </li> <li>Each row contains a vector of length <code>embedding_dim</code>.  </li> </ul> <p>For this example, let\u2019s set <code>embedding_dim = 8</code>. That means every token ID will be mapped to an 8-dimensional vector.  </p> <p>A (very small) portion of the embedding matrix might look like this at initialization (values are random floats):  </p> <pre><code>0:     [ 0.11, -0.07,  0.45,  0.02, -0.33,  0.19, -0.48,  0.05]\n1:     [-0.21,  0.34, -0.11, -0.08,  0.27, -0.39,  0.17, -0.43]\n2:     [ 0.09,  0.13,  0.28, -0.47, -0.36,  0.22,  0.41, -0.18]\n3:     [-0.15,  0.54,  0.28,  0.12, -0.41, -0.41, -0.53,  0.44]\n4:     [ 0.12,  0.25, -0.58,  0.56,  0.4,  -0.35, -0.38, -0.38]\n5:     [-0.23,  0.03, -0.08, -0.25,  0.13, -0.43, -0.25, -0.16]\n...\n73:    [ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27]\n...\n786:   [-0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04]\n...\n2934:  [ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33]\n...\n</code></pre> <p>Step 3. Lookup </p> <p>Now, to embed our sentence <code>[73, 786, 2934]</code>, the embedding layer simply selects the rows at those indices:  </p> <ul> <li>Token ID 73 (\u201cI\u201d) \u2192 <code>[ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27 ]</code> </li> <li>Token ID 786 (\u201clove\u201d) \u2192 <code>[ -0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04 ]</code> </li> <li>Token ID 2934 (\u201cdogs\u201d) \u2192 <code>[ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33 ]</code> </li> </ul> <p>Step 4. Output Tensor </p> <p>Stacking them together, the embedding layer outputs a tensor:  </p> <pre><code>[\n  [ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27 ],   # \"I\"\n  [ -0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04 ], # \"love\"\n  [ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33 ]   # \"dogs\"\n]\n</code></pre> <p>Shape = <code>(3, 8)</code> \u2192 3 tokens, each represented by an 8-dimensional vector.</p> <p>Essentially, given an input 1d tensor of tokens, which the number of tokens is often referred to as <code>(seq_len,)</code>, we transform it into a tensor of shape <code>(seq_len, n_embd)</code> </p> <p>In this example, it is <code>(3, 8)</code></p> <p>This is the format that gets passed on to the Decoder Block. </p> <p>A very important note is that there's almost always a third dimension, called a Batch dimension.  This allows parallel processing, which makes training much faster.  Batch dimension is always the very first dimension, so the shape of output tensor is <code>(batch, seq_len, n_embd)</code> In this case, since we only have a single example sentence, batch dimension value would be 1, which is</p> <p><code>(1, 3, 8)</code></p>"},{"location":"concat/#model_architectureend_to_endmd","title":"model_architecture\\end_to_end.md","text":""},{"location":"concat/#end-to-end-walkthrough","title":"End-to-End Walkthrough","text":"<p>Now at this point, most of the aspects of the architecture and pipeline have been covered in detail. This final page will be used to tie everything together and give a more thorough, step-by-step overview of how all the parts interact to form a working large language model.  </p>"},{"location":"concat/#1-from-raw-text-to-tokens","title":"1. From Raw Text to Tokens","text":"<p>We first start out with a massive corpus of text \u2014 think in the scale of hundreds of gigabytes upwards, containing billions or even trillions of tokens. This corpus is gathered from sources like books, Wikipedia, academic papers, code repositories, and curated parts of the internet.  </p> <p>But raw text isn\u2019t useful to the model. The model only works with numbers, so the very first step is to tokenize this text using a pretrained tokenizer.  </p> <p>For example, suppose we have the sentence:  </p> <pre><code>\"The quick brown fox jumps over the lazy dog.\"\n</code></pre> <p>The tokenizer will break this into smaller units (subwords or characters depending on the algorithm) and then convert each into an integer ID.  </p> <p>That means something like:  </p> <pre><code>[\"The\", \" quick\", \" brown\", \" fox\", \" jumps\", \" over\", \" the\", \" lazy\", \" dog\", \".\"]\n\u2192 [1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209]\n</code></pre> <p>Now the sentence is represented as a sequence of integers. This is the form that the neural network can actually process.  </p>"},{"location":"concat/#2-batching-and-shaping-the-data","title":"2. Batching and Shaping the Data","text":"<p>Instead of feeding one sentence at a time, training uses mini-batches to process many sequences in parallel. This is crucial for efficiency on GPUs/TPUs.  </p> <p>Suppose we have a long stream of tokens like:  </p> <pre><code>[1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209, 954, 4461, 3546, 206, 4401, ...]\n</code></pre> <p>If we set:</p> <ul> <li><code>batch_size = 2</code> </li> <li><code>seq_len = 6</code> </li> </ul> <p>We would take the first <code>batch_size * seq_len = 12</code> tokens and reshape them into a <code>(batch, seq_len)</code> tensor:  </p> <pre><code>batch_size = 2\nseq_len = 6\ntokens = [1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209, 954, 4461, 3546, 206, 4401, ...]\n\ninput_tokens = torch.tensor(tokens[:batch_size * seq_len]).reshape(batch_size, seq_len)\n\n# When printed, input_tokens would look like:\n# tensor([[1202,  850,  149, 4211,  769, 1839],\n#         [3521, 4879, 2035, 1209,  954, 4461]])\n</code></pre> <p>This reshaped tensor is now ready for the model.  </p> <p>In realistic training runs, values are much larger, e.g.: - <code>batch_size = 32</code> (process 32 sequences in parallel) - <code>seq_len = 2048</code> (each sequence is 2048 tokens long)  </p> <p>So the model processes a tensor of shape <code>(32, 2048)</code> in one forward pass.  </p>"},{"location":"concat/#3-passing-through-the-transformer-model","title":"3. Passing Through the Transformer Model","text":"<p>Next, this tensor of token IDs is passed into the transformer model. The model is composed of the architecture we previously touched upon in detail: embeddings, attention, feedforward networks, normalization, and residual connections, all stacked together into many decoder blocks.  </p> <p>Here is the structure of the <code>LLaMaTransformer</code> class that uses all the previous building blocks:  </p> <pre><code>class LLaMaTransformer(nn.Module):\n    def __init__(self,\n                 config: any,\n                 tokenizer: tokenizers.Tokenizer,\n                 device: str):\n\n        super().__init__()\n\n        # === Unpack config ===\n        max_seq_len = config.max_seq_len\n        n_embd = config.n_embd\n        n_heads = config.n_heads\n        n_layers = config.n_layers\n        multiple_of = config.multiple_of\n        # And many more, will omit for documentation\n\n        assert n_embd % n_heads == 0, f\"n_embd ({n_embd}) % n_heads ({n_heads}) must equal 0!\"\n        assert (n_embd // n_heads) % 2 == 0 and qk_rope_head_dim % 2 == 0, \"head_dim must be even for RoPE!\"\n\n\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        self.device = device\n        self.embeddings = nn.Embedding(tokenizer.get_vocab_size(), n_embd)\n        self.decoder_blocks = nn.ModuleList(\n            [DecoderBlock(n_embd=n_embd, n_heads=n_heads, multiple_of=multiple_of, use_mla=use_mla, ...)\n             for _ in range(n_layers)])\n        self.norm = RMSNorm(n_embd, eps)\n        self.final_linear = nn.Linear(n_embd, tokenizer.get_vocab_size(), bias=False)\n\n        h_dim = qk_rope_head_dim if use_mla else n_embd // n_heads\n        self.freq_complex = precompute_theta_pos_frequencies(max_seq_len, h_dim, theta, device)\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n\n        # (batch, seq_len) -&gt; (batch, seq_len, n_embd)\n        h = self.embeddings(x)  # Each token ID mapped to vector\n        freqs_complex = self.freq_complex[:seq_len]\n\n        # Pass through all Decoder Blocks\n        for dec_block in self.decoder_blocks:\n            h = dec_block(h, freqs_complex)\n\n        h = self.norm(h)\n        return self.final_linear(h)  # (batch, seq_len, n_embd) -&gt; (batch, seq_len, vocab_size)\n</code></pre>"},{"location":"concat/#4-step-by-step-inside-the-model","title":"4. Step-by-Step Inside the Model","text":""},{"location":"concat/#41-embedding-layer","title":"4.1 Embedding Layer","text":"<ul> <li>Input: <code>(batch, seq_len)</code> of integers.  </li> <li>Each integer token is mapped into a dense vector of length <code>n_embd</code>.  </li> <li>Output: <code>(batch, seq_len, n_embd)</code> </li> </ul> <p>This is the semantic representation of tokens: instead of just IDs, we now have vectors that carry meaning and relationships.  </p>"},{"location":"concat/#42-positional-information","title":"4.2 Positional Information","text":"<p>We then fetch <code>freqs_complex</code>, which stores precomputed values for Rotary Position Embeddings (RoPE). RoPE encodes the positions of tokens into the attention mechanism, so the model knows order (e.g., difference between \u201cdog bites man\u201d and \u201cman bites dog\u201d).  </p>"},{"location":"concat/#43-decoder-blocks","title":"4.3 Decoder Blocks","text":"<p>The embedded tensor is then passed into the first Decoder Block. Each block applies: - RMSNorm \u2192 normalizes values for stability. - Multi-Head Attention \u2192 lets each token attend to others in the sequence. - Feedforward Network (SwiGLU) \u2192 adds nonlinear transformation capacity. - Residual Connections \u2192 add the original input back to preserve information.  </p> <p>The internal computations change the contents of the tensor but not its shape: it remains <code>(batch, seq_len, n_embd)</code>.  </p> <p>The output of block 1 is passed into block 2, then block 3, and so on, until it passes through all <code>n_layers</code>.  </p>"},{"location":"concat/#44-final-normalization","title":"4.4 Final Normalization","text":"<p>After the last decoder block, the tensor goes through one final RMSNorm layer. This ensures the distribution of activations is stable before the final projection.  </p>"},{"location":"concat/#45-final-linear-layer","title":"4.5 Final Linear Layer","text":"<p>Finally, we project each hidden vector of size <code>n_embd</code> into a vector the size of the vocabulary. - Shape: <code>(batch, seq_len, n_embd) \u2192 (batch, seq_len, vocab_size)</code> </p> <p>This gives logits \u2014 raw scores for each token in the vocabulary.  </p>"},{"location":"concat/#5-from-logits-to-predictions","title":"5. From Logits to Predictions","text":"<p>The logits are not yet probabilities. To turn them into probabilities, we apply softmax across the vocabulary dimension:  </p> <pre><code>probabilities = softmax(logits, dim=-1)\n</code></pre> <p>Now, for each position in the sequence, we get a probability distribution over the vocabulary. For example:  </p> <pre><code>At position 6, \"fox\" might have 0.72 probability, \"dog\" 0.05, \"cat\" 0.02, ...\n</code></pre> <p>During training, we compare these probabilities against the actual next token using cross-entropy loss. This loss guides backpropagation, which updates the model weights via gradient descent.  </p>"},{"location":"concat/#6-training-loop-connection","title":"6. Training Loop Connection","text":"<p>To summarize the training loop connection:  </p> <ol> <li>Start with batch of token sequences.  </li> <li>Forward pass through embeddings \u2192 decoder blocks \u2192 normalization \u2192 linear layer.  </li> <li>Produce logits <code>(batch, seq_len, vocab_size)</code>.  </li> <li>Apply softmax to get probabilities.  </li> <li>Compute loss vs. ground truth next tokens.  </li> <li>Backpropagate gradients.  </li> <li>Update weights with optimizer (AdamW, etc.).  </li> <li>Repeat across billions of tokens until the model converges.  </li> </ol> <p>Over time, the model gradually learns grammar, facts, and semantics purely by predicting the next token.  </p>"},{"location":"concat/#key-takeaway_1","title":"Key Takeaway","text":"<p>This end-to-end flow shows how everything connects: - Tokenization converts raw text into IDs. - Embeddings + RoPE give meaning and order. - Decoder Blocks repeatedly transform and refine the representations. - Final Linear Layer + Softmax produce predictions over the vocabulary. - Loss and Optimization allow the model to learn from its mistakes.  </p> <p>By stacking these stages together, and scaling up with billions of tokens and parameters, we arrive at a large language model capable of generating coherent and context-aware text.  </p>"},{"location":"concat/#model_architecturefeedforwardmd","title":"model_architecture\\feedforward.md","text":""},{"location":"concat/#feedforward-networks-ffn-in-transformers","title":"Feedforward Networks (FFN) in Transformers","text":"<p>When people first learn about Transformers, the attention mechanism usually takes the spotlight. But the feedforward network (FFN) is equally important \u2014 in fact, it often accounts for the majority of parameters in the model.  </p>"},{"location":"concat/#why-do-we-need-feedforward-layers","title":"Why Do We Need Feedforward Layers?","text":"<p>Attention layers are powerful, but they are still fundamentally linear operations (matrix multiplications, weighted sums). A stack of only linear layers would remain a linear model, which cannot approximate complex, nonlinear functions.  </p> <p>The feedforward network adds nonlinearity and capacity to transform information. It allows the model to map representations into a higher-dimensional space, apply nonlinear activation, and then project back down.  </p> <p>In practice, every Transformer block has the structure:  </p> <pre><code>Input \u2192 [Attention] \u2192 [Feedforward] \u2192 Output\n</code></pre> <p>Both attention and FFN are wrapped with normalization and residual connections.</p>"},{"location":"concat/#vanilla-transformer-ffn","title":"Vanilla Transformer FFN","text":"<p>In the original Transformer paper (Vaswani et al., 2017), the FFN was defined as:  </p> <pre><code>FFN(x) = max(0, xW1 + b1)W2 + b2\n</code></pre> <ul> <li>Two linear layers with a ReLU in between.  </li> <li>The hidden dimension is usually set to 4\u00d7 the embedding dimension, then projected back down.  </li> </ul> <p>For example, if embedding size = 1024, the FFN hidden size = 4096.  </p> <p>This \u201cexpand and contract\u201d pattern gives the model strong nonlinear mixing power, because the network has a wide layer to mix features, then projects it back to the model\u2019s working dimension.  </p>"},{"location":"concat/#implementation-example","title":"Implementation Example","text":"<pre><code>class FeedForward(nn.Module):\n    def __init__(self, n_embd: int):\n        super().__init__()\n        self.layer1 = nn.Linear(n_embd, 4 * n_embd)\n        self.layer2 = nn.Linear(4 * n_embd, n_embd)\n\n    def forward(self, x: torch.Tensor):\n        return self.layer2(torch.nn.functional.relu(self.layer1(x)))\n</code></pre> <p>Let\u2019s walk through this carefully: 1. Input tensor <code>x</code> has shape <code>(batch, seq_len, n_embd)</code>. 2. First layer projects it into <code>(batch, seq_len, 4 * n_embd)</code>. 3. Apply ReLU to introduce non-linearity (important because without it, stacking linear layers would still just be linear). 4. Second layer projects it back down to <code>(batch, seq_len, n_embd)</code>.  </p> <p>So while the shape going into and out of the FFN is the same, the hidden computation in between allows the network to express far richer functions.</p>"},{"location":"concat/#llama-style-ffn-swiglu","title":"LLaMA-Style FFN (SwiGLU)","text":"<p>The LLaMA architecture introduced a key modification: instead of the plain ReLU-based FFN, it uses a SwiGLU activation (SiLU-Gated Linear Unit).  </p> <p>Here\u2019s how it looks in code:  </p> <pre><code>class FeedForward(nn.Module):\n    def __init__(self, n_embd: int, multiple_of: int, dropout: float):\n        super().__init__()\n\n        hidden_dim = int(4 * n_embd * (2 / 3))  # Authors of LLaMa used 2/3 of 4*n_embd\n        # Round hidden_dim up to a nicer multiple for efficient GPU utilization\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(n_embd, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, n_embd, bias=False)\n        self.w3 = nn.Linear(n_embd, hidden_dim, bias=False)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor):\n        h = F.silu(self.w1(x)) * self.w3(x)  # SwiGLU block\n        return self.dropout(self.w2(h))\n</code></pre>"},{"location":"concat/#breaking-it-down-step-by-step","title":"Breaking It Down Step by Step","text":"<ul> <li>In the vanilla FFN we had two linear layers.  </li> <li>In LLaMA\u2019s FFN we now have three linear layers. Why? To introduce a gating mechanism.  </li> </ul> <p>The formula looks like this:  </p> \\[ h = \\text{SiLU}(W_1x) \\odot (W_3x) \\] <p>where <code>\u2299</code> is elementwise (Hadamard) multiplication.  </p> <ul> <li><code>W1x</code>: main transformation path.  </li> <li>Apply SiLU activation (smooth, like ReLU but differentiable everywhere).  </li> <li><code>W3x</code>: produces a second transformed version of <code>x</code>, used as a gate.  </li> <li>Multiply them elementwise: the gate decides how much of each hidden unit passes through.  </li> </ul> <p>This means: - If a value in <code>W3x</code> is near 0 \u2192 the signal from <code>W1x</code> gets suppressed. - If large \u2192 it amplifies the signal. - If negative \u2192 it can flip the sign of the signal.  </p> <p>So the gating function lets the model regulate the flow of information more flexibly than a simple ReLU cutoff.</p>"},{"location":"concat/#parameter-balancing","title":"Parameter Balancing","text":"<p>But wait \u2014 adding a third projection layer means more parameters, right? Yes, but the authors balanced this by shrinking the hidden dimension.  </p> <ul> <li>Vanilla FFN parameters: about <code>8 * n_embd\u00b2</code>.   (from <code>(n_embd \u00d7 4n_embd) + (4n_embd \u00d7 n_embd)</code>).  </li> <li>LLaMA FFN: uses hidden_dim = <code>int(4 * n_embd * 2/3)</code>.   With three projections (<code>w1</code>, <code>w2</code>, <code>w3</code>), total \u2248 <code>8 * n_embd\u00b2</code>.  </li> </ul> <p>So both end up with roughly the same parameter budget, but LLaMA gets more expressive power via gating.  </p>"},{"location":"concat/#shapes-in-action","title":"Shapes in Action","text":"<ul> <li>Input: <code>(batch, seq_len, n_embd)</code> </li> <li>After <code>w1</code> and <code>w3</code>: <code>(batch, seq_len, hidden_dim)</code> </li> <li>After SiLU + elementwise product: <code>(batch, seq_len, hidden_dim)</code> </li> <li>After <code>w2</code>: <code>(batch, seq_len, n_embd)</code> </li> </ul> <p>Input and output shapes match the original \u2014 only the internal transformation is richer.  </p>"},{"location":"concat/#multiple-of-trick","title":"Multiple-of Trick","text":"<p>The <code>multiple_of</code> hyperparameter ensures <code>hidden_dim</code> is divisible by a convenient number (like 64, 128, 256). This is purely for GPU efficiency: matrix multiplications run faster on dimensions aligned to powers of two.  </p> <p>For example: - Without adjustment: <code>n_embd=1024</code> \u2192 hidden_dim = 2730. - With <code>multiple_of=128</code>: hidden_dim becomes 2816, which is far more efficient for hardware.  </p>"},{"location":"concat/#dropout","title":"Dropout","text":"<p>At the end, dropout is applied. This isn\u2019t heavily used during large-scale pretraining, but it becomes important in fine-tuning (SFT, RLHF) to regularize and prevent overfitting when datasets are smaller.  </p>"},{"location":"concat/#summary_1","title":"Summary","text":"<ul> <li>Feedforward layers provide the nonlinear expressiveness that attention alone cannot.  </li> <li>Vanilla Transformer FFN: Linear \u2192 ReLU \u2192 Linear, with a 4\u00d7 expansion.  </li> <li>LLaMA FFN: SwiGLU-style (SiLU + gating with a third linear layer).  </li> <li>Gating allows richer feature modulation (suppress, amplify, flip).  </li> <li>Parameter count is balanced to stay ~<code>8 * n_embd\u00b2</code>.  </li> <li>Input/output shapes stay the same, but the internal computation is more expressive.  </li> <li>Most of a Transformer\u2019s parameters live in these FFNs \u2014 they are just as crucial as attention.  </li> </ul>"},{"location":"concat/#model_architecturenormalizationmd","title":"model_architecture\\normalization.md","text":""},{"location":"concat/#normalization","title":"Normalization","text":"<p>When training deep neural networks, one of the recurring problems is that activations can either explode (grow without bound) or vanish (shrink toward zero) as they pass through many layers. This makes optimization unstable and slows down learning.  </p> <p>Another issue is internal covariate shift \u2014 the idea that the distribution of activations keeps changing as each layer is updated during training, which makes it harder for later layers to adapt.  </p> <p>Internal Covariate Shift can be thought of as follows: </p> <p>Imagine you\u2019re a student preparing for math exams.  </p> <ul> <li>On the first day, you get an Algebra I exam. You do your best, turn it in, get feedback from the teacher, and feel ready to improve for the next one.  </li> <li>Based on the feedback, you adjust your study strategy and expect another Algebra test.  </li> <li>But the next exam you receive is suddenly Calculus \u2014 a totally different level of difficulty. All the preparation you just did no longer matches what you\u2019re being tested on.  </li> </ul> <p>This is what happens in deep neural networks without normalization. Each hidden layer is like a student preparing for its next \u201cexam\u201d (the next round of inputs). After every weight update, the distribution of outputs from the previous layer shifts. That means the \u201cexam\u201d the next layer sees can suddenly look very different than what it was trained on.  </p> <p>As the network gets deeper, these unexpected shifts compound, making training unstable. Layers spend more time re-adapting to constantly changing input distributions rather than actually learning useful features.</p> <p>Normalization layers (like BatchNorm, LayerNorm, etc.) fix this problem. They act like a teacher who ensures that every new exam stays at the same Algebra level \u2014 same general difficulty, same type of questions \u2014 just slightly adjusted each time. This consistency allows each layer to steadily improve rather than getting thrown off by wild distribution shifts.</p> <p>In short: - Without normalization: \u201cI prepared for Algebra, but got Calculus.\u201d - With normalization: \u201cI keep getting Algebra, just with different numbers.\u201d  </p> <p>Normalization layers stabilize the distribution of activations, keep gradients more predictable, and generally allow networks to train deeper and faster.</p>"},{"location":"concat/#layer-normalization-layernorm","title":"Layer Normalization (LayerNorm)","text":"<p>In Transformers (like the original paper \u201cAttention Is All You Need\u201d), the normalization method of choice was Layer Normalization (LayerNorm).</p> <p>How it works: - Given a tensor <code>x</code> of shape <code>(batch, seq_len, n_embd)</code>, LayerNorm normalizes across the embedding dimension for each token. - For each token vector, it computes the mean and variance across its <code>n_embd</code> values. - The normalized vector is then scaled and shifted by learnable parameters (<code>gamma</code> and <code>beta</code>).  </p> <p>Mathematically:</p> <pre><code>LayerNorm(x) = gamma * (x - mean(x)) / sqrt(var(x) + eps) + beta\n</code></pre> <p>Where: - <code>mean(x)</code> and <code>var(x)</code> are computed over the last dimension (<code>hidden_dim</code>). - <code>gamma</code> and <code>beta</code> are learnable parameters that allow the model to \"undo\" normalization if needed. - <code>eps</code> is a small constant to prevent division by zero.  </p> <p>Effectively, LayerNorm re-centers (subtracts the mean) and re-scales (divides by standard deviation). This ensures each token\u2019s hidden vector has roughly zero mean and unit variance before being rescaled.</p> <p>LayerNorm is still widely used in most Transformer implementations, but it comes with a computational cost: subtracting means, computing variances, and performing square roots for every vector.</p>"},{"location":"concat/#root-mean-square-normalization-rmsnorm","title":"Root Mean Square Normalization (RMSNorm)","text":"<p>LLaMA and some later models (including this implementation) instead use RMSNorm, a simpler but surprisingly effective alternative.</p> <p>The key idea: Research showed that re-centering (subtracting the mean) is less important than re-scaling (fixing the variance). In other words, what really stabilizes activations is making sure their magnitude (energy) doesn\u2019t blow up, not whether they\u2019re mean-centered.</p> <p>So RMSNorm skips the mean subtraction entirely.</p> <p>Mathematically:</p> <pre><code>RMSNorm(x) = (x / RMS(x)) * weight\n</code></pre> <p>Where: - <code>RMS(x) = sqrt(mean(x^2))</code> - <code>weight</code> is a learnable scaling vector (similar to <code>gamma</code> in LayerNorm). - No <code>beta</code>, since there\u2019s no re-centering.  </p> <p>This implementation of <code>RMSNorm</code> shows this clearly:</p> <pre><code>class RMSNorm(nn.Module):\n    def __init__(self, n_embd: int, eps: float):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(n_embd))\n\n    def _norm(self, x: torch.Tensor):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x: torch.Tensor):\n        return self.weight * self._norm(x.float()).type_as(x)\n</code></pre> <p>Step by step: 1. Square all elements (<code>x.pow(2)</code>). 2. Take the mean across the last dimension (<code>mean(-1, keepdim=True)</code>). 3. Add a tiny epsilon for numerical stability. 4. Take the reciprocal square root (<code>rsqrt</code>). 5. Multiply back with the original <code>x</code> \u2192 now the activations are normalized to unit RMS. 6. Finally, scale by a learnable weight vector (one parameter per hidden dimension).  </p> <p>This achieves the stabilization effect of LayerNorm but with slightly fewer operations.</p>"},{"location":"concat/#why-rmsnorm","title":"Why RMSNorm?","text":"<p>So why is RMSNorm used in models like LLaMA (and this project)?</p> <ol> <li> <p>Efficiency    RMSNorm is simpler than LayerNorm. It removes the mean subtraction, which slightly reduces compute and memory usage \u2014 especially important when running at very large scales.</p> </li> <li> <p>Empirical stability    Experiments (see the RMSNorm paper) showed that mean-centering didn\u2019t improve stability much. The key factor was scaling by the variance (or root mean square).</p> </li> <li> <p>Better fit for Transformers    Since Transformers already have residual connections and other stabilizing tricks, skipping the mean step doesn\u2019t hurt \u2014 and in fact, models trained with RMSNorm often match or exceed performance of LayerNorm.</p> </li> </ol>"},{"location":"concat/#putting-it-together","title":"Putting It Together","text":"<p>In this project, RMSNorm appears inside the Decoder Block in two places:</p> <pre><code>h = x + self.attention(self.norm1(x), freqs_complex)\nout = h + self.ffwd(self.norm2(h))\n</code></pre> <p>Here, <code>norm1</code> and <code>norm2</code> are both <code>RMSNorm</code> layers. They normalize activations before passing them into attention and feedforward sublayers. This is known as a Pre-Norm Transformer design (normalize before the sublayer, then add the residual). Pre-Norm improves gradient flow compared to the original Post-Norm design.</p>"},{"location":"concat/#summary_2","title":"Summary","text":"<ul> <li>Normalization stabilizes deep networks, preventing exploding/vanishing activations.  </li> <li>Transformers originally used LayerNorm, which re-centers and re-scales each hidden vector.  </li> <li>RMSNorm drops the mean subtraction, keeping only the re-scaling step.  </li> <li>Despite being simpler, RMSNorm works just as well (sometimes better) and is used in LLaMA and your implementation.  </li> <li>In SimpleLLaMA, RMSNorm ensures stable training across all decoder blocks while keeping the implementation lightweight.  </li> </ul>"},{"location":"concat/#model_architectureoverviewmd","title":"model_architecture\\overview.md","text":""},{"location":"concat/#model-architecture-overview","title":"Model Architecture Overview","text":"<p>At the heart of SimpleLLaMA is the transformer architecture \u2014 the same family of models that power GPT, LLaMA, and DeepSeek. Transformers are flexible neural networks designed to process sequences of data (like text), and they\u2019ve become the standard for large language models.</p>"},{"location":"concat/#big-picture","title":"Big Picture","text":"<p>When you give the model a prompt, like:</p> <p><code>\"The cat sat on the mat\"</code></p> <p>the pipeline looks like this: ~ 1. Tokenization \u2013 Text is broken into tokens (e.g., \"The\", \" cat\", \" sat\", \" on\", \" the\", \" mat\"). 2. Embeddings \u2013 Each token is turned into a dense vector representation. 3. Transformer Blocks \u2013 A stack of repeated layers where the real learning happens:    - Attention \u2192 figures out relationships between tokens.    - Feedforward \u2192 transforms and mixes the information.    - Residuals + Normalization \u2192 stabilize and speed up training. 4. Output Projection \u2013 Final layer maps hidden states back to the vocabulary. 5. Softmax \u2013 Converts raw scores into probabilities for the next token.  </p> <p></p> <p>Should add a reference to Umar! </p>"},{"location":"concat/#decoder-only-design","title":"Decoder-Only Design","text":"<p>This project uses a decoder-only transformer, which means: - The model only predicts the next token given all tokens before it. - It\u2019s autoregressive: it generates text left to right, one token at a time. - This design is perfect for language modeling, where the task is \u201cpredict what comes next.\u201d</p>"},{"location":"concat/#why-transformers","title":"Why Transformers?","text":"<p>Transformers replaced older sequence models (RNNs, LSTMs) because: - They scale much better with data and compute. - Attention allows the model to directly connect distant tokens (e.g., the start and end of a paragraph). - Parallelization makes them efficient to train on GPUs.</p> <p>In the following sections, we\u2019ll break the model down into its core components: - Embeddings \u2013 how tokens are represented as vectors. - Attention \u2013 how the model connects words together. - Layer Block \u2013 the repeating unit of the transformer. - Output \u2013 how predictions are made.  </p>"},{"location":"concat/#tokenizationalgorithmsmd","title":"tokenization\\algorithms.md","text":""},{"location":"concat/#training-a-tokenizer","title":"Training a Tokenizer","text":"<p>A tokenizer isn\u2019t just a simple rule that splits text on spaces or punctuation \u2014 it\u2019s actually trained on a large text corpus, very much like the LLM itself is trained on text. The idea is to learn how to break down words into pieces in a way that balances efficiency and flexibility.</p> <p>There are different algorithms to do this, but the two most common are:</p> <ul> <li>BPE (Byte Pair Encoding)</li> <li>SentencePiece (often using Unigram LM)</li> </ul> <p>In this section, we\u2019ll focus on BPE because it\u2019s easier to walk through step by step and is widely used in models like GPT-2.</p>"},{"location":"concat/#example-bpe-in-action","title":"Example: BPE in Action","text":"<p>Let\u2019s say our toy text corpus is:</p> <p>\"I love my banana bandana\"</p> <p>We\u2019ll show how BPE gradually learns to compress repeated character pairs into new tokens.</p>"},{"location":"concat/#step-1-characters-bytes","title":"Step 1. Characters \u2192 Bytes","text":"<p>Everything starts with raw characters. BPE first converts each character into its ASCII/UTF-8 byte value.</p> <pre><code>text = \"I love my banana bandana\"\nfor character in text:\n    print(ord(character))\n</code></pre> <p>Resulting sequence of numbers:</p> <p><code>[73, 32, 108, 111, 118, 101, 32, 109, 121, 32, 98, 97, 110, 97, 110, 97, 32, 98, 97, 110, 100, 97, 110, 97]</code></p> <p>Here: \"I\" -&gt; 73 \" \" -&gt; 32 \"l\" -&gt; 108 \"o\" -&gt; 111 \"v\" -&gt; 118 \"e\" -&gt; 101 ...  </p> <p>So now the sentence is just a list of numbers.</p>"},{"location":"concat/#step-2-count-frequent-pairs","title":"Step 2. Count frequent pairs","text":"<p>BPE works by repeatedly finding the most common adjacent pair of symbols and merging it into a new symbol. We start with a sliding window of size 2 and count all pairs in the byte list.</p> <pre><code>count = {}\nbyte_repr = [73, 32, 108, 111, 118, 101, 32, 109, 121, 32, \n             98, 97, 110, 97, 110, 97, 32, 98, 97, 110, 100, 97, 110, 97]\n\nfor i, j in zip(byte_repr[:-1], byte_repr[1:]):\n    pair = (i, j)\n    count[pair] = count.get(pair, 0) + 1\n\nsorted_keys = sorted(count, key=lambda x: count[x], reverse=True)\nfor key in sorted_keys:\n    print(f\"{key} : {count[key]}\")\n</code></pre> <p>The most frequent pair is:</p> <p>(97, 110) : 4</p> <p>This corresponds to the two characters \"a\" (97) and \"n\" (110), which together form \"an\". Looking at our sentence, \u201cbanana bandana,\u201d it makes sense: \"an\" repeats a lot.</p>"},{"location":"concat/#step-3-merge-and-replace","title":"Step 3. Merge and replace","text":"<p>We now assign a new token ID for \"an\", say 256. Then we replace all occurrences of the pair (97, 110) with this new symbol:</p> <p><code>[73, 32, 108, 111, 118, 101, 32, 109, 121, 32, 98, 256, 256, 97, 32, 98, 256, 100, 256, 97]</code></p> <p>The list is shorter \u2014 we compressed the repeated \"an\" pairs.</p>"},{"location":"concat/#step-4-repeat-the-process","title":"Step 4. Repeat the process","text":"<p>We do the same thing again: scan for the most frequent pair, merge, and replace.  </p> <p>Now the top pairs would be: - (32, 98) : 2 - (98, 256) : 2 - (256, 97) : 2  </p> <p>Let\u2019s pick <code>(32, 98)</code> first. Remember, 32 is the ASCII code for a space <code>\" \"</code>, and 98 is <code>\"b\"</code>. Together they represent <code>\" b\"</code>. We can merge them into a new token ID, say <code>257</code>, so now the tokenizer knows <code>\" b\"</code> is a single symbol.  </p> <p>If we run the process again, the frequent pairs update. Now we see pairs like: - (257, 256) : 2 - (256, 97) : 2  </p> <p>This means <code>\" b\"</code> (257) is often followed by <code>\"an\"</code> (256), which together form <code>\" ban\"</code>. We can merge <code>(257, 256)</code> into <code>258</code>, representing <code>\" ban\"</code>.  </p> <p>Each time we merge, the sequence gets shorter and starts capturing bigger chunks of meaning. By repeating this again and again, we eventually build up tokens for frequent pieces of words, until we reach the target vocabulary size.</p>"},{"location":"concat/#step-5-maintain-reverse-mapping","title":"Step 5. Maintain reverse mapping","text":"<p>For decoding back into text, the tokenizer keeps track of a reverse dictionary that remembers what each new token stands for.</p> <p>For example:</p> <p>73  -&gt; \"I\" 32  -&gt; \" \" 108 -&gt; \"l\" 111 -&gt; \"o\" 118 -&gt; \"v\" 101 -&gt; \"e\" ... 256 -&gt; \"an\" 257 -&gt; \" b\" 258 -&gt; \" ban\"  </p> <p>When we want to reconstruct text from tokens, the tokenizer looks up these mappings.</p>"},{"location":"concat/#final-tokenization","title":"Final Tokenization","text":"<p>With this toy tokenizer, the original sentence:</p> <p>\"I love my banana bandana\"</p> <p>might end up tokenized as:</p> <p>[\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"m\", \"y\", \" ban\", \"an\", \"a\", \" ban\", \"d\", \"an\", \"a\"]</p> <p>It\u2019s not very compressed at this tiny scale, but notice how frequent chunks like \"an\" and \" ban\" became their own tokens.  </p>"},{"location":"concat/#recap","title":"Recap","text":"<p>BPE builds a tokenizer by: 1. Starting with characters as symbols. 2. Repeatedly finding the most frequent adjacent pair. 3. Merging that pair into a new token. 4. Updating the text and repeating until the vocabulary reaches a set size.  </p> <p>The result is a subword tokenizer that\u2019s much more efficient than character-level and word-level tokenization, which will be discussed in the next section. This balance is why BPE (or related methods like Unigram LM) became the standard in modern LLMs.</p>"},{"location":"concat/#tokenizationexamplesmd","title":"tokenization\\examples.md","text":""},{"location":"concat/#tokenization-examples","title":"Tokenization Examples","text":"<p>In this section, we\u2019ll look at some more concrete examples of how tokenization works in practice, and why subword tokenization ends up being the most effective choice for LLMs.</p>"},{"location":"concat/#character-level-tokenization","title":"Character-Level Tokenization","text":"<p>Example sentence: \"Hello, how are you doing today?\"</p> <p>At the character level, this becomes: [\"H\",\"e\",\"l\",\"l\",\"o\",\",\",\" \",\"h\",\"o\",\"w\",\" \",\"a\",\"r\",\"e\",\" \",\"y\",\"o\",\"u\",\" \",\"d\",\"o\",\"i\",\"n\",\"g\",\" \",\"t\",\"o\",\"d\",\"a\",\"y\",\"?\"]</p> <p>That\u2019s 31 tokens for a short sentence!  </p> <p>Pros: - Very simple, no training required. - Small vocabulary (if limited to ASCII, just 128 possible tokens).  </p> <p>Cons: - Extremely inefficient: every character, space, and punctuation becomes a token. - Long sequences mean the model uses much more compute to learn the given text. - Context windows get consumed very quickly. Imagine trying to process an entire book one character at a time \u2014 it would explode into millions of tokens. - Doesn\u2019t scale well to languages with large character sets (Chinese, Korean) where the vocabulary would still be huge and sequences still long.</p>"},{"location":"concat/#word-level-tokenization","title":"Word-Level Tokenization","text":"<p>If we instead split on words, we get: [\"Hello\", \"how\", \"are\", \"you\", \"doing\", \"today\", \"?\"]</p> <p>Now it\u2019s only 7 tokens \u2014 much shorter than the character-level case.  </p> <p>Pros: - Far fewer tokens per sentence. - Faster training since each token carries more semantic meaning. - Intuitively matches how humans think about words.  </p> <p>Cons: - Vocabulary explosion: English alone has 100,000+ words, and that number grows quickly once you consider inflections, misspellings, slang, and technical jargon. - Out-of-Vocabulary (OOV) errors: any word not seen during training cannot be encoded at inference. For example, if the word \u201cquantumbotics\u201d appears but wasn\u2019t in the training data, the tokenizer simply has no way to represent it. - Poor handling of morphologically rich languages. For example, Turkish or Finnish can produce many forms of a single root word, each of which would need its own token. - Non-space-delimited languages (like Chinese or Japanese) are a nightmare for word-level tokenization since there\u2019s no clear way to split words. Tokenizing character-by-character just reverts back to the inefficiency of character-level methods.  </p>"},{"location":"concat/#subword-level-tokenization","title":"Subword-Level Tokenization","text":"<p>Subword tokenization finds a balance between the two. The same sentence could become something like: [\"Hello\", \",\", \" how\", \" are\", \" you\", \" do\", \"ing\", \" today\", \"?\"]</p> <p>Now the length is around 9 tokens \u2014 not too long, not too short.  </p> <p>Pros: - Vocabulary size is controllable (usually 30k\u2013100k tokens). - Can represent any word, even unseen ones, by splitting into smaller known pieces. - Captures common roots, prefixes, and suffixes (\u201cwalk\u201d, \u201cwalking\u201d, \u201cwalked\u201d share \u201cwalk\u201d). - Much more efficient than character-level while avoiding the fragility of word-level. - Works across languages, including ones without spaces.  </p> <p>Cons: - More complex to train than character- or word-level. - Some awkward splits still happen (rare words may be broken down oddly). - Token boundaries aren\u2019t always human-intuitive.  </p>"},{"location":"concat/#why-subword-wins","title":"Why Subword Wins","text":"<p>Character-level forces the model to learn spelling and structure from scratch, while word-level makes the vocabulary too large and brittle. Subword-level combines the strengths of both: - Compression is strong enough for efficient training. - Vocabulary is manageable. - Handles unseen or rare words gracefully.  </p> <p>This is why nearly every modern LLM \u2014 GPT, LLaMA, DeepSeek, etc. \u2014 uses a subword tokenizer (BPE, Unigram, or a variant) at its core.</p>"},{"location":"concat/#final-notes-about-bpe","title":"Final Notes About BPE","text":"<p>BPE is a simple but highly efficient algorithm for tokenization, but a few additional caveats to be aware of:</p> <ol> <li> <p>Handling non-ASCII characters.    In multilingual settings, non-ASCII characters may cause problems. If the model predicts invalid continuation tokens, you can end up with \u201cgarbled\u201d outputs (invalid byte artifacts).</p> </li> <li> <p>No notion of words.    BPE works purely on frequency of symbol pairs. It doesn\u2019t know about word boundaries, so sometimes splits can be unintuitive. For example:    \"unhappy\" \u2192 \"un\", \"hap\", \"py\"    While unlikely for very common words, this does illustrate the lack of semantic awareness.</p> </li> <li> <p>Inefficient for rare words.    Since BPE prioritizes frequent patterns, rare words often get split into many tokens.    Example: \"Mississippi\" \u2192 \"Mi\", \"ssi\", \"ssi\", \"ppi\" (4 tokens for 1 word).    Not catastrophic, but inefficient.</p> </li> <li> <p>Whitespace quirks.    Spaces often get \u201cglued\u201d to words (\" dog\" instead of \"dog\"). This is by design, but can look odd.</p> </li> <li> <p>Static once trained.    Once a BPE tokenizer is trained, its vocabulary is fixed. If new words become common later (say a new slang term or product name), they won\u2019t be merged automatically.</p> </li> </ol> <p>For this project, these limitations are minor \u2014 but it\u2019s good to be aware of them. Overall, BPE still provides an excellent trade-off and is much better suited for LLMs than raw character- or word-level tokenization.</p>"},{"location":"concat/#tokenizationoverviewmd","title":"tokenization\\overview.md","text":""},{"location":"concat/#what-is-tokenization","title":"What is Tokenization?","text":"<p>Now that we have a text corpus to work with, the next step is tokenization. Tokenization is the process of converting text into numerical values, since models can only work with numbers as input. Before a model can learn any patterns in language, it needs the text expressed in a consistent numerical form. Tokenization is the bridge between raw text and the numerical world that neural networks actually understand.</p>"},{"location":"concat/#the-big-picture","title":"The Big Picture","text":"<p>Take the example sentence:</p> <p><code>\"Hawaii is great for vacationing.\"</code></p> <p>This same sentence can be broken down in different ways depending on the tokenization approach:</p> <ul> <li>Character-level: <code>[\"H\",\"a\",\"w\",\"a\",\"i\",\"i\",\" \",\"i\",\"s\",\" \",\"g\",\"r\",\"e\",\"a\",\"t\",\" \",\"f\",\"o\",\"r\",\" \",\"v\",\"a\",\"c\",\"a\",\"t\",\"i\",\"o\",\"n\",\"i\",\"n\",\"g\",\".\"]</code></li> <li>Word-level: <code>[\"Hawaii\",\"is\",\"great\",\"for\",\"vacationing\",\".\"]</code></li> <li>Subword-level: <code>[\"Ha\",\"wa\",\"ii \",\"is \",\"great \",\"for \",\"vac\",\"ation\",\"ing\",\".\"]</code></li> </ul> <p>Notice how some words remain whole in the subword case (<code>\"is\"</code>, <code>\"great\"</code>, <code>\"for\"</code>), while others like <code>\"Hawaii\"</code> and <code>\"vacationing\"</code> are broken into pieces. Subword tokenization hits the middle ground between characters and words \u2014 efficient compression without losing flexibility for new or unusual words.</p>"},{"location":"concat/#why-subword-tokenization","title":"Why Subword Tokenization?","text":"<p>Character-level tokenization is too inefficient (long sequences), while word-level tokenization has problems with exploding vocabulary size and out-of-vocabulary words. Subword tokenization avoids both: it keeps vocabulary manageable while still handling unseen words by splitting them into smaller known pieces. That balance is why modern LLMs almost always use subword tokenization.</p> <p>So how do we actually decide the splits? That\u2019s where the tokenizer itself comes in. In the next section, we\u2019ll walk through how tokenizers are trained, starting with the most common approach: Byte Pair Encoding (BPE).</p>"},{"location":"concat/#tokenizationprojectmd","title":"tokenization\\project.md","text":""},{"location":"concat/#tokenization-in-simplellama","title":"Tokenization in SimpleLLaMA","text":"<p>Now that the dataset has been gathered and sharded, the next step is to actually train a tokenizer and use it to encode the data into numerical form. This section walks through how this was done in SimpleLLaMA.</p>"},{"location":"concat/#training-the-tokenizer","title":"Training the Tokenizer","text":"<p>We use a ByteLevel BPE tokenizer for this project, provided by the <code>tokenizers</code> library</p>"},{"location":"concat/#special-tokens","title":"Special Tokens","text":"<p>On top of the usual vocabulary, we add custom tokens to support training:  </p> <ul> <li><code>&lt;SOS&gt;</code> : Start of sequence  </li> <li><code>&lt;EOS&gt;</code> : End of sequence  </li> <li><code>&lt;PAD&gt;</code> : Padding (for batching sequences of different lengths)  </li> <li><code>&lt;UNK&gt;</code> : Unknown token (fallback for anything not in vocab)  </li> <li><code>&lt;SOU&gt;</code> / <code>&lt;EOU&gt;</code> : Mark user messages in dialogue datasets  </li> <li><code>&lt;SOA&gt;</code> / <code>&lt;EOA&gt;</code> : Mark assistant messages  </li> <li><code>&lt;SOT&gt;</code> / <code>&lt;EOT&gt;</code> : Mark templates or system prompts  </li> </ul> <p>These tokens are crucial for supervised fine-tuning and RLHF stages later on, where we want the model to clearly distinguish between different roles and contexts.</p>"},{"location":"concat/#example-script","title":"Example Script","text":"<pre><code>from tokenizers import Tokenizer, models, trainers\nfrom tokenizers.pre_tokenizers import ByteLevel\n\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n\nspecial_tokens = [\"&lt;SOS&gt;\", \"&lt;EOS&gt;\", \"&lt;PAD&gt;\", \"&lt;UNK&gt;\", \"&lt;SOU&gt;\", \"&lt;EOU&gt;\", \"&lt;SOA&gt;\", \"&lt;EOA&gt;\", \"&lt;SOT&gt;\", \"&lt;EOT&gt;\"]\ntrainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=special_tokens, min_frequency=16)\n\nfiles = [\"short_0001.txt\", \"short_0002.txt\", \"short_0003.txt\"]\ntokenizer.train(files, trainer)\ntokenizer.save(\"bpe_8k.json\")\n</code></pre> <p>Note the <code>vocab_size=8192</code> part. This is a value that we can adjust as needed. A vocabulary size of <code>8192</code> means that we undergo compression until we have 8192 mapping in our dict.  One can think of it as a balancer: If vocab size is set too low (e.g. 256), BPE will collaspe into character level tokenization. However if vocab size is too large, like 1 million, it will converge towards something like a word level tokenizer.  Generally, BPE tokenizers have vocabulary size of 32k+, however since we are dealing with ascii only dataset, 8192 works fine. </p>"},{"location":"concat/#key-design-choices","title":"Key Design Choices","text":"<ul> <li>ByteLevel PreTokenizer \u2192 Ensures consistent handling of whitespace. For example, <code>\" dog\"</code> and <code>\"dog\"</code> are treated as distinct.  </li> <li>add_prefix_space=True \u2192 Preserves leading spaces, which otherwise could get lost.  </li> <li>Vocabulary size = 8192 \u2192 Small enough for efficient training, large enough to capture common words and subwords.  </li> <li>min_frequency=16 \u2192 Rare patterns are ignored, preventing the vocabulary from bloating with noise.  </li> </ul>"},{"location":"concat/#encoding-the-dataset","title":"Encoding the Dataset","text":"<p>Once the tokenizer is trained, the next step is to encode the raw text into token IDs. This step converts every <code>.txt</code> shard that was previously gathered from FineWebEdu into a <code>.npy</code> file of integers.  </p> <p>Why? Because: - Encoding once upfront is faster than re-tokenizing on-the-fly. - <code>.npy</code> arrays are lightweight and can be memory-mapped during training. - Smaller storage space on system (~4:1 compression ratio, where assuming 1 byte is needed for each character, assuming 2 bytes per token, reduce storage to 50%)  </p>"},{"location":"concat/#example-script_1","title":"Example Script","text":"<pre><code>from tokenizers import Tokenizer\nimport numpy as np, os\n\ntokenizer = Tokenizer.from_file(\"bpe_8k.json\")\ntokenizer.model.unk_token = \"&lt;UNK&gt;\"  # Set unknown token\n\nsrc_dir, dst_dir = \"short_1800\", \"short_1800_tokens\"\nos.makedirs(dst_dir, exist_ok=True)\n\nfor file in os.listdir(src_dir):\n    if file.endswith(\".txt\"):\n        text = open(os.path.join(src_dir, file)).read()\n        tokens = np.array(tokenizer.encode(text).ids, dtype=np.uint16)\n        np.save(f\"{dst_dir}/{file.replace('.txt','.npy')}\", tokens)\n</code></pre>"},{"location":"concat/#notes_1","title":"Notes","text":"<ul> <li>We set <code>unk_token = \"&lt;UNK&gt;\"</code> as a fallback. In practice, almost all text will map cleanly since we used ByteLevel.  </li> <li>Tokens are stored as <code>uint16</code> because our vocabulary size is &lt; 2**16. This is more memory-efficient than <code>int32</code> or <code>int64</code>.  </li> <li>For a 10,000 character text file, this typically compresses down to ~2,500 tokens, depending on content.  </li> </ul> <p>By the end of this stage, the dataset has gone from raw text \u2192 clean shards \u2192 token IDs, all ready to be fed into the pretraining pipeline. For the remainder of this documentation/tutorial, I will show tokenization on word level for simplicity. </p>"},{"location":"custom_training/pretraining/","title":"Pretraining","text":"<p>Add note about:</p> <p>torch.serialization.add_safe_globals({TrainingConfig})</p> <p>Traceback (most recent call last):   File \"/workspace/SL_Package/simple_llama/finetune/full_sft/finetune.py\", line 116, in      ckpt = torch.load(sft_configs.model_path, map_location=device)   File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 1470, in load     raise pickle.UnpicklingError(_get_wo_message(str(e))) from None _pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, do those steps only if you trust the source of the checkpoint.         (1) In PyTorch 2.6, we changed the default value of the <code>weights_only</code> argument in <code>torch.load</code> from <code>False</code> to <code>True</code>. Re-running <code>torch.load</code> with <code>weights_only</code> set to <code>False</code> will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.         (2) Alternatively, to load with <code>weights_only=True</code> please check the recommended steps in the following error message.         WeightsUnpickler error: Unsupported global: GLOBAL simple_llama.pretraining.config.TrainingConfig was not an allowed global by default. Please use <code>torch.serialization.add_safe_globals([TrainingConfig])</code> or the <code>torch.serialization.safe_globals([TrainingConfig])</code> context manager to allowlist this global if you trust this class/function. <p>Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.</p>"},{"location":"custom_training/pretraining/#pretraining-walkthrough","title":"Pretraining Walkthrough","text":"<p>This section provides a complete, hands-on walkthrough for running the pretraining process of the SimpleLLaMA model. Unlike the earlier sections that explained the architecture, dataset, and optimization theory, this chapter focuses on how to actually execute the training pipeline \u2014 from environment setup to monitoring and resuming runs.</p>"},{"location":"custom_training/pretraining/#1-prerequisites","title":"1. Prerequisites","text":"<p>Before running the pretraining pipeline, confirm that the following requirements are met:</p>"},{"location":"custom_training/pretraining/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>A NVIDIA GPU for faster computation, (cpu would work, but limited to at most millions to tens of millions of parameters), any VRAM would work, even 4GB, but larger the vram, the larger model that can be trained</li> <li>For larger models or higher batch sizes, multi-GPU setups are recommended.</li> <li>Ensure adequate system RAM and disk space for datasets and checkpoints (again, this depends on user setups and size of model to train but at the very minimum should allocate around 10GB of diskspace and a few GB of RAM)</li> </ul> <p>Pull repo from here (placeholder for now)</p>"},{"location":"custom_training/pretraining/#dependencies","title":"Dependencies","text":"<p>Install the dependencies via <code>requirements.txt</code>: <pre><code>pip install -r requirements.txt\n</code></pre> or manually: <pre><code>pip install tokenizers==0.21.1 torch==2.6.0 matplotlib==3.10.6 numpy==2.2.6\n</code></pre></p> <p>Then, within SimpleLLaMA folder, you should see the <code>setup.py</code> file, at that level, run <code>pip install -e .</code> That will add a symbolic link from this package to the <code>site-packages/</code> folder, where -e allows editable install for development. </p>"},{"location":"custom_training/pretraining/#2-dataset-preparation","title":"2. Dataset Preparation","text":"<p>Refer to the <code>Dataset Preparation</code> page back in <code>Pretraining</code> section for details about the dataset.  </p> <p>There are 2 main options: </p> <ol> <li>For a minimal test run, users can use the provided .npy file that contains 25M tokens, used in conjunction with provided <code>bpe_8k.json</code> tokenizer file</li> <li>Users can manually pull and create their own dataset from various sources (C4, ThePile, RedPajama, FineWeb, etc.)</li> </ol>"},{"location":"custom_training/pretraining/#option-1-provided-dataset","title":"Option 1- Provided Dataset","text":"<p>If you desire to just use the provided dataset for simple test/experimental runs, there isn't anything for you to change/modify, proceed to the next section</p>"},{"location":"custom_training/pretraining/#option-2-using-a-chosen-dataset","title":"Option 2- Using a chosen dataset","text":"<p>If you wish to use a chosen dataset, you would need to first need to collect it. </p> <p>Here I will show the process to get the FineWebEdu dataset, which is the one I used for this project. </p> <p>First, use the <code>simple_llama\\dataset\\gather_dataset.py</code> file to collect the dataset (If you want to use another dataset, modify it accordingly or perhaps create one of your own)</p> <p>Next, in the terminal run </p> <p><code>python3 gather_dataset.py --split &lt;dataset_split&gt;</code></p> <p>Where you would replace the split with something like <code>CC-MAIN-2013-20</code>, any valid subset from FineWebEdu, which can be easily checked on the Huggingface website.</p> <p>What the script does is follows:  </p> <ul> <li>Define a shardsize </li> </ul> <p>In this line: </p> <pre><code>    shard_text[bucket_index].append(f\"&lt;SOS&gt;{ex['text']}&lt;EOS&gt;\")\n</code></pre> <p>Also, mention the command to use and also the resulting folder structure</p>"},{"location":"misc/benchmarking/","title":"Misc Overview","text":"<p>benchmarking.md file</p>"},{"location":"misc/inference/","title":"Misc Overview","text":"<p>inference.md file</p>"},{"location":"misc/notes/","title":"Misc Overview","text":"<p>notes.md file</p>"},{"location":"pretraining/dataset/","title":"Dataset Preparation","text":""},{"location":"pretraining/dataset/#dataset-preparation","title":"Dataset Preparation","text":""},{"location":"pretraining/dataset/#1-why-the-dataset-matters","title":"1. Why the Dataset Matters","text":"<p>The dataset is one of the most important factors in pretraining. But what exactly do we mean by \u201cdata\u201d here?</p> <p>In this context, it\u2019s textual information gathered from the internet \u2014 things like blog posts, articles, books, and discussion sites (Reddit being one of the most popular). All of this text is combined into a huge \u201ccorpus,\u201d which developers then clean and process.</p> <p>It\u2019s not just about having a massive amount of data. Quality matters more than quantity. There\u2019s a saying: garbage in, garbage out. If a model is trained on tons of low-quality text (bad grammar, extreme biases, incoherent scraps from random social media posts), then the model will happily learn to imitate that bad text.</p> <p>Of course, \u201chigh-quality text\u201d doesn\u2019t have a strict definition. But generally, removing bias-heavy content, incorrect information, and extreme or repetitive junk makes the dataset more useful for training.</p>"},{"location":"pretraining/dataset/#2-dataset-used","title":"2. Dataset Used","text":"<p>There are a few well-known public datasets often used in LLM training:  </p> <ul> <li>CommonCrawl \u2013 A massive raw dump of the internet, updated monthly. Very messy and unprocessed.</li> <li>C4 \u2013 Colossal Cleaned Common Crawl, a cleaned-up version of CommonCrawl curated by Google.</li> <li>The Pile \u2013 Curated by EleutherAI, it\u2019s a big mix of sources like arXiv, Wikipedia, GitHub, StackExchange, and more.</li> <li>FineWeb \u2013 A cleaned dataset created by Hugging Face, focusing on education-oriented web text dataset derived from CommonCrawl</li> </ul> <p>For this project, the FineWebEdu dataset will be used, which is a subset of FineWeb that's further filtered and processed.</p> <p>FineWeb is a large-scale dataset derived from CommonCrawl, but extensively filtered. The result is a 15 trillion token corpus of much higher quality text.</p> <p></p> <p>As shown above, the FineWeb team applied several filters:  </p> <ul> <li>URL filtering \u2013 remove adult/unsafe sites.</li> <li>Text extraction \u2013 clean raw HTML into usable text.</li> <li>Language filtering \u2013 use a fastText classifier to keep only English text with score \u2265 0.65.</li> <li>LM filtering \u2013 use a smaller language model to toss out very low-quality passages.</li> <li>MinHash deduplication \u2013 remove near-duplicate documents.</li> <li>C4 filters \u2013 apply the same cleaning rules used in Google\u2019s C4 dataset.</li> <li>Custom filters \u2013 e.g., require punctuation, remove pages with lots of tiny lines.</li> <li>PII removal \u2013 scrub out personally identifiable information.</li> </ul>"},{"location":"pretraining/dataset/#3-finewebedu-subset","title":"3. FineWebEdu Subset","text":"<p>FineWebEdu goes even further. It selects content with an educational focus, resulting in two smaller but more useful subsets:</p> <ul> <li>1.3 trillion tokens \u2013 very high educational quality.</li> <li>5.4 trillion tokens \u2013 high educational quality.</li> </ul> <p>How was this done? The FineWeb authors actually fine-tuned a LLaMA-3 70B model to act as a text quality rater. The model was trained to assign each passage a score from 0 to 5, where higher means more \u201ceducational.\u201d</p> <p>Threshold = 3 \u2192 keep only text scoring \u2265 3 \u2192 results in the 1.3T token dataset.</p> <p>Threshold = 2 \u2192 keep only text scoring \u2265 2 \u2192 results in the 5.4T token dataset.</p> <p>So instead of just cleaning mechanically, they used an LLM itself to filter text by \u201ceducational quality.\u201d</p> <p>There\u2019s a great Hugging Face writeup if you want to dive deeper: FineWeb Blogpost</p> <p>(Note: FineWeb token counts are reported using the GPT-2 tokenizer. More about tokens and tokenization in the next section.)</p>"},{"location":"pretraining/dataset/#4-this-project","title":"4. This Project","text":"<p>For this project, I\u2019ll be using FineWebEdu as the main pretraining dataset, focusing on ACSII only text. </p> <p>A general rule of thumb for dataset sizing: you want a parameter-to-token ratio of at least 1:20 (20 tokens for every model parameter). Once you get to very high ratios (like 1:100), you start seeing diminishing returns.</p>"},{"location":"pretraining/dataset/#dataset-gathering-sharding","title":"Dataset Gathering &amp; Sharding","text":"<p>The first step is to collect and prepare the dataset. Since this tokenizer will be used to tokenize the FineWebEdu dataset we gathered earlier, it would also be used to train the tokenizer (generally, it's better to train the tokenizer on the same distribution as the pretraining data for LLM). But even then, the raw dataset is too large to work with as a single file, so we break it into smaller shards.</p>"},{"location":"pretraining/dataset/#why-sharding","title":"Why Sharding?","text":"<ul> <li>Large text corpora can easily exceed hundreds of gigabytes.</li> <li>Training requires fast streaming of tokens \u2014 you don\u2019t want to load the entire dataset into memory.  </li> <li>By splitting into smaller shards (e.g., ~100MB each), we can load them efficiently and resume from checkpoints if needed.  </li> </ul>"},{"location":"pretraining/dataset/#short-medium-long-buckets","title":"Short / Medium / Long Buckets","text":"<p>The script doesn\u2019t just shard randomly \u2014 it separates text into three \u201cbuckets\u201d based on length:  </p> <ul> <li>Short: under ~7,500 characters.  </li> <li>Medium: 7,500\u201312,000 characters.  </li> <li>Long: over 12,000 characters.  </li> </ul> <p>Why? Because training efficiency changes with sequence length. Training on shorter examples first lets the model pick up basic structure faster, while longer sequences come later when it has learned more.  </p>"},{"location":"pretraining/dataset/#intentional-leakage","title":"Intentional \u201cLeakage\u201d","text":"<p>The script also allows some examples to \u201cleak\u201d into bigger buckets. For instance:  </p> <ul> <li>~15% of short samples are redirected into medium.  </li> <li>~10% of short samples are redirected into long.  </li> </ul> <p>This prevents the model from overfitting to only very long text near the end of training. In practice, real-world queries are often much shorter, so keeping a blend of lengths makes the model more robust.  </p>"},{"location":"pretraining/dataset/#example-snippet-simplified","title":"Example Snippet (Simplified)","text":"<p>Here\u2019s a stripped-down version of the gather script:  </p> <pre><code>if ex[\"text\"].isascii() and len(ex[\"text\"]) &lt;= max_length:\n    # pick bucket based on text length\n    if len(ex[\"text\"]) &lt;= 7500:\n        bucket = \"short\"\n    elif len(ex[\"text\"]) &lt;= 12000:\n        bucket = \"medium\"\n    else:\n        bucket = \"long\"\n\n    # allow some short \u2192 medium/long leakage\n    if bucket == \"short\" and random.random() &lt; 0.25:\n        bucket = \"medium\" if random.random() &lt; 0.6 else \"long\"\n\n    shard_text[bucket].append(f\"&lt;SOS&gt;{ex['text']}&lt;EOS&gt;\")\n</code></pre> <p>Notice a couple of important details:  </p> <ul> <li>All text is wrapped with <code>&lt;SOS&gt;</code> (start of sequence) and <code>&lt;EOS&gt;</code> (end of sequence).  </li> <li>This guarantees the tokenizer and model know exactly where an example begins and ends.  </li> <li>Filtering to ASCII-only ensures consistency and avoids tricky edge cases with multilingual characters (important for compute-constrained projects).  </li> </ul> <p>By the end of this step, the dataset is organized into neatly sharded text files, ready to be fed into the tokenizer training process.</p>"},{"location":"pretraining/overview/","title":"Pretraining Overview","text":"<p>In this pretraining section, we\u2019ll walk through the core parts: what pretraining is for, how the dataset is gathered, the model architecture, and the overall training process.</p>"},{"location":"pretraining/overview/#what-is-pretraining","title":"What is Pretraining?","text":"<p>Pretraining is the very first step in building a large language model (LLM). Like all deep neural networks, the model\u2019s weights start out completely random (usually sampled from a normal distribution). At this stage, the model is basically useless \u2014 if you ask it to generate text, it\u2019ll just spit out noise.</p> <p>The goal of pretraining is to give the model a general sense of language. It gets exposed to a massive dataset of text and learns patterns like grammar, sentence structure, vocabulary, and even some factual knowledge just from raw co-occurrence. The training objective is next-token prediction \u2014 given some text, the model tries to predict the next word (or token).</p> <p>After enough training, the model learns how to produce text that flows naturally. At this point, if you give it an input sequence, it\u2019ll continue in the way that\u2019s statistically most likely. It\u2019s not yet an \u201cassistant\u201d that follows instructions \u2014 it\u2019s more like a text autocomplete engine. That\u2019s where later stages like SFT and RLHF come in.</p>"},{"location":"pretraining/overview/#dataset","title":"Dataset","text":"<p>For this project, I\u2019m using FineWebEdu from HuggingFace. It\u2019s a large and diverse internet-based dataset that\u2019s been heavily filtered. Filtering steps include things like deduplication, removing boilerplate (like HTML tags from scraped pages), and content filtering to keep it relatively clean. The idea is to make sure the model sees a broad variety of text without too much junk.</p>"},{"location":"pretraining/overview/#model-architecture","title":"Model Architecture","text":"<p>The base model is a decoder-only transformer, similar to the LLaMA-2 design. It also supports an alternative attention mechanism called MLA (Multi-head Latent Attention), adapted from DeepSeek, though that\u2019s optional in this pipeline.</p>"},{"location":"pretraining/overview/#training-process","title":"Training Process","text":"<p>Training uses the standard cross-entropy loss with the AdamW optimizer. Since training is iterative, the model gradually improves through many passes of gradient descent \u2014 in this case, across tens of billions of tokens.</p> <p>By the end of pretraining, the model develops a kind of \u201clanguage sense.\u201d It can generate coherent text, but it\u2019s still very raw. It doesn\u2019t know how to follow instructions or have a chat \u2014 those abilities come from the later stages: SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning with Human Feedback).</p>"},{"location":"pretraining/overview/#key-takeaway","title":"Key Takeaway","text":"<p>Pretraining lays the foundation: it teaches the model how language works at a broad level, but not how to use it in a helpful way. Think of it as teaching a child to read and write before teaching them how to answer questions or follow directions.</p>"},{"location":"pretraining/model_architecture/attention/","title":"Attention","text":"<p>Attention is the heart of the transformer. It\u2019s the mechanism that lets the model decide: \u201cWhich other tokens in the sequence are relevant for predicting the next one?\u201d</p>"},{"location":"pretraining/model_architecture/attention/#vanilla-self-attention","title":"Vanilla Self-Attention","text":"<p>In the vanilla self-attention, we first start out with a given tensor, <code>x</code>, of shape <code>(batch, seq_len, n_embd)</code> as mentioned in the previous <code>embeddings</code> section.</p> <p>Given that tensor, we compute three tensors, Each of them is a linear projection from the input tensor <code>x</code> - Query (Q): what each token is looking for. - Key (K): what each token offers. - Value (V): the information carried by each token.  </p> <p>This is done by either creating 3 separate linear layers in the constructor for the self-attention class: </p> <pre><code>self.q_proj = nn.Linear(n_embd, n_embd)\nself.k_proj = nn.Linear(n_embd, n_embd)\nself.v_proj = nn.Linear(n_embd, n_embd)\n\nself.o_proj = nn.Linear(n_embd, n_embd)  # This will be used and touched upon later on\n</code></pre> <p>Where it can later be invoked to create the Q, K, and V tensors that will be used in attention computation:</p> <pre><code># Takes an input tensor x of shape (batch, seq_len, n_embd) and linearly project it into another tensor, retaining the shape\nq = self.q_proj(x)  \nk = self.k_proj(x)\nv = self.v_proj(x)\n</code></pre> <p>However a more common method is the merge all of those linear layers into a single one, for more efficient computation: <pre><code>self.qkv_proj = nn.Linear(n_embd, 3 * n_embd)\n\nself.o_proj = nn.Linear(n_embd, n_embd)\n</code></pre></p> <p>Then later on, use that to get: </p> <pre><code># Input shape: (batch, seq_len, n_embd), output shape: (batch, seq_len, 3 * n_embd)\nqkv = self.qkv_proj(x)\n\n# Split the tensor along the last dimension \nq, k, v = qkv.chunk(3, dim=-1)\n</code></pre> <p>Both of those will produce the same result. </p> <p>At this point, given an input tensor <code>x</code>, we now have 3 separate tensors <code>q</code>, <code>k</code>, and <code>v</code>, all of the shape <code>(batch, seq_len, n_embd)</code> </p> <p>Next, we 'expand' the tensor from 3d to 4d, using the <code>n_heads</code> hyperparameter.  <code>n_heads</code> defines how many 'heads' we want in attention. Further explanation as to what it does will be given below. </p> <p>We would use the last dimension, <code>n_embd</code> and divide that into <code>(n_heads, head_dim)</code>, based on the formula <code>head_dim = n_embd // n_heads</code> For example, given <code>n_embd=1024</code>, <code>n_heads=16</code>, then <code>head_dim=1024//16=64</code>, meaning we transform our 1024 embedding dimensions into 16 heads, each head have 64 dimension to work with.  It is crucial to add an assertion that <code>n_embd % n_heads == 0</code> to make sure it evenly divides. </p> <p>Given the hyperparameter <code>n_heads</code>, calculate <code>head_dim</code>, then view/reshape the tensor accordingly as follows:  <pre><code># (batch, seq_len, n_embd) -&gt; (batch, seq_len, n_heads, head_dim)\nq = q.view(batch, seq_len, n_heads, head_dim)\nk = k.view(batch, seq_len, n_heads, head_dim)\nv = v.view(batch, seq_len, n_heads, head_dim)\n</code></pre></p> <p>Finally, we swap the dimension for <code>seq_len</code> and <code>n_heads</code></p> <pre><code># (batch, seq_len, n_heads, head_dim) -&gt; (batch, n_heads, seq_len, head_dim)\nq = q.permute(0, 2, 1, 3)\nk = k.permute(0, 2, 1, 3)\nv = v.permute(0, 2, 1, 3)\n</code></pre> <p>Now that our Q, K, V matrices are all of shape <code>(batch, n_heads, seq_len, head_dim)</code>, we apply the self attention formula: </p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + Mask \\right) V \\] <p>The first step is to compute \\(\\(QK^\\top\\)\\) Looking at the shape of the tensors, we will get an output: <code>(batch, n_heads, seq_len, head_dim) @ (batch, n_heads, head_dim, seq_len) = (batch, n_heads, seq_len, seq_len)</code> which we can denote as <code>attn_scores = QK^T</code> </p> <p>Note the <code>(seq_len, seq_len)</code> ending dimensions. That's the reason why people say that attention computation scales quadratically w.r.t the seq_length of the model</p> <p>Next, we apply element-wise division on the computed matrix, where the divisor is the sqrt of <code>d_k</code>, which from the paper refers to the dimensions of each head (<code>head_dim</code>)</p> <p><code>attn_scores = attn_scores / math.sqrt(head_dim)</code></p> <p>The attention scores is normalized by head-dim primarily because of the softmax operation that will immediately take place next. In short, typically the Q, K, V matrices have unit gaussian distribution, when we apply matrix multiplication, the variance scales by <code>head_dim</code> However with how softmax works, values that are more than, say, 3 units less than the maximum in the dimension of interest will be heavily squashed to approximately 0.  In our example scenario, if <code>head_dim=64</code>, that means the std increased from 1 to 8, which would compress the tensor into something similar to a one-hot vector.  </p> <p>Moving along, after we normalize the attention scores, which doesn't change the shape of the tensor, we would need to apply a triangular mask to <code>attn_scores</code>.  </p> <p>Typically, LLMs would add in these mask to prevent the model from 'cheating' by looking at future tokens. Sort of like: If someone gives you a sentence, 'I love my dog' and asks, 'What is the word after 'my'?' It's trivial. The answer is 'dog'. However masking prevents that by, as the name suggests, masking out the next token.  In that same example, the other person would give 'I love my _' and then ask, 'What is the word after 'my'?'</p> <p>Now in this example, let's use the sentence: \"That is a blue dog\" tokenized into <code>[That, is, a, blue, dog]</code> (Since there are only 5 tokens, that means <code>seq_len=5</code> in this example) After going through the embedding layer and above steps, we will reach a tensor of shape <code>(batch, n_heads, 5, 5)</code> Grabbing one of the (5, 5) matrices arbitrarily might look something like: </p> <pre><code>\"That\"   \"is\"     \"a\"      \"blue\"   \"dog\"\n----------------------------------------------\n[ 1.93    1.49     0.90    -2.11     0.68 ]   \u2190 \"That\"\n[-1.23   -0.04    -1.60    -0.75    -0.69 ]   \u2190 \"is\"\n[-0.49    0.24    -1.11     0.09    -2.32 ]   \u2190 \"a\"\n[-0.22   -1.38    -0.40     0.80    -0.62 ]   \u2190 \"blue\"\n[-0.59   -0.06    -0.83     0.33    -1.56 ]   \u2190 \"dog\"\n</code></pre> <p>That tells us how much attention each token pays to each other.  Now these are un-normalized values, so it would be hard to interpret, at least for now. </p> <p>We then apply the triangular mask that prevents tokens to look ahead. It would be something like: </p> <pre><code>[  0   -\u221e   -\u221e   -\u221e   -\u221e ]\n[  0    0   -\u221e   -\u221e   -\u221e ]\n[  0    0    0   -\u221e   -\u221e ]\n[  0    0    0    0   -\u221e ]\n[  0    0    0    0    0 ]\n</code></pre> <p>Applying via element-wise addition: <code>attn_scores = attn_scores + mask</code> </p> <p>The result would now look like:  <pre><code>\"That\"   \"is\"     \"a\"      \"blue\"   \"dog\"\n----------------------------------------------\n[  1.93    -\u221e      -\u221e      -\u221e      -\u221e  ]   \u2190 \"That\"\n[-1.23   -0.04     -\u221e      -\u221e      -\u221e  ]   \u2190 \"is\"\n[-0.49    0.24   -1.11     -\u221e      -\u221e  ]   \u2190 \"a\"\n[-0.22   -1.38   -0.40     0.80     -\u221e  ]   \u2190 \"blue\"\n[-0.59   -0.06   -0.83     0.33   -1.56 ]   \u2190 \"dog\"\n</code></pre></p> <p>we pass it through the softmax function to transform it into something like a probability distribution. </p> <pre><code>attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n</code></pre> <pre><code>\"That\"   \"is\"     \"a\"      \"blue\"   \"dog\"\n----------------------------------------------\n[1.0000   0.0000   0.0000   0.0000   0.0000 ]   \u2190 \"That\"\n[0.2330   0.7670   0.0000   0.0000   0.0000 ]   \u2190 \"is\"\n[0.2759   0.5753   0.1488   0.0000   0.0000 ]   \u2190 \"a\"\n[0.2032   0.0632   0.1699   0.5637   0.0000 ]   \u2190 \"blue\"\n[0.1566   0.2658   0.1236   0.3942   0.0596 ]   \u2190 \"dog\"\n</code></pre> <p>As you can see, starting out with the token corresponding to 'That', it can only pay attention to itself, since it's the first token in the sequence Next is the word 'is', which splits the attention between 'That' and itself So on and so forth. This tells the model how much each token attends to each other. </p> <p>At this point, the <code>attn_weights</code> tensor is still of shape <code>(batch, n_heads, seq_len, seq_len)</code> since both normalization and softmax doesn't change the tensor shape  </p> <p>Now we process the final steps: </p> <pre><code># (batch, n_heads, seq_len, seq_len) @ (batch, n_heads, seq_len, head_dim) = (batch, n_heads, seq_len, head_dim)\nattn_output = attn_weights @ v\n\n# (batch, n_heads, seq_len, head_dim) -&gt; (batch, seq_len, n_heads, head_dim) -&gt; (batch, seq_len, n_embd)\nattn_output = attn_output.permute(0, 2, 1, 3).view(batch, seq_len, n_embd)\n\nreturn self.o_proj(attn_output)\n</code></pre> <p>The final step here is to first matrix multiply with the <code>v</code> tensor to get the tensor of shape <code>(batch, n_heads, seq_len, head_dim)</code> We revert the permutation and viewing to get back the original input shape <code>(batch, seq_len, n_embd)</code> Finally, apply the output projection matrix to <code>attn_output</code> before returning. </p> <p>So what's the purpose of output matrix? </p> <p>One can think of it as a way to combine the information that each head learned.  Recall that when we apply attention, we use multiple heads. Each head process their own 'chunk' of embedding dimensions compeltely separately from each other.  It's beneficial to allow them to learn their own information, however at the end, we merely concatenate them together. </p> <p>The final output projection matrix allows the information to get 'aggregated' and combined. </p>"},{"location":"pretraining/model_architecture/attention/#in-this-project-implementation","title":"In This Project: Implementation","text":"<p>In <code>MHSelfAttention</code>, queries, keys, and values are created together, where it's first viewed then chunked along the last dimension:</p> <pre><code>qkv = self.qkv_linear(x)  \nq, k, v = qkv.view(batch, seq_len, n_heads, 3 * h_dim).chunk(3, dim=-1)\n</code></pre> <ul> <li>Each input token embedding is linearly projected into Q, K, V vectors.  </li> <li>Shape after splitting: <code>(batch, seq_len, n_heads, h_dim)</code>.  </li> </ul> <p>Then, before computing attention, Rotary Position Embeddings (RoPE) are applied to Q and K:</p> <pre><code>q = apply_rotary_embeddings(q, freqs_complex)\nk = apply_rotary_embeddings(k, freqs_complex)\n</code></pre> <p>Why? Token embeddings alone tell the model what a word is, but not where it appears. Without positional info,  </p> <ul> <li>\u201cThe cat sat on the mat.\u201d  </li> <li>\u201cThe mat sat on the cat.\u201d  </li> </ul> <p>would look identical.</p> <p>Instead of adding positional vectors (like the original Transformer\u2019s sinusoidal method), RoPE rotates Q and K in the complex plane by an amount proportional to their position. This makes attention directly sensitive to relative distances between tokens.</p> <p>For our purposes, you can think of RoPE as: \u201ca lightweight operation on Q and K that encodes order, without changing tensor shapes.\u201d</p> <p>(If you want to dive deeper, check the RoPE paper on arxiv.)</p> <p>Next, permute the tensors then apply scaled dot-product attention:</p> <pre><code>q = q.permute(0, 2, 1, 3)\nk = k.permute(0, 2, 1, 3)\nv = v.permute(0, 2, 1, 3)\n\nscores = q @ k.transpose(-2, -1) / sqrt(h_dim)\nmask = torch.tril(torch.ones((seq_len, seq_len), device=self.device))\nscores = scores.masked_fill(~mask, float('-inf'))\nweights = F.softmax(scores, dim=-1)\nout = weights @ v\n</code></pre> <ul> <li>The causal mask ensures each token can only attend to past tokens (left-to-right).  </li> <li>Softmax converts similarity scores into weights.  </li> <li>Weighted sum with V produces the final attended representation.</li> </ul> <p>Finally, results from all heads are concatenated and passed through a linear projection to mix information.</p>"},{"location":"pretraining/model_architecture/attention/#notes","title":"Notes","text":"<ol> <li> <p>Why not Multi-Query Attention?    The original LLaMA-2 paper uses multi-query attention (MQA), where all heads share the same K and V but have separate Q.    This greatly reduces KV-cache memory usage, which is important for scaling to very large models and efficient inference.    For this project, memory pressure from KV-cache isn\u2019t a bottleneck, so standard multi-head attention is simpler and sufficient.</p> </li> <li> <p>What about DeepSeek\u2019s MLA?    This project includes an optional implementation of Multi-Head Latent Attention (MLA), which is a refinement that reduces KV-cache memory even further while keeping multiple latent spaces.    It\u2019s more efficient than MQA, but again \u2014 KV-cache isn\u2019t the limiting factor here.    Since the focus is educational clarity, SimpleLLaMA sticks with classic multi-head attention.</p> </li> </ol>"},{"location":"pretraining/model_architecture/decoder_block/","title":"Transformer Decoder Block","text":"<p>The decoder block is the fundamental building unit of the transformer. Each block combines attention, feedforward networks, normalization, and residual connections into a repeatable structure.</p>"},{"location":"pretraining/model_architecture/decoder_block/#structure-of-a-decoder-block","title":"Structure of a Decoder Block","text":"<p>A decoder block has two main parts:</p> <ol> <li>Multi-Head Self-Attention (MHA) \u2192 lets tokens exchange information.  </li> <li>Feedforward Network (FFN) \u2192 transforms the attended features into richer representations.  </li> </ol> <p>Surrounding these are: - RMSNorm \u2192 stabilizes training by normalizing activations. - Residual Connections \u2192 ensure information from earlier layers isn\u2019t lost.  </p> <p>The primary block flow is:</p> <pre><code>Input \u2192 Norm \u2192 Attention \u2192 Residual \u2192 Norm \u2192 Feedforward \u2192 Residual \u2192 Output\n</code></pre> <p>This \u201cpre-norm\u201d setup (normalize before each sub-layer) is known to improve stability in deep transformers.</p>"},{"location":"pretraining/model_architecture/decoder_block/#example-walkthrough","title":"Example Walkthrough","text":"<p>Let\u2019s step through what happens inside one decoder block. Suppose we have an input tensor <code>x</code> of shape <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"pretraining/model_architecture/decoder_block/#1-first-normalization","title":"1. First Normalization","text":"<p><pre><code>h = self.norm1(x)\n</code></pre> - RMSNorm is applied to <code>x</code>. - This ensures the activations are scaled to a stable range before entering attention. - Unlike LayerNorm, RMSNorm does not recenter the mean \u2014 it only rescales variance.  </p>"},{"location":"pretraining/model_architecture/decoder_block/#2-multi-head-self-attention","title":"2. Multi-Head Self-Attention","text":"<p><pre><code>attn_out = self.attention(h, freqs_complex)\n</code></pre> - Each token produces query, key, and value vectors. - Rotary Position Embeddings (RoPE) are applied to Q and K to inject positional info. - Attention computes how strongly each token attends to others in the sequence. - The output has the same shape as the input: <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"pretraining/model_architecture/decoder_block/#3-first-residual-connection","title":"3. First Residual Connection","text":"<p><pre><code>h = x + attn_out\n</code></pre> - Here we add the original input <code>x</code> back to the attention output. - This is called a residual connection (or skip connection).  </p> <p>Why is this important? - Imagine stacking dozens of layers. Without skip connections, the network could \"forget\" the original signal after being transformed multiple times. - By adding <code>x</code> back, we preserve the original information while also giving the model access to the new transformed features from attention. - During backpropagation, residuals also help gradients flow more smoothly, preventing vanishing or exploding gradients. - In practice, you can think of it as: the model learns adjustments (deltas) on top of the original input, instead of rewriting it from scratch every time. </p>"},{"location":"pretraining/model_architecture/decoder_block/#4-second-normalization","title":"4. Second Normalization","text":"<p><pre><code>h_norm = self.norm2(h)\n</code></pre> - Again, normalize before the next sublayer. - This keeps the values stable before passing into the FFN.  </p>"},{"location":"pretraining/model_architecture/decoder_block/#5-feedforward-network","title":"5. Feedforward Network","text":"<p><pre><code>ffn_out = self.ffwd(h_norm)\n</code></pre> - Input passes through a SwiGLU feedforward network (as described in detail in the FFN doc). - Adds nonlinearity and transformation capacity. - Output shape: <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"pretraining/model_architecture/decoder_block/#6-second-residual-connection","title":"6. Second Residual Connection","text":"<p><pre><code>out = h + ffn_out\n</code></pre> - Again, the skip connection ensures that the block doesn\u2019t overwrite the information coming from the attention stage. - Instead, it layers on additional transformations from the FFN. - By the time you stack many decoder blocks, each one is contributing refinements while keeping the original context intact. - This makes the network much more robust and trainable.  </p> <p>Final output shape: <code>(batch, seq_len, n_embd)</code>.</p>"},{"location":"pretraining/model_architecture/decoder_block/#in-this-project","title":"In This Project","text":"<ul> <li>Attention type: defaults to standard multi-head self-attention, with optional MLA for efficiency.  </li> <li>Normalization: RMSNorm used everywhere (simpler than LayerNorm, but empirically stable).  </li> <li>Activation: SiLU-based feedforward (SwiGLU).  </li> <li>Dropout: applied after projections, mainly used during fine-tuning (SFT/RLHF).  </li> <li>Residuals: used after both the attention and FFN sublayers.  </li> </ul> <p>Together, these form the repeating backbone of the SimpleLLaMA model. By stacking many of these blocks, the network can build increasingly complex representations of text sequences.</p>"},{"location":"pretraining/model_architecture/embeddings/","title":"Embeddings","text":"<p>Embeddings are the first step in turning discrete tokens (integers from tokenization) into continuous vectors for a neural network to process.</p>"},{"location":"pretraining/model_architecture/embeddings/#token-embeddings","title":"Token Embeddings","text":"<p>After tokenization, each word or subword is represented by an integer ID. But LLMs don\u2019t work directly with these integers. Instead, we map each token ID into a dense vector of fixed size (the embedding dimension).</p> <p>In PyTorch, this is done with an <code>nn.Embedding</code> layer. In the <code>LLaMaTransformer</code> class from this project, you\u2019ll see:</p> <pre><code>self.embeddings = nn.Embedding(tokenizer.get_vocab_size(), n_embd)\n</code></pre> <p>(Often times the embedding dimensionality of the model, <code>n_embd</code> in this case, is referred to under different names. Common ones include <code>embedding_dim</code>, <code>d_model</code>, and <code>hidden_size</code>)</p> <p>Here, an embedding layer is created, which serves as a lookup table for the model. It takes in two primary values, <code>vocab_size</code> and <code>n_embd</code> and create a matrix of shape <code>(vocab_size, n_embd)</code> Each row corresponds to a token ID and is a trainable vector. For example:</p> <ul> <li>Token \"I\" \u2192 73 \u2192 [0.12, -0.44, 1.05, ...]</li> <li>Token \"an\" \u2192 256 \u2192 [1.33, 0.05, -0.72, ...]</li> </ul> <p>Both will be map to unique vectors, all of which will be of length <code>n_embd</code> At initialization, these vectors are random. During training, the model adjusts them so that it learns semantic relationship between tokens.  </p> <p><code>n_embd</code> is a crucial hyperparameter when creating a LLM. It essentially gives the model the flexibility of how much 'semantics' each token can hold.   </p> <p>For example, say the word <code>man</code> and <code>woman</code> can be represented by a single token, <code>1098</code> and <code>1290</code> respectively.  Passing those through the embedding layer, the model will grab the vector at row index of <code>1098</code> to represent that as man, and row index <code>1290</code> for woman</p> <p>Their vectors will differ, but both have shape <code>(n_embd,)</code>. You can think of each dimension in this vector space as encoding some abstract feature the model has learned. For example, one combination of dimensions might capture gender-like differences (man vs. woman), while another might capture whether something is an animate being or an object. However this is just a simplified way of explanation. In reality, these dimension are polysemantic and is much more complex. (Should include explanation that each value is a dimension as well?)</p> <p>Once we convert our list of tokens into a list of vectors, we can proceed with passing that to the Decoder Block.</p>"},{"location":"pretraining/model_architecture/embeddings/#embeddings-in-this-project","title":"Embeddings in This Project","text":"<ul> <li>Embedding dimension (<code>n_embd</code>) is configurable (e.g., 768, 1024, or higher).  </li> <li>RoPE is used for positional encoding by default, following LLaMA.  </li> <li>Initialization: embeddings start random and are updated through backpropagation.  </li> <li>Tied weights: this project experimented with tying embeddings to the final output projection layer (a trick used in some models). But in practice, training became unstable, so it was disabled.</li> </ul>"},{"location":"pretraining/model_architecture/embeddings/#walkthrough-example","title":"Walkthrough Example","text":"<p>Let\u2019s walk through a toy example with the sentence:  </p> <p>\u201cI love dogs\u201d </p> <p>Step 1. Tokenization </p> <p>Using an arbitrary word-level tokenizer, each word is mapped to an integer ID:  </p> <ul> <li><code>\"I\"</code> \u2192 73  </li> <li><code>\"love\"</code> \u2192 786  </li> <li><code>\"dogs\"</code> \u2192 2934  </li> </ul> <p>So the input sequence is:  </p> <pre><code>[73, 786, 2934]\n</code></pre> <p>Step 2. Embedding Matrix </p> <p>When we create an <code>nn.Embedding(vocab_size, embedding_dim)</code> layer, it internally builds a big lookup table (a matrix).  </p> <ul> <li>Shape = <code>(vocab_size, embedding_dim)</code> </li> <li>Each row index corresponds to a token ID.  </li> <li>Each row contains a vector of length <code>embedding_dim</code>.  </li> </ul> <p>For this example, let\u2019s set <code>embedding_dim = 8</code>. That means every token ID will be mapped to an 8-dimensional vector.  </p> <p>A (very small) portion of the embedding matrix might look like this at initialization (values are random floats):  </p> <pre><code>0:     [ 0.11, -0.07,  0.45,  0.02, -0.33,  0.19, -0.48,  0.05]\n1:     [-0.21,  0.34, -0.11, -0.08,  0.27, -0.39,  0.17, -0.43]\n2:     [ 0.09,  0.13,  0.28, -0.47, -0.36,  0.22,  0.41, -0.18]\n3:     [-0.15,  0.54,  0.28,  0.12, -0.41, -0.41, -0.53,  0.44]\n4:     [ 0.12,  0.25, -0.58,  0.56,  0.4,  -0.35, -0.38, -0.38]\n5:     [-0.23,  0.03, -0.08, -0.25,  0.13, -0.43, -0.25, -0.16]\n...\n73:    [ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27]\n...\n786:   [-0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04]\n...\n2934:  [ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33]\n...\n</code></pre> <p>Step 3. Lookup </p> <p>Now, to embed our sentence <code>[73, 786, 2934]</code>, the embedding layer simply selects the rows at those indices:  </p> <ul> <li>Token ID 73 (\u201cI\u201d) \u2192 <code>[ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27 ]</code> </li> <li>Token ID 786 (\u201clove\u201d) \u2192 <code>[ -0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04 ]</code> </li> <li>Token ID 2934 (\u201cdogs\u201d) \u2192 <code>[ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33 ]</code> </li> </ul> <p>Step 4. Output Tensor </p> <p>Stacking them together, the embedding layer outputs a tensor:  </p> <pre><code>[\n  [ 0.22, -0.51,  0.36,  0.08, -0.44,  0.19, -0.09,  0.27 ],   # \"I\"\n  [ -0.13,  0.42,  0.07, -0.36,  0.55, -0.22,  0.18,  0.04 ], # \"love\"\n  [ 0.31, -0.14, -0.25,  0.49, -0.07,  0.61, -0.12, -0.33 ]   # \"dogs\"\n]\n</code></pre> <p>Shape = <code>(3, 8)</code> \u2192 3 tokens, each represented by an 8-dimensional vector.</p> <p>Essentially, given an input 1d tensor of tokens, which the number of tokens is often referred to as <code>(seq_len,)</code>, we transform it into a tensor of shape <code>(seq_len, n_embd)</code> </p> <p>In this example, it is <code>(3, 8)</code></p> <p>This is the format that gets passed on to the Decoder Block. </p> <p>A very important note is that there's almost always a third dimension, called a Batch dimension.  This allows parallel processing, which makes training much faster.  Batch dimension is always the very first dimension, so the shape of output tensor is <code>(batch, seq_len, n_embd)</code> In this case, since we only have a single example sentence, batch dimension value would be 1, which is</p> <p><code>(1, 3, 8)</code></p>"},{"location":"pretraining/model_architecture/end_to_end/","title":"End-to-End Walkthrough","text":"<p>Now at this point, most of the aspects of the architecture and pipeline have been covered in detail. This final page will be used to tie everything together and give a more thorough, step-by-step overview of how all the parts interact to form a working large language model.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#1-from-raw-text-to-tokens","title":"1. From Raw Text to Tokens","text":"<p>We first start out with a massive corpus of text \u2014 think in the scale of hundreds of gigabytes upwards, containing billions or even trillions of tokens. This corpus is gathered from sources like books, Wikipedia, academic papers, code repositories, and curated parts of the internet.  </p> <p>But raw text isn\u2019t useful to the model. The model only works with numbers, so the very first step is to tokenize this text using a pretrained tokenizer.  </p> <p>For example, suppose we have the sentence:  </p> <pre><code>\"The quick brown fox jumps over the lazy dog.\"\n</code></pre> <p>The tokenizer will break this into smaller units (subwords or characters depending on the algorithm) and then convert each into an integer ID.  </p> <p>That means something like:  </p> <pre><code>[\"The\", \" quick\", \" brown\", \" fox\", \" jumps\", \" over\", \" the\", \" lazy\", \" dog\", \".\"]\n\u2192 [1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209]\n</code></pre> <p>Now the sentence is represented as a sequence of integers. This is the form that the neural network can actually process.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#2-batching-and-shaping-the-data","title":"2. Batching and Shaping the Data","text":"<p>Instead of feeding one sentence at a time, training uses mini-batches to process many sequences in parallel. This is crucial for efficiency on GPUs/TPUs.  </p> <p>Suppose we have a long stream of tokens like:  </p> <pre><code>[1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209, 954, 4461, 3546, 206, 4401, ...]\n</code></pre> <p>If we set:</p> <ul> <li><code>batch_size = 2</code> </li> <li><code>seq_len = 6</code> </li> </ul> <p>We would take the first <code>batch_size * seq_len = 12</code> tokens and reshape them into a <code>(batch, seq_len)</code> tensor:  </p> <pre><code>batch_size = 2\nseq_len = 6\ntokens = [1202, 850, 149, 4211, 769, 1839, 3521, 4879, 2035, 1209, 954, 4461, 3546, 206, 4401, ...]\n\ninput_tokens = torch.tensor(tokens[:batch_size * seq_len]).reshape(batch_size, seq_len)\n\n# When printed, input_tokens would look like:\n# tensor([[1202,  850,  149, 4211,  769, 1839],\n#         [3521, 4879, 2035, 1209,  954, 4461]])\n</code></pre> <p>This reshaped tensor is now ready for the model.  </p> <p>In realistic training runs, values are much larger, e.g.: - <code>batch_size = 32</code> (process 32 sequences in parallel) - <code>seq_len = 2048</code> (each sequence is 2048 tokens long)  </p> <p>So the model processes a tensor of shape <code>(32, 2048)</code> in one forward pass.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#3-passing-through-the-transformer-model","title":"3. Passing Through the Transformer Model","text":"<p>Next, this tensor of token IDs is passed into the transformer model. The model is composed of the architecture we previously touched upon in detail: embeddings, attention, feedforward networks, normalization, and residual connections, all stacked together into many decoder blocks.  </p> <p>Here is the structure of the <code>LLaMaTransformer</code> class that uses all the previous building blocks:  </p> <pre><code>class LLaMaTransformer(nn.Module):\n    def __init__(self,\n                 config: any,\n                 tokenizer: tokenizers.Tokenizer,\n                 device: str):\n\n        super().__init__()\n\n        # === Unpack config ===\n        max_seq_len = config.max_seq_len\n        n_embd = config.n_embd\n        n_heads = config.n_heads\n        n_layers = config.n_layers\n        multiple_of = config.multiple_of\n        # And many more, will omit for documentation\n\n        assert n_embd % n_heads == 0, f\"n_embd ({n_embd}) % n_heads ({n_heads}) must equal 0!\"\n        assert (n_embd // n_heads) % 2 == 0 and qk_rope_head_dim % 2 == 0, \"head_dim must be even for RoPE!\"\n\n\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        self.device = device\n        self.embeddings = nn.Embedding(tokenizer.get_vocab_size(), n_embd)\n        self.decoder_blocks = nn.ModuleList(\n            [DecoderBlock(n_embd=n_embd, n_heads=n_heads, multiple_of=multiple_of, use_mla=use_mla, ...)\n             for _ in range(n_layers)])\n        self.norm = RMSNorm(n_embd, eps)\n        self.final_linear = nn.Linear(n_embd, tokenizer.get_vocab_size(), bias=False)\n\n        h_dim = qk_rope_head_dim if use_mla else n_embd // n_heads\n        self.freq_complex = precompute_theta_pos_frequencies(max_seq_len, h_dim, theta, device)\n\n    def forward(self, x):\n        batch, seq_len = x.shape\n\n        # (batch, seq_len) -&gt; (batch, seq_len, n_embd)\n        h = self.embeddings(x)  # Each token ID mapped to vector\n        freqs_complex = self.freq_complex[:seq_len]\n\n        # Pass through all Decoder Blocks\n        for dec_block in self.decoder_blocks:\n            h = dec_block(h, freqs_complex)\n\n        h = self.norm(h)\n        return self.final_linear(h)  # (batch, seq_len, n_embd) -&gt; (batch, seq_len, vocab_size)\n</code></pre>"},{"location":"pretraining/model_architecture/end_to_end/#4-step-by-step-inside-the-model","title":"4. Step-by-Step Inside the Model","text":""},{"location":"pretraining/model_architecture/end_to_end/#41-embedding-layer","title":"4.1 Embedding Layer","text":"<ul> <li>Input: <code>(batch, seq_len)</code> of integers.  </li> <li>Each integer token is mapped into a dense vector of length <code>n_embd</code>.  </li> <li>Output: <code>(batch, seq_len, n_embd)</code> </li> </ul> <p>This is the semantic representation of tokens: instead of just IDs, we now have vectors that carry meaning and relationships.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#42-positional-information","title":"4.2 Positional Information","text":"<p>We then fetch <code>freqs_complex</code>, which stores precomputed values for Rotary Position Embeddings (RoPE). RoPE encodes the positions of tokens into the attention mechanism, so the model knows order (e.g., difference between \u201cdog bites man\u201d and \u201cman bites dog\u201d).  </p>"},{"location":"pretraining/model_architecture/end_to_end/#43-decoder-blocks","title":"4.3 Decoder Blocks","text":"<p>The embedded tensor is then passed into the first Decoder Block. Each block applies: - RMSNorm \u2192 normalizes values for stability. - Multi-Head Attention \u2192 lets each token attend to others in the sequence. - Feedforward Network (SwiGLU) \u2192 adds nonlinear transformation capacity. - Residual Connections \u2192 add the original input back to preserve information.  </p> <p>The internal computations change the contents of the tensor but not its shape: it remains <code>(batch, seq_len, n_embd)</code>.  </p> <p>The output of block 1 is passed into block 2, then block 3, and so on, until it passes through all <code>n_layers</code>.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#44-final-normalization","title":"4.4 Final Normalization","text":"<p>After the last decoder block, the tensor goes through one final RMSNorm layer. This ensures the distribution of activations is stable before the final projection.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#45-final-linear-layer","title":"4.5 Final Linear Layer","text":"<p>Finally, we project each hidden vector of size <code>n_embd</code> into a vector the size of the vocabulary. - Shape: <code>(batch, seq_len, n_embd) \u2192 (batch, seq_len, vocab_size)</code> </p> <p>This gives logits \u2014 raw scores for each token in the vocabulary.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#5-from-logits-to-predictions","title":"5. From Logits to Predictions","text":"<p>The logits are not yet probabilities. To turn them into probabilities, we apply softmax across the vocabulary dimension:  </p> <pre><code>probabilities = softmax(logits, dim=-1)\n</code></pre> <p>Now, for each position in the sequence, we get a probability distribution over the vocabulary. For example:  </p> <pre><code>At position 6, \"fox\" might have 0.72 probability, \"dog\" 0.05, \"cat\" 0.02, ...\n</code></pre> <p>During training, we compare these probabilities against the actual next token using cross-entropy loss. This loss guides backpropagation, which updates the model weights via gradient descent.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#6-training-loop-connection","title":"6. Training Loop Connection","text":"<p>To summarize the training loop connection:  </p> <ol> <li>Start with batch of token sequences.  </li> <li>Forward pass through embeddings \u2192 decoder blocks \u2192 normalization \u2192 linear layer.  </li> <li>Produce logits <code>(batch, seq_len, vocab_size)</code>.  </li> <li>Apply softmax to get probabilities.  </li> <li>Compute loss vs. ground truth next tokens.  </li> <li>Backpropagate gradients.  </li> <li>Update weights with optimizer (AdamW, etc.).  </li> <li>Repeat across billions of tokens until the model converges.  </li> </ol> <p>Over time, the model gradually learns grammar, facts, and semantics purely by predicting the next token.  </p>"},{"location":"pretraining/model_architecture/end_to_end/#key-takeaway","title":"Key Takeaway","text":"<p>This end-to-end flow shows how everything connects: - Tokenization converts raw text into IDs. - Embeddings + RoPE give meaning and order. - Decoder Blocks repeatedly transform and refine the representations. - Final Linear Layer + Softmax produce predictions over the vocabulary. - Loss and Optimization allow the model to learn from its mistakes.  </p> <p>By stacking these stages together, and scaling up with billions of tokens and parameters, we arrive at a large language model capable of generating coherent and context-aware text.  </p>"},{"location":"pretraining/model_architecture/feedforward/","title":"Feedforward Networks (FFN) in Transformers","text":"<p>When people first learn about Transformers, the attention mechanism usually takes the spotlight. But the feedforward network (FFN) is equally important \u2014 in fact, it often accounts for the majority of parameters in the model.  </p>"},{"location":"pretraining/model_architecture/feedforward/#why-do-we-need-feedforward-layers","title":"Why Do We Need Feedforward Layers?","text":"<p>Attention layers are powerful, but they are still fundamentally linear operations (matrix multiplications, weighted sums). A stack of only linear layers would remain a linear model, which cannot approximate complex, nonlinear functions.  </p> <p>The feedforward network adds nonlinearity and capacity to transform information. It allows the model to map representations into a higher-dimensional space, apply nonlinear activation, and then project back down.  </p> <p>In practice, every Transformer block has the structure:  </p> <pre><code>Input \u2192 [Attention] \u2192 [Feedforward] \u2192 Output\n</code></pre> <p>Both attention and FFN are wrapped with normalization and residual connections.</p>"},{"location":"pretraining/model_architecture/feedforward/#vanilla-transformer-ffn","title":"Vanilla Transformer FFN","text":"<p>In the original Transformer paper (Vaswani et al., 2017), the FFN was defined as:  </p> <pre><code>FFN(x) = max(0, xW1 + b1)W2 + b2\n</code></pre> <ul> <li>Two linear layers with a ReLU in between.  </li> <li>The hidden dimension is usually set to 4\u00d7 the embedding dimension, then projected back down.  </li> </ul> <p>For example, if embedding size = 1024, the FFN hidden size = 4096.  </p> <p>This \u201cexpand and contract\u201d pattern gives the model strong nonlinear mixing power, because the network has a wide layer to mix features, then projects it back to the model\u2019s working dimension.  </p>"},{"location":"pretraining/model_architecture/feedforward/#implementation-example","title":"Implementation Example","text":"<pre><code>class FeedForward(nn.Module):\n    def __init__(self, n_embd: int):\n        super().__init__()\n        self.layer1 = nn.Linear(n_embd, 4 * n_embd)\n        self.layer2 = nn.Linear(4 * n_embd, n_embd)\n\n    def forward(self, x: torch.Tensor):\n        return self.layer2(torch.nn.functional.relu(self.layer1(x)))\n</code></pre> <p>Let\u2019s walk through this carefully: 1. Input tensor <code>x</code> has shape <code>(batch, seq_len, n_embd)</code>. 2. First layer projects it into <code>(batch, seq_len, 4 * n_embd)</code>. 3. Apply ReLU to introduce non-linearity (important because without it, stacking linear layers would still just be linear). 4. Second layer projects it back down to <code>(batch, seq_len, n_embd)</code>.  </p> <p>So while the shape going into and out of the FFN is the same, the hidden computation in between allows the network to express far richer functions.</p>"},{"location":"pretraining/model_architecture/feedforward/#llama-style-ffn-swiglu","title":"LLaMA-Style FFN (SwiGLU)","text":"<p>The LLaMA architecture introduced a key modification: instead of the plain ReLU-based FFN, it uses a SwiGLU activation (SiLU-Gated Linear Unit).  </p> <p>Here\u2019s how it looks in code:  </p> <pre><code>class FeedForward(nn.Module):\n    def __init__(self, n_embd: int, multiple_of: int, dropout: float):\n        super().__init__()\n\n        hidden_dim = int(4 * n_embd * (2 / 3))  # Authors of LLaMa used 2/3 of 4*n_embd\n        # Round hidden_dim up to a nicer multiple for efficient GPU utilization\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = nn.Linear(n_embd, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, n_embd, bias=False)\n        self.w3 = nn.Linear(n_embd, hidden_dim, bias=False)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x: torch.Tensor):\n        h = F.silu(self.w1(x)) * self.w3(x)  # SwiGLU block\n        return self.dropout(self.w2(h))\n</code></pre>"},{"location":"pretraining/model_architecture/feedforward/#breaking-it-down-step-by-step","title":"Breaking It Down Step by Step","text":"<ul> <li>In the vanilla FFN we had two linear layers.  </li> <li>In LLaMA\u2019s FFN we now have three linear layers. Why? To introduce a gating mechanism.  </li> </ul> <p>The formula looks like this:  </p> \\[ h = \\text{SiLU}(W_1x) \\odot (W_3x) \\] <p>where <code>\u2299</code> is elementwise (Hadamard) multiplication.  </p> <ul> <li><code>W1x</code>: main transformation path.  </li> <li>Apply SiLU activation (smooth, like ReLU but differentiable everywhere).  </li> <li><code>W3x</code>: produces a second transformed version of <code>x</code>, used as a gate.  </li> <li>Multiply them elementwise: the gate decides how much of each hidden unit passes through.  </li> </ul> <p>This means: - If a value in <code>W3x</code> is near 0 \u2192 the signal from <code>W1x</code> gets suppressed. - If large \u2192 it amplifies the signal. - If negative \u2192 it can flip the sign of the signal.  </p> <p>So the gating function lets the model regulate the flow of information more flexibly than a simple ReLU cutoff.</p>"},{"location":"pretraining/model_architecture/feedforward/#parameter-balancing","title":"Parameter Balancing","text":"<p>But wait \u2014 adding a third projection layer means more parameters, right? Yes, but the authors balanced this by shrinking the hidden dimension.  </p> <ul> <li>Vanilla FFN parameters: about <code>8 * n_embd\u00b2</code>.   (from <code>(n_embd \u00d7 4n_embd) + (4n_embd \u00d7 n_embd)</code>).  </li> <li>LLaMA FFN: uses hidden_dim = <code>int(4 * n_embd * 2/3)</code>.   With three projections (<code>w1</code>, <code>w2</code>, <code>w3</code>), total \u2248 <code>8 * n_embd\u00b2</code>.  </li> </ul> <p>So both end up with roughly the same parameter budget, but LLaMA gets more expressive power via gating.  </p>"},{"location":"pretraining/model_architecture/feedforward/#shapes-in-action","title":"Shapes in Action","text":"<ul> <li>Input: <code>(batch, seq_len, n_embd)</code> </li> <li>After <code>w1</code> and <code>w3</code>: <code>(batch, seq_len, hidden_dim)</code> </li> <li>After SiLU + elementwise product: <code>(batch, seq_len, hidden_dim)</code> </li> <li>After <code>w2</code>: <code>(batch, seq_len, n_embd)</code> </li> </ul> <p>Input and output shapes match the original \u2014 only the internal transformation is richer.  </p>"},{"location":"pretraining/model_architecture/feedforward/#multiple-of-trick","title":"Multiple-of Trick","text":"<p>The <code>multiple_of</code> hyperparameter ensures <code>hidden_dim</code> is divisible by a convenient number (like 64, 128, 256). This is purely for GPU efficiency: matrix multiplications run faster on dimensions aligned to powers of two.  </p> <p>For example: - Without adjustment: <code>n_embd=1024</code> \u2192 hidden_dim = 2730. - With <code>multiple_of=128</code>: hidden_dim becomes 2816, which is far more efficient for hardware.  </p>"},{"location":"pretraining/model_architecture/feedforward/#dropout","title":"Dropout","text":"<p>At the end, dropout is applied. This isn\u2019t heavily used during large-scale pretraining, but it becomes important in fine-tuning (SFT, RLHF) to regularize and prevent overfitting when datasets are smaller.  </p>"},{"location":"pretraining/model_architecture/feedforward/#summary","title":"Summary","text":"<ul> <li>Feedforward layers provide the nonlinear expressiveness that attention alone cannot.  </li> <li>Vanilla Transformer FFN: Linear \u2192 ReLU \u2192 Linear, with a 4\u00d7 expansion.  </li> <li>LLaMA FFN: SwiGLU-style (SiLU + gating with a third linear layer).  </li> <li>Gating allows richer feature modulation (suppress, amplify, flip).  </li> <li>Parameter count is balanced to stay ~<code>8 * n_embd\u00b2</code>.  </li> <li>Input/output shapes stay the same, but the internal computation is more expressive.  </li> <li>Most of a Transformer\u2019s parameters live in these FFNs \u2014 they are just as crucial as attention.  </li> </ul>"},{"location":"pretraining/model_architecture/normalization/","title":"Normalization","text":"<p>When training deep neural networks, one of the recurring problems is that activations can either explode (grow without bound) or vanish (shrink toward zero) as they pass through many layers. This makes optimization unstable and slows down learning.  </p> <p>Another issue is internal covariate shift \u2014 the idea that the distribution of activations keeps changing as each layer is updated during training, which makes it harder for later layers to adapt.  </p> <p>Internal Covariate Shift can be thought of as follows: </p> <p>Imagine you\u2019re a student preparing for math exams.  </p> <ul> <li>On the first day, you get an Algebra I exam. You do your best, turn it in, get feedback from the teacher, and feel ready to improve for the next one.  </li> <li>Based on the feedback, you adjust your study strategy and expect another Algebra test.  </li> <li>But the next exam you receive is suddenly Calculus \u2014 a totally different level of difficulty. All the preparation you just did no longer matches what you\u2019re being tested on.  </li> </ul> <p>This is what happens in deep neural networks without normalization. Each hidden layer is like a student preparing for its next \u201cexam\u201d (the next round of inputs). After every weight update, the distribution of outputs from the previous layer shifts. That means the \u201cexam\u201d the next layer sees can suddenly look very different than what it was trained on.  </p> <p>As the network gets deeper, these unexpected shifts compound, making training unstable. Layers spend more time re-adapting to constantly changing input distributions rather than actually learning useful features.</p> <p>Normalization layers (like BatchNorm, LayerNorm, etc.) fix this problem. They act like a teacher who ensures that every new exam stays at the same Algebra level \u2014 same general difficulty, same type of questions \u2014 just slightly adjusted each time. This consistency allows each layer to steadily improve rather than getting thrown off by wild distribution shifts.</p> <p>In short: - Without normalization: \u201cI prepared for Algebra, but got Calculus.\u201d - With normalization: \u201cI keep getting Algebra, just with different numbers.\u201d  </p> <p>Normalization layers stabilize the distribution of activations, keep gradients more predictable, and generally allow networks to train deeper and faster.</p>"},{"location":"pretraining/model_architecture/normalization/#layer-normalization-layernorm","title":"Layer Normalization (LayerNorm)","text":"<p>In Transformers (like the original paper \u201cAttention Is All You Need\u201d), the normalization method of choice was Layer Normalization (LayerNorm).</p> <p>How it works: - Given a tensor <code>x</code> of shape <code>(batch, seq_len, n_embd)</code>, LayerNorm normalizes across the embedding dimension for each token. - For each token vector, it computes the mean and variance across its <code>n_embd</code> values. - The normalized vector is then scaled and shifted by learnable parameters (<code>gamma</code> and <code>beta</code>).  </p> <p>Mathematically:</p> <pre><code>LayerNorm(x) = gamma * (x - mean(x)) / sqrt(var(x) + eps) + beta\n</code></pre> <p>Where: - <code>mean(x)</code> and <code>var(x)</code> are computed over the last dimension (<code>hidden_dim</code>). - <code>gamma</code> and <code>beta</code> are learnable parameters that allow the model to \"undo\" normalization if needed. - <code>eps</code> is a small constant to prevent division by zero.  </p> <p>Effectively, LayerNorm re-centers (subtracts the mean) and re-scales (divides by standard deviation). This ensures each token\u2019s hidden vector has roughly zero mean and unit variance before being rescaled.</p> <p>LayerNorm is still widely used in most Transformer implementations, but it comes with a computational cost: subtracting means, computing variances, and performing square roots for every vector.</p>"},{"location":"pretraining/model_architecture/normalization/#root-mean-square-normalization-rmsnorm","title":"Root Mean Square Normalization (RMSNorm)","text":"<p>LLaMA and some later models (including this implementation) instead use RMSNorm, a simpler but surprisingly effective alternative.</p> <p>The key idea: Research showed that re-centering (subtracting the mean) is less important than re-scaling (fixing the variance). In other words, what really stabilizes activations is making sure their magnitude (energy) doesn\u2019t blow up, not whether they\u2019re mean-centered.</p> <p>So RMSNorm skips the mean subtraction entirely.</p> <p>Mathematically:</p> <pre><code>RMSNorm(x) = (x / RMS(x)) * weight\n</code></pre> <p>Where: - <code>RMS(x) = sqrt(mean(x^2))</code> - <code>weight</code> is a learnable scaling vector (similar to <code>gamma</code> in LayerNorm). - No <code>beta</code>, since there\u2019s no re-centering.  </p> <p>This implementation of <code>RMSNorm</code> shows this clearly:</p> <pre><code>class RMSNorm(nn.Module):\n    def __init__(self, n_embd: int, eps: float):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(n_embd))\n\n    def _norm(self, x: torch.Tensor):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x: torch.Tensor):\n        return self.weight * self._norm(x.float()).type_as(x)\n</code></pre> <p>Step by step: 1. Square all elements (<code>x.pow(2)</code>). 2. Take the mean across the last dimension (<code>mean(-1, keepdim=True)</code>). 3. Add a tiny epsilon for numerical stability. 4. Take the reciprocal square root (<code>rsqrt</code>). 5. Multiply back with the original <code>x</code> \u2192 now the activations are normalized to unit RMS. 6. Finally, scale by a learnable weight vector (one parameter per hidden dimension).  </p> <p>This achieves the stabilization effect of LayerNorm but with slightly fewer operations.</p>"},{"location":"pretraining/model_architecture/normalization/#why-rmsnorm","title":"Why RMSNorm?","text":"<p>So why is RMSNorm used in models like LLaMA (and this project)?</p> <ol> <li> <p>Efficiency    RMSNorm is simpler than LayerNorm. It removes the mean subtraction, which slightly reduces compute and memory usage \u2014 especially important when running at very large scales.</p> </li> <li> <p>Empirical stability    Experiments (see the RMSNorm paper) showed that mean-centering didn\u2019t improve stability much. The key factor was scaling by the variance (or root mean square).</p> </li> <li> <p>Better fit for Transformers    Since Transformers already have residual connections and other stabilizing tricks, skipping the mean step doesn\u2019t hurt \u2014 and in fact, models trained with RMSNorm often match or exceed performance of LayerNorm.</p> </li> </ol>"},{"location":"pretraining/model_architecture/normalization/#putting-it-together","title":"Putting It Together","text":"<p>In this project, RMSNorm appears inside the Decoder Block in two places:</p> <pre><code>h = x + self.attention(self.norm1(x), freqs_complex)\nout = h + self.ffwd(self.norm2(h))\n</code></pre> <p>Here, <code>norm1</code> and <code>norm2</code> are both <code>RMSNorm</code> layers. They normalize activations before passing them into attention and feedforward sublayers. This is known as a Pre-Norm Transformer design (normalize before the sublayer, then add the residual). Pre-Norm improves gradient flow compared to the original Post-Norm design.</p>"},{"location":"pretraining/model_architecture/normalization/#summary","title":"Summary","text":"<ul> <li>Normalization stabilizes deep networks, preventing exploding/vanishing activations.  </li> <li>Transformers originally used LayerNorm, which re-centers and re-scales each hidden vector.  </li> <li>RMSNorm drops the mean subtraction, keeping only the re-scaling step.  </li> <li>Despite being simpler, RMSNorm works just as well (sometimes better) and is used in LLaMA and your implementation.  </li> <li>In SimpleLLaMA, RMSNorm ensures stable training across all decoder blocks while keeping the implementation lightweight.  </li> </ul>"},{"location":"pretraining/model_architecture/overview/","title":"Model Architecture Overview","text":"<p>At the heart of SimpleLLaMA is the transformer architecture \u2014 the same family of models that power GPT, LLaMA, and DeepSeek. Transformers are flexible neural networks designed to process sequences of data (like text), and they\u2019ve become the standard for large language models.</p>"},{"location":"pretraining/model_architecture/overview/#big-picture","title":"Big Picture","text":"<p>When you give the model a prompt, like:</p> <p><code>\"The cat sat on the mat\"</code></p> <p>the pipeline looks like this: ~ 1. Tokenization \u2013 Text is broken into tokens (e.g., \"The\", \" cat\", \" sat\", \" on\", \" the\", \" mat\"). 2. Embeddings \u2013 Each token is turned into a dense vector representation. 3. Transformer Blocks \u2013 A stack of repeated layers where the real learning happens:    - Attention \u2192 figures out relationships between tokens.    - Feedforward \u2192 transforms and mixes the information.    - Residuals + Normalization \u2192 stabilize and speed up training. 4. Output Projection \u2013 Final layer maps hidden states back to the vocabulary. 5. Softmax \u2013 Converts raw scores into probabilities for the next token.  </p> <p></p> <p>Should add a reference to Umar! </p>"},{"location":"pretraining/model_architecture/overview/#decoder-only-design","title":"Decoder-Only Design","text":"<p>This project uses a decoder-only transformer, which means: - The model only predicts the next token given all tokens before it. - It\u2019s autoregressive: it generates text left to right, one token at a time. - This design is perfect for language modeling, where the task is \u201cpredict what comes next.\u201d</p>"},{"location":"pretraining/model_architecture/overview/#why-transformers","title":"Why Transformers?","text":"<p>Transformers replaced older sequence models (RNNs, LSTMs) because: - They scale much better with data and compute. - Attention allows the model to directly connect distant tokens (e.g., the start and end of a paragraph). - Parallelization makes them efficient to train on GPUs.</p> <p>In the following sections, we\u2019ll break the model down into its core components: - Embeddings \u2013 how tokens are represented as vectors. - Attention \u2013 how the model connects words together. - Layer Block \u2013 the repeating unit of the transformer. - Output \u2013 how predictions are made.  </p>"},{"location":"pretraining/tokenization/algorithms/","title":"Algorithms","text":""},{"location":"pretraining/tokenization/algorithms/#training-a-tokenizer","title":"Training a Tokenizer","text":"<p>A tokenizer isn\u2019t just a simple rule that splits text on spaces or punctuation \u2014 it\u2019s actually trained on a large text corpus, very much like the LLM itself is trained on text. The idea is to learn how to break down words into pieces in a way that balances efficiency and flexibility.</p> <p>There are different algorithms to do this, but the two most common are:</p> <ul> <li>BPE (Byte Pair Encoding)</li> <li>SentencePiece (often using Unigram LM)</li> </ul> <p>In this section, we\u2019ll focus on BPE because it\u2019s easier to walk through step by step and is widely used in models like GPT-2.</p>"},{"location":"pretraining/tokenization/algorithms/#example-bpe-in-action","title":"Example: BPE in Action","text":"<p>Let\u2019s say our toy text corpus is:</p> <p>\"I love my banana bandana\"</p> <p>We\u2019ll show how BPE gradually learns to compress repeated character pairs into new tokens.</p>"},{"location":"pretraining/tokenization/algorithms/#step-1-characters-bytes","title":"Step 1. Characters \u2192 Bytes","text":"<p>Everything starts with raw characters. BPE first converts each character into its ASCII/UTF-8 byte value.</p> <pre><code>text = \"I love my banana bandana\"\nfor character in text:\n    print(ord(character))\n</code></pre> <p>Resulting sequence of numbers:</p> <p><code>[73, 32, 108, 111, 118, 101, 32, 109, 121, 32, 98, 97, 110, 97, 110, 97, 32, 98, 97, 110, 100, 97, 110, 97]</code></p> <p>Here: \"I\" -&gt; 73 \" \" -&gt; 32 \"l\" -&gt; 108 \"o\" -&gt; 111 \"v\" -&gt; 118 \"e\" -&gt; 101 ...  </p> <p>So now the sentence is just a list of numbers.</p>"},{"location":"pretraining/tokenization/algorithms/#step-2-count-frequent-pairs","title":"Step 2. Count frequent pairs","text":"<p>BPE works by repeatedly finding the most common adjacent pair of symbols and merging it into a new symbol. We start with a sliding window of size 2 and count all pairs in the byte list.</p> <pre><code>count = {}\nbyte_repr = [73, 32, 108, 111, 118, 101, 32, 109, 121, 32, \n             98, 97, 110, 97, 110, 97, 32, 98, 97, 110, 100, 97, 110, 97]\n\nfor i, j in zip(byte_repr[:-1], byte_repr[1:]):\n    pair = (i, j)\n    count[pair] = count.get(pair, 0) + 1\n\nsorted_keys = sorted(count, key=lambda x: count[x], reverse=True)\nfor key in sorted_keys:\n    print(f\"{key} : {count[key]}\")\n</code></pre> <p>The most frequent pair is:</p> <p>(97, 110) : 4</p> <p>This corresponds to the two characters \"a\" (97) and \"n\" (110), which together form \"an\". Looking at our sentence, \u201cbanana bandana,\u201d it makes sense: \"an\" repeats a lot.</p>"},{"location":"pretraining/tokenization/algorithms/#step-3-merge-and-replace","title":"Step 3. Merge and replace","text":"<p>We now assign a new token ID for \"an\", say 256. Then we replace all occurrences of the pair (97, 110) with this new symbol:</p> <p><code>[73, 32, 108, 111, 118, 101, 32, 109, 121, 32, 98, 256, 256, 97, 32, 98, 256, 100, 256, 97]</code></p> <p>The list is shorter \u2014 we compressed the repeated \"an\" pairs.</p>"},{"location":"pretraining/tokenization/algorithms/#step-4-repeat-the-process","title":"Step 4. Repeat the process","text":"<p>We do the same thing again: scan for the most frequent pair, merge, and replace.  </p> <p>Now the top pairs would be: - (32, 98) : 2 - (98, 256) : 2 - (256, 97) : 2  </p> <p>Let\u2019s pick <code>(32, 98)</code> first. Remember, 32 is the ASCII code for a space <code>\" \"</code>, and 98 is <code>\"b\"</code>. Together they represent <code>\" b\"</code>. We can merge them into a new token ID, say <code>257</code>, so now the tokenizer knows <code>\" b\"</code> is a single symbol.  </p> <p>If we run the process again, the frequent pairs update. Now we see pairs like: - (257, 256) : 2 - (256, 97) : 2  </p> <p>This means <code>\" b\"</code> (257) is often followed by <code>\"an\"</code> (256), which together form <code>\" ban\"</code>. We can merge <code>(257, 256)</code> into <code>258</code>, representing <code>\" ban\"</code>.  </p> <p>Each time we merge, the sequence gets shorter and starts capturing bigger chunks of meaning. By repeating this again and again, we eventually build up tokens for frequent pieces of words, until we reach the target vocabulary size.</p>"},{"location":"pretraining/tokenization/algorithms/#step-5-maintain-reverse-mapping","title":"Step 5. Maintain reverse mapping","text":"<p>For decoding back into text, the tokenizer keeps track of a reverse dictionary that remembers what each new token stands for.</p> <p>For example:</p> <p>73  -&gt; \"I\" 32  -&gt; \" \" 108 -&gt; \"l\" 111 -&gt; \"o\" 118 -&gt; \"v\" 101 -&gt; \"e\" ... 256 -&gt; \"an\" 257 -&gt; \" b\" 258 -&gt; \" ban\"  </p> <p>When we want to reconstruct text from tokens, the tokenizer looks up these mappings.</p>"},{"location":"pretraining/tokenization/algorithms/#final-tokenization","title":"Final Tokenization","text":"<p>With this toy tokenizer, the original sentence:</p> <p>\"I love my banana bandana\"</p> <p>might end up tokenized as:</p> <p>[\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"m\", \"y\", \" ban\", \"an\", \"a\", \" ban\", \"d\", \"an\", \"a\"]</p> <p>It\u2019s not very compressed at this tiny scale, but notice how frequent chunks like \"an\" and \" ban\" became their own tokens.  </p>"},{"location":"pretraining/tokenization/algorithms/#recap","title":"Recap","text":"<p>BPE builds a tokenizer by: 1. Starting with characters as symbols. 2. Repeatedly finding the most frequent adjacent pair. 3. Merging that pair into a new token. 4. Updating the text and repeating until the vocabulary reaches a set size.  </p> <p>The result is a subword tokenizer that\u2019s much more efficient than character-level and word-level tokenization, which will be discussed in the next section. This balance is why BPE (or related methods like Unigram LM) became the standard in modern LLMs.</p>"},{"location":"pretraining/tokenization/examples/","title":"Examples","text":""},{"location":"pretraining/tokenization/examples/#tokenization-examples","title":"Tokenization Examples","text":"<p>In this section, we\u2019ll look at some more concrete examples of how tokenization works in practice, and why subword tokenization ends up being the most effective choice for LLMs.</p>"},{"location":"pretraining/tokenization/examples/#character-level-tokenization","title":"Character-Level Tokenization","text":"<p>Example sentence: \"Hello, how are you doing today?\"</p> <p>At the character level, this becomes: [\"H\",\"e\",\"l\",\"l\",\"o\",\",\",\" \",\"h\",\"o\",\"w\",\" \",\"a\",\"r\",\"e\",\" \",\"y\",\"o\",\"u\",\" \",\"d\",\"o\",\"i\",\"n\",\"g\",\" \",\"t\",\"o\",\"d\",\"a\",\"y\",\"?\"]</p> <p>That\u2019s 31 tokens for a short sentence!  </p> <p>Pros: - Very simple, no training required. - Small vocabulary (if limited to ASCII, just 128 possible tokens).  </p> <p>Cons: - Extremely inefficient: every character, space, and punctuation becomes a token. - Long sequences mean the model uses much more compute to learn the given text. - Context windows get consumed very quickly. Imagine trying to process an entire book one character at a time \u2014 it would explode into millions of tokens. - Doesn\u2019t scale well to languages with large character sets (Chinese, Korean) where the vocabulary would still be huge and sequences still long.</p>"},{"location":"pretraining/tokenization/examples/#word-level-tokenization","title":"Word-Level Tokenization","text":"<p>If we instead split on words, we get: [\"Hello\", \"how\", \"are\", \"you\", \"doing\", \"today\", \"?\"]</p> <p>Now it\u2019s only 7 tokens \u2014 much shorter than the character-level case.  </p> <p>Pros: - Far fewer tokens per sentence. - Faster training since each token carries more semantic meaning. - Intuitively matches how humans think about words.  </p> <p>Cons: - Vocabulary explosion: English alone has 100,000+ words, and that number grows quickly once you consider inflections, misspellings, slang, and technical jargon. - Out-of-Vocabulary (OOV) errors: any word not seen during training cannot be encoded at inference. For example, if the word \u201cquantumbotics\u201d appears but wasn\u2019t in the training data, the tokenizer simply has no way to represent it. - Poor handling of morphologically rich languages. For example, Turkish or Finnish can produce many forms of a single root word, each of which would need its own token. - Non-space-delimited languages (like Chinese or Japanese) are a nightmare for word-level tokenization since there\u2019s no clear way to split words. Tokenizing character-by-character just reverts back to the inefficiency of character-level methods.  </p>"},{"location":"pretraining/tokenization/examples/#subword-level-tokenization","title":"Subword-Level Tokenization","text":"<p>Subword tokenization finds a balance between the two. The same sentence could become something like: [\"Hello\", \",\", \" how\", \" are\", \" you\", \" do\", \"ing\", \" today\", \"?\"]</p> <p>Now the length is around 9 tokens \u2014 not too long, not too short.  </p> <p>Pros: - Vocabulary size is controllable (usually 30k\u2013100k tokens). - Can represent any word, even unseen ones, by splitting into smaller known pieces. - Captures common roots, prefixes, and suffixes (\u201cwalk\u201d, \u201cwalking\u201d, \u201cwalked\u201d share \u201cwalk\u201d). - Much more efficient than character-level while avoiding the fragility of word-level. - Works across languages, including ones without spaces.  </p> <p>Cons: - More complex to train than character- or word-level. - Some awkward splits still happen (rare words may be broken down oddly). - Token boundaries aren\u2019t always human-intuitive.  </p>"},{"location":"pretraining/tokenization/examples/#why-subword-wins","title":"Why Subword Wins","text":"<p>Character-level forces the model to learn spelling and structure from scratch, while word-level makes the vocabulary too large and brittle. Subword-level combines the strengths of both: - Compression is strong enough for efficient training. - Vocabulary is manageable. - Handles unseen or rare words gracefully.  </p> <p>This is why nearly every modern LLM \u2014 GPT, LLaMA, DeepSeek, etc. \u2014 uses a subword tokenizer (BPE, Unigram, or a variant) at its core.</p>"},{"location":"pretraining/tokenization/examples/#final-notes-about-bpe","title":"Final Notes About BPE","text":"<p>BPE is a simple but highly efficient algorithm for tokenization, but a few additional caveats to be aware of:</p> <ol> <li> <p>Handling non-ASCII characters.    In multilingual settings, non-ASCII characters may cause problems. If the model predicts invalid continuation tokens, you can end up with \u201cgarbled\u201d outputs (invalid byte artifacts).</p> </li> <li> <p>No notion of words.    BPE works purely on frequency of symbol pairs. It doesn\u2019t know about word boundaries, so sometimes splits can be unintuitive. For example:    \"unhappy\" \u2192 \"un\", \"hap\", \"py\"    While unlikely for very common words, this does illustrate the lack of semantic awareness.</p> </li> <li> <p>Inefficient for rare words.    Since BPE prioritizes frequent patterns, rare words often get split into many tokens.    Example: \"Mississippi\" \u2192 \"Mi\", \"ssi\", \"ssi\", \"ppi\" (4 tokens for 1 word).    Not catastrophic, but inefficient.</p> </li> <li> <p>Whitespace quirks.    Spaces often get \u201cglued\u201d to words (\" dog\" instead of \"dog\"). This is by design, but can look odd.</p> </li> <li> <p>Static once trained.    Once a BPE tokenizer is trained, its vocabulary is fixed. If new words become common later (say a new slang term or product name), they won\u2019t be merged automatically.</p> </li> </ol> <p>For this project, these limitations are minor \u2014 but it\u2019s good to be aware of them. Overall, BPE still provides an excellent trade-off and is much better suited for LLMs than raw character- or word-level tokenization.</p> <p>Here's a great Tokenization Playground to get a more hands-on experience with tokenization  </p> <p>Here's an example: </p> <p></p> <p>Note how most of the words is a single token, due to their common occurrence in the tokenizer's vocabulary. However once a relatively uncommon word appears, it would be broken down into chunks. </p>"},{"location":"pretraining/tokenization/overview/","title":"Overview","text":""},{"location":"pretraining/tokenization/overview/#what-is-tokenization","title":"What is Tokenization?","text":"<p>Now that we have a text corpus to work with, the next step is tokenization. Tokenization is the process of converting text into numerical values, since models can only work with numbers as input. Before a model can learn any patterns in language, it needs the text expressed in a consistent numerical form. Tokenization is the bridge between raw text and the numerical world that neural networks actually understand.</p>"},{"location":"pretraining/tokenization/overview/#the-big-picture","title":"The Big Picture","text":"<p>Take the example sentence:</p> <p><code>\"Hawaii is great for vacationing.\"</code></p> <p>This same sentence can be broken down in different ways depending on the tokenization approach:</p> <ul> <li>Character-level: <code>[\"H\",\"a\",\"w\",\"a\",\"i\",\"i\",\" \",\"i\",\"s\",\" \",\"g\",\"r\",\"e\",\"a\",\"t\",\" \",\"f\",\"o\",\"r\",\" \",\"v\",\"a\",\"c\",\"a\",\"t\",\"i\",\"o\",\"n\",\"i\",\"n\",\"g\",\".\"]</code></li> <li>Word-level: <code>[\"Hawaii\",\"is\",\"great\",\"for\",\"vacationing\",\".\"]</code></li> <li>Subword-level: <code>[\"Ha\",\"wa\",\"ii \",\"is \",\"great \",\"for \",\"vac\",\"ation\",\"ing\",\".\"]</code></li> </ul> <p>Notice how some words remain whole in the subword case (<code>\"is\"</code>, <code>\"great\"</code>, <code>\"for\"</code>), while others like <code>\"Hawaii\"</code> and <code>\"vacationing\"</code> are broken into pieces. Subword tokenization hits the middle ground between characters and words \u2014 efficient compression without losing flexibility for new or unusual words.</p>"},{"location":"pretraining/tokenization/overview/#why-subword-tokenization","title":"Why Subword Tokenization?","text":"<p>Character-level tokenization is too inefficient (long sequences), while word-level tokenization has problems with exploding vocabulary size and out-of-vocabulary words. Subword tokenization avoids both: it keeps vocabulary manageable while still handling unseen words by splitting them into smaller known pieces. That balance is why modern LLMs almost always use subword tokenization.</p> <p>So how do we actually decide the splits? That\u2019s where the tokenizer itself comes in. In the next section, we\u2019ll walk through how tokenizers are trained, starting with the most common approach: Byte Pair Encoding (BPE).</p>"},{"location":"pretraining/tokenization/project/","title":"Project Usage","text":""},{"location":"pretraining/tokenization/project/#tokenization-in-simplellama","title":"Tokenization in SimpleLLaMA","text":"<p>Now that the dataset has been gathered and sharded, the next step is to actually train a tokenizer and use it to encode the data into numerical form. This section walks through how this was done in SimpleLLaMA.</p>"},{"location":"pretraining/tokenization/project/#training-the-tokenizer","title":"Training the Tokenizer","text":"<p>We use a ByteLevel BPE tokenizer for this project, provided by the <code>tokenizers</code> library</p>"},{"location":"pretraining/tokenization/project/#special-tokens","title":"Special Tokens","text":"<p>On top of the usual vocabulary, we add custom tokens to support training:  </p> <ul> <li><code>&lt;SOS&gt;</code> : Start of sequence  </li> <li><code>&lt;EOS&gt;</code> : End of sequence  </li> <li><code>&lt;PAD&gt;</code> : Padding (for batching sequences of different lengths)  </li> <li><code>&lt;UNK&gt;</code> : Unknown token (fallback for anything not in vocab)  </li> <li><code>&lt;SOU&gt;</code> / <code>&lt;EOU&gt;</code> : Mark user messages in dialogue datasets  </li> <li><code>&lt;SOA&gt;</code> / <code>&lt;EOA&gt;</code> : Mark assistant messages  </li> <li><code>&lt;SOT&gt;</code> / <code>&lt;EOT&gt;</code> : Mark templates or system prompts  </li> </ul> <p>These tokens are crucial for supervised fine-tuning and RLHF stages later on, where we want the model to clearly distinguish between different roles and contexts.</p>"},{"location":"pretraining/tokenization/project/#example-script","title":"Example Script","text":"<pre><code>from tokenizers import Tokenizer, models, trainers\nfrom tokenizers.pre_tokenizers import ByteLevel\n\ntokenizer = Tokenizer(models.BPE())\ntokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n\nspecial_tokens = [\"&lt;SOS&gt;\", \"&lt;EOS&gt;\", \"&lt;PAD&gt;\", \"&lt;UNK&gt;\", \"&lt;SOU&gt;\", \"&lt;EOU&gt;\", \"&lt;SOA&gt;\", \"&lt;EOA&gt;\", \"&lt;SOT&gt;\", \"&lt;EOT&gt;\"]\ntrainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=special_tokens, min_frequency=16)\n\nfiles = [\"short_0001.txt\", \"short_0002.txt\", \"short_0003.txt\"]\ntokenizer.train(files, trainer)\ntokenizer.save(\"bpe_8k.json\")\n</code></pre> <p>Note the <code>vocab_size=8192</code> part. This is a value that we can adjust as needed. A vocabulary size of <code>8192</code> means that we undergo compression until we have 8192 mapping in our dict.  One can think of it as a balancer: If vocab size is set too low (e.g. 256), BPE will collaspe into character level tokenization. However if vocab size is too large, like 1 million, it will converge towards something like a word level tokenizer.  Generally, BPE tokenizers have vocabulary size of 32k+, however since we are dealing with ascii only dataset, 8192 works fine. </p>"},{"location":"pretraining/tokenization/project/#key-design-choices","title":"Key Design Choices","text":"<ul> <li>ByteLevel PreTokenizer \u2192 Ensures consistent handling of whitespace. For example, <code>\" dog\"</code> and <code>\"dog\"</code> are treated as distinct.  </li> <li>add_prefix_space=True \u2192 Preserves leading spaces, which otherwise could get lost.  </li> <li>Vocabulary size = 8192 \u2192 Small enough for efficient training, large enough to capture common words and subwords.  </li> <li>min_frequency=16 \u2192 Rare patterns are ignored, preventing the vocabulary from bloating with noise.  </li> </ul>"},{"location":"pretraining/tokenization/project/#encoding-the-dataset","title":"Encoding the Dataset","text":"<p>Once the tokenizer is trained, the next step is to encode the raw text into token IDs. This step converts every <code>.txt</code> shard that was previously gathered from FineWebEdu into a <code>.npy</code> file of integers.  </p> <p>Why? Because: - Encoding once upfront is faster than re-tokenizing on-the-fly. - <code>.npy</code> arrays are lightweight and can be memory-mapped during training. - Smaller storage space on system (~4:1 compression ratio, where assuming 1 byte is needed for each character, assuming 2 bytes per token, reduce storage to 50%)  </p>"},{"location":"pretraining/tokenization/project/#example-script_1","title":"Example Script","text":"<pre><code>from tokenizers import Tokenizer\nimport numpy as np, os\n\ntokenizer = Tokenizer.from_file(\"bpe_8k.json\")\ntokenizer.model.unk_token = \"&lt;UNK&gt;\"  # Set unknown token\n\nsrc_dir, dst_dir = \"short_1800\", \"short_1800_tokens\"\nos.makedirs(dst_dir, exist_ok=True)\n\nfor file in os.listdir(src_dir):\n    if file.endswith(\".txt\"):\n        text = open(os.path.join(src_dir, file)).read()\n        tokens = np.array(tokenizer.encode(text).ids, dtype=np.uint16)\n        np.save(f\"{dst_dir}/{file.replace('.txt','.npy')}\", tokens)\n</code></pre>"},{"location":"pretraining/tokenization/project/#notes","title":"Notes","text":"<ul> <li>We set <code>unk_token = \"&lt;UNK&gt;\"</code> as a fallback. In practice, almost all text will map cleanly since we used ByteLevel.  </li> <li>Tokens are stored as <code>uint16</code> because our vocabulary size is &lt; 2**16. This is more memory-efficient than <code>int32</code> or <code>int64</code>.  </li> <li>For a 10,000 character text file, this typically compresses down to ~2,500 tokens, depending on content.  </li> </ul> <p>By the end of this stage, the dataset has gone from raw text \u2192 clean shards \u2192 token IDs, all ready to be fed into the pretraining pipeline. For the remainder of this documentation/tutorial, I will show tokenization on word level for simplicity. </p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/","title":"Checkpointing and Evaluation in LLM Pretraining","text":"<p>Effective checkpointing and evaluation strategies are crucial for long-running LLM training sessions. This section covers how the training script manages model saving, progress tracking, and model assessments during pretraining.</p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#checkpoint-strategy-resumable-training","title":"Checkpoint Strategy: Resumable Training","text":""},{"location":"pretraining/training_advanced/ckpt_and_eval/#what-gets-saved","title":"What Gets Saved","text":"<p>The checkpointing method in this project saves everything needed to resume training exactly where it was left off:</p> <pre><code>save_ckpt = {\n    \"config\": config,                                 # Training configuration\n    \"model_state_dict\": model.state_dict(),           # Model weights\n    \"optimizer_state_dict\": optimizer.state_dict(),   # Optimizer state\n    \"scheduler_state_dict\": scheduler.state_dict(),   # Learning rate schedule\n    \"max_lr\": max_lr,                                 # Hyperparameters for validation\n    \"min_lr\": min_lr,\n    \"train_iterations\": train_iterations,\n    \"total_tok_trained\": total_tok_trained + prev_tok_trained,  # Total progress\n    \"file_idx\": dataset_loader.file_idx,               # Dataset position\n    \"tok_idx\": dataset_loader.tok_idx                  # Token position within file\n}\n</code></pre> <p>The most important items to save is:  </p> <ul> <li>Config: The configuration used in a certain training run</li> <li>Model State Dict: Holds model parameter and architecture</li> <li>Optimizer State Dict: Holds the first and second moment values per model parameter</li> <li>Scheduler State Dict: Holds the state of the scheduler</li> <li>Dataset Location: Most of the remaining values is used to update the dataset loader to point to where the previous run left off at</li> </ul> <p>This is mostly used as a way to ensure progress isn't lost if somehow the training process is suddenly stopped (e.g. power outage, accidental Ctrl+C and such)</p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#token-based-checkpoint-logic","title":"Token-Based Checkpoint Logic","text":"<p>Instead of saving based on training steps or time, this uses token-based checkpointing:</p> <pre><code># Token-based checkpoint intervals\ntoken_ckpt = int(1e9)          # Save model at every 1B tokens interval\nnext_token_ckpt = token_ckpt\n\nfor step in range(1, train_iterations+1):\n\n    ...  # Forward pass and loss calculation step\n\n    total_tok_trained += tokens_per_step  # Track total tokens processed\n\n    # Save when we hit checkpoint threshold or at the end\n    if (total_tok_trained &gt; next_token_ckpt or step == train_iterations) and master_process:\n        next_token_ckpt += token_ckpt\n\n        # Smart filename with tokens and loss\n        n = 2500  # Average last n losses for filename\n        avg_loss = int((sum(all_losses[-n:]) / len(all_losses[-n:])) * 1000)\n        combined_tokens = total_tok_trained + prev_tok_trained\n\n        if combined_tokens &lt; 1e10:\n            filename = f\"model_{int(combined_tokens / 1e6)}M_{avg_loss}L_{max_seq_len}MSQ.pth\"\n        else:\n            filename = f\"model_{int(combined_tokens / 1e9)}B_{avg_loss}L_{max_seq_len}MSQ.pth\"\n\n        torch.save(save_ckpt, f\"{ckpt_dir}/{filename}\")\n</code></pre> <p>Token based checkpointing is used because it's typically easier to compare against other checkpoints across different runs  </p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#resume-training-mechanics","title":"Resume Training Mechanics","text":"<p>When loading a checkpoint, we restore the exact training state:</p> <pre><code>if load_ckpt:\n    ckpt_dict, prev_tok_trained = load_checkpoint(config=config, ckpt_dir=ckpt_dir, \n                                                 ddp=ddp, master_process=master_process)\n\n    model.load_state_dict(ckpt_dict[\"model_state_dict\"])\n    optimizer.load_state_dict(ckpt_dict[\"optimizer_state_dict\"])\n\n    # Restore dataset position\n    dataset_loader.file_idx = ckpt_dict[\"file_idx\"]\n    dataset_loader.tok_idx = ckpt_dict[\"tok_idx\"]\n    dataset_loader.file_data = np.load(dataset_loader.filepaths[dataset_loader.file_idx])\n</code></pre> <p>This allows us to pause and resume training near seamlessly, which is essential for multi-day training runs.</p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#evaluation-metrics-quantifying-progress","title":"Evaluation Metrics: Quantifying Progress","text":"<p>During pretraining, the training loop logs a variety of metrics at regular intervals. These logs serve two roles: 1. Quantitative progress tracking (loss, perplexity, learning rate, token throughput). 2. Qualitative assessment through sampled generations.  </p> <p>Here is a typical output snippet when training a small model:</p> <pre><code>Step: 256 steps    |   Training Progress: 0.02%   |   Training Loss: 8.0160   |   Perplexity: 3029.09   |   Learning Rate: 0.00008   |   Norm: 1.0915   |   Tokens Processed: 8M (8M)     |   tok/s: 157961   |   Time: 53s\nStep: 512 steps    |   Training Progress: 0.04%   |   Training Loss: 7.0701   |   Perplexity: 1176.23   |   Learning Rate: 0.00015   |   Norm: 0.2549   |   Tokens Processed: 16M (16M)   |   tok/s: 142851   |   Time: 58s\nStep: 768 steps    |   Training Progress: 0.06%   |   Training Loss: 6.5323   |   Perplexity: 686.96    |   Learning Rate: 0.00023   |   Norm: 0.1649   |   Tokens Processed: 25M (25M)   |   tok/s: 187962   |   Time: 44s\nStep: 1024 steps   |   Training Progress: 0.07%   |   Training Loss: 5.8950   |   Perplexity: 363.23    |   Learning Rate: 0.00031   |   Norm: 0.2274   |   Tokens Processed: 33M (33M)   |   tok/s: 187884   |   Time: 44s\nStep: 1280 steps   |   Training Progress: 0.09%   |   Training Loss: 5.6318   |   Perplexity: 279.16    |   Learning Rate: 0.00038   |   Norm: 0.2636   |   Tokens Processed: 41M (41M)   |   tok/s: 187881   |   Time: 44s\n...\nStep: 3072 steps   |   Training Progress: 0.22%   |   Training Loss: 4.4724   |   Perplexity: 87.57     |   Learning Rate: 0.00060   |   Norm: 0.6141   |   Tokens Processed: 100M (100M) |   tok/s: 187720   |   Time: 44s\n</code></pre>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#training-loss","title":"Training Loss","text":"<p>The training loss is the mean cross-entropy loss over the current batch.  </p> <ul> <li>Early in training, loss values are very high (e.g., ~8.0 in the first steps), meaning the model is essentially guessing at random.  </li> <li>Over time, loss decreases and gradually plateaus, indicating the model has learned patterns in the dataset.  </li> </ul> <p>Loss is the most direct objective being optimized, but it is not always intuitive to interpret \u2014 which is where perplexity comes in.</p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#perplexity-a-more-intuitive-metric","title":"Perplexity: A More Intuitive Metric","text":"<p>Perplexity (PPL) is defined as the exponential of the cross-entropy loss:</p> \\[ \\text{Perplexity} = e^{\\text{Loss}} \\] <p>Interpretation: </p> <ul> <li>Perplexity can be thought of as \u201cthe average number of choices the model is uncertain between\u201d when predicting the next token.  </li> <li>A perplexity of ~3000 (as at step 256 above) means the model is basically clueless, nearly random.  </li> <li>As training progresses, perplexity falls sharply (to ~87 by step 3072 in the logs). This means that instead of being equally confused among thousands of tokens, the model is now narrowing down to a few dozen likely options.  </li> </ul> <p>In large-scale LLMs, tracking perplexity across billions of tokens is often one of the primary signal for when to stop training or when scaling laws are being followed.</p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#gradient-norm","title":"Gradient Norm","text":"<p>The Norm entry refers to the L2 norm of the gradients after backpropagation. It provides a health check for training stability.</p> <p>Mathematically, for a parameter vector \\(\\theta\\) with gradient \\(g = \\nabla_\\theta L(\\theta)\\), the L2 norm is:</p> \\[ \\| g \\|_2 = \\sqrt{\\sum_i g_i^2} \\] <p>This norm summarizes the overall magnitude of the gradients across all parameters.  </p> <ul> <li>Large norms \u2192 indicate exploding gradients, which can cause unstable training steps and parameters to diverge.  </li> <li>Tiny norms \u2192 suggest vanishing gradients, where updates become so small that learning stalls.  </li> </ul> <p>In practice, large gradient norms can result in disproportionately large parameter updates, especially if only a few layers produce outlier gradients. To mitigate this, gradient clipping is applied:</p> <pre><code>norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n</code></pre> <p>This clamps the total L2 norm of gradients to a maximum value (here, 1.0). If the computed norm exceeds this threshold, the gradients are rescaled proportionally:</p> \\[ g_i \\leftarrow g_i \\cdot \\frac{\\text{max_norm}}{\\| g \\|_2} \\] <p>This prevents any single update step from destabilizing the training.  </p> <p>In the logged training runs, gradient norms typically range between 0.16 and 0.6 (after the very first steps), which is a healthy regime. This indicates that updates are neither too aggressive nor too weak, and clipping acts only as a safeguard against rare spikes.</p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#throughput-tokens-per-second","title":"Throughput: Tokens per Second","text":"<p>Another practical metric logged is tok/s, or tokens processed per second. This is calculated as:</p> <pre><code>int((eval_interval * tokens_per_step) // elapsed)\n</code></pre> <p>where <code>tokens_per_step</code> = batch_size \u00d7 sequence_length \u00d7 world_size.  </p> <p>Throughput is crucial for estimating wall-clock training time. For example:  </p> <ul> <li>At ~187k tokens/s (as in the logs), training 50B tokens would take about 3 days on this configuration.  </li> <li>Drops in throughput can indicate hardware bottlenecks (e.g., I/O, CPU-GPU imbalance).  </li> </ul>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#sampled-generations-qualitative-evaluation","title":"Sampled Generations: Qualitative Evaluation","text":"<p>At scheduled intervals, the model is prompted with fixed few-shot inputs and its completions are logged. These samples act as a sanity check:  </p> <ul> <li>Early stage: outputs are mostly garbled text with nonsensical words.  </li> <li>Later stage: the model starts forming coherent sentences, even if they are still awkward or logically inconsistent.  </li> </ul> <p>This qualitative feedback complements perplexity: the two together give a better picture of progress.</p>"},{"location":"pretraining/training_advanced/ckpt_and_eval/#why-these-metrics-matter","title":"Why These Metrics Matter","text":"<ol> <li>Loss/Perplexity \u2192 Track whether the model is genuinely learning patterns from data.  </li> <li>Gradient Norm \u2192 Prevent silent training failures due to exploding or vanishing gradients.  </li> <li>Throughput \u2192 Ensure compute resources are fully utilized and allow scaling estimates.  </li> <li>Sampled Generations \u2192 Provide interpretable checkpoints for human inspection.  </li> </ol> <p>Together, these metrics provide both numerical rigor and human-readable signals during long training runs, making it possible to monitor and debug billion-token experiments without waiting until the very end.</p>"},{"location":"pretraining/training_advanced/ddp/","title":"Distributed Data Parallel","text":""},{"location":"pretraining/training_advanced/ddp/#introduction","title":"Introduction","text":"<p>Training modern large language models requires immense computational resources. Even for a small LLM like the one trained in this project, with 1.3B parameters, training on 50B tokens would take many tens of days on a single GPU. Scaling upwards on larger model and bigger dataset sizes, it would become infeasible, and so we need to find a way to expand to multiple GPU usage. </p> <p>Distributed Data Parallel (DDP) is PyTorch's solution for multi-GPU training that allows you to:  </p> <ul> <li>Distribute the training workload across multiple GPUs</li> <li>Scale to much larger batch sizes</li> <li>Reduce training time significantly</li> <li>Utilize expensive hardware efficiently</li> </ul> <p>In this guide, we'll explore how DDP works and how it's implemented in the LLM training script.</p>"},{"location":"pretraining/training_advanced/ddp/#how-ddp-works-the-core-concepts","title":"How DDP Works: The Core Concepts","text":""},{"location":"pretraining/training_advanced/ddp/#1-process-based-parallelism","title":"1. Process-Based Parallelism","text":"<p>DDP uses a multi-process approach where each GPU runs its own independent process with a complete copy of the model:</p> <pre><code>Process 0 (GPU 0)   Process 1 (GPU 1)   Process 2 (GPU 2)\n     \u2193                   \u2193                   \u2193\n Model Copy 0       Model Copy 1         Model Copy 2\n     \u2193                   \u2193                   \u2193\n Data Shard 0       Data Shard 1         Data Shard 2\n</code></pre> <p>Here, each process has it's own model replica. In each forward pass, they would get their own subset of data, compute gradients independently, and synchronize gradients across all processes.</p>"},{"location":"pretraining/training_advanced/ddp/#2-the-synchronization-process","title":"2. The Synchronization Process","text":"<p>The key to DDP is gradient synchronization. Here's what happens each training step:</p> <ol> <li>Forward Pass: Each GPU processes its mini-batch independently  </li> <li>Backward Pass: Each GPU computes gradients for its portion  </li> <li>All-Reduce: Gradients are averaged across all GPUs  </li> <li>Optimizer Step: Each GPU updates its model weights identically  </li> </ol> <p>This ensures all model replicas stay synchronized throughout training.</p>"},{"location":"pretraining/training_advanced/ddp/#ddp-implementation-in-our-training-code","title":"DDP Implementation in Our Training Code","text":""},{"location":"pretraining/training_advanced/ddp/#initialization-and-setup","title":"Initialization and Setup","text":"<p>Let's examine how DDP is initialized in the script:</p> <pre><code># DDP Initialization\nddp = \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ\nif ddp:\n    assert torch.cuda.is_available(), \"Should have cuda available if using DDP!\"\n    init_process_group(backend=\"nccl\")  # Initialize the distributed communication backend\n    ddp_rank = int(os.environ[\"RANK\"])\n    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n    device = f\"cuda:{ddp_rank}\"\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0\nelse:  # Non-Distributed setup\n    ddp_rank = 0\n    ddp_world_size = 1\n    master_process = True\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</code></pre> <p>Key Components: </p> <ul> <li>RANK: Unique identifier for each process (0, 1, 2, ...)  </li> <li>WORLD_SIZE: Total number of processes (equal to number of GPUs)  </li> <li>backend=\"nccl\": NVIDIA Collective Communications Library - optimized for GPU-to-GPU communication  </li> <li>Master Process: This is the Rank 0 process that handles logging, checkpointing, and other major functionalities</li> </ul> <p>Small note about Master Process:  When training on a single GPU, that process is the only process running, which is the master process. However when dealing with multiple GPUs, we would need to choose a single process to serve as the 'master process'. Otherwise, when we save model checkpoints, loggings, and such later on, it will be duplicated many times.  By convention, master process is the process with a Rank value of 0. </p>"},{"location":"pretraining/training_advanced/ddp/#launching-ddp-training","title":"Launching DDP Training","text":"<p>DDP requires a specific launch command that sets up the environment variables:</p> <pre><code># Launch with 8 GPUs\ntorchrun --standalone --nproc_per_node=8 train.py\n\n# Alternative older syntax\npython -m torch.distributed.launch --nproc_per_node=8 train.py\n</code></pre> <p>The <code>torchrun</code> command automatically sets:  </p> <ul> <li>RANK - process rank (0 to 7)  </li> <li>WORLD_SIZE - total processes (8)  </li> <li>LOCAL_RANK - local GPU index  </li> </ul>"},{"location":"pretraining/training_advanced/ddp/#data-distribution-strategy","title":"Data Distribution Strategy","text":"<p>Batch Distribution In DDP, we distribute various batches to different GPUs.  Recall the example in the previous section about gradient accumulation.  Assuming we have <code>batch_size=4</code>, <code>seq_len=2048</code>, <code>tokens_per_update=2**19</code>, then we would need 64 forward passes before we take a single optimizer step.  </p> <p>However if we now parallize this operation across 4 GPUs, in each forward pass, we would process 4 subsets at once, reducing the total number of forward passes by a factor of <code>num_gpus</code>, in this case, from 64 forwards passes per optimizer step to 16 forward passes per optimizer step (since each forward pass now is equivalent to 4 forward pass on a single GPU)</p> <p>Our dataset loader handles this distribution:</p> <pre><code>dataset_loader = DatasetLoader(\n    batch=batch_size, \n    seq_len=max_seq_len, \n    process_rank=ddp_rank,\n    num_processes=ddp_world_size, \n    dataset_dir=config.dataset_dir, \n    device=device\n)\n</code></pre> <p>Each process gets a different shard of data, ensuring no overlap between GPUs.</p>"},{"location":"pretraining/training_advanced/ddp/#gradient-accumulation-with-ddp","title":"Gradient Accumulation with DDP","text":"<p>Gradient accumulation requires special handling in DDP. We need to ensure the accumulation steps are properly synchronized:</p> <pre><code># Need to make sure gradient accumulation step is evenly divisible by # GPUs\nassert grad_accum_steps % ddp_world_size == 0, (\n    f\"{grad_accum_steps=} % {ddp_world_size=} != 0\\n\"\n    f\"Please adjust 'tokens_per_update' in config file accordingly!\"\n)\n\n# Adjust accumulation steps per process\ngrad_accum_steps = grad_accum_steps // ddp_world_size\n</code></pre> <p>This matters because each process accumulates gradients independently. We need to ensure the total accumulation across all processes matches our desired effective batch size.</p>"},{"location":"pretraining/training_advanced/ddp/#model-wrapping-and-compilation","title":"Model Wrapping and Compilation","text":"<p>DDP Model Setup The model needs to be wrapped with DDP after moving it to the appropriate device:</p> <pre><code># Basic DDP wrapping\nif ddp:\n    model_handle = DDP(model, device_ids=[ddp_rank])\n</code></pre> <p>Advanced: DDP with Model Compilation Our code handles the complex interaction between DDP and <code>torch.compile</code>:</p> <pre><code># Compiling the model via torch.compile reduces the training time\n# Though may not be compatible with certain GPUs. If so, turn \"compile_model\" in config to False\nif enable_compilation and ddp:\n    # model_handle = DDP(torch.compile(model), device_ids=[ddp_rank]) REMOVE\n\n    # Interestingly enough, DDP docs recommends applying ddp wrapper before compiling\n    # Karpathy's implementation is the other way around, compile -&gt; ddp wrapper\n    model_handle = torch.compile(DDP(model, device_ids=[ddp_rank]))\nelif enable_compilation and not ddp:\n    model_handle = torch.compile(model)\nelif ddp:\n    model_handle = DDP(model, device_ids=[ddp_rank])\nelse:\n    model_handle = model  # Plain case, not recommended for actual usage\n</code></pre> <p>Important: The order matters! As mentioned in the comments, the DDP documentations from PyTorch said that the recommended order is apply DDP first, then compile the model. However others have also done it the other way around. Seems like people are split between the two? Here, I just follow the docs recommendation. </p> <p>(Further details about model compilation will be detailed in the <code>Throughput Optimizations</code> page)</p>"},{"location":"pretraining/training_advanced/ddp/#gradient-synchronization-mechanics","title":"Gradient Synchronization Mechanics","text":"<p>The All-Reduce Operation DDP uses an all-reduce operation to synchronize gradients. Here's what happens: During each backward pass, all GPUs compute their local gradients based on the given batch of data. When we are about to update the parameter values in the mode, applying All-reduce averages gradients across GPUs The result is that every GPU has identical averaged gradients and the model is kept in sync.</p> <p>Efficient Synchronization with Gradient Accumulation For gradient accumulation, we need to control when synchronization happens:</p> <pre><code># Only synchronize if at the step right before stepping optimizer\nif ddp:\n    model_handle.require_backward_grad_sync = (step % grad_accum_steps == 0)\n\nloss.backward()\n\nif step % grad_accum_steps == 0:\n    optimizer.step()\n    optimizer.zero_grad(set_to_none=True)\n</code></pre> <p>This ensures we only perform the expensive all-reduce operation when we're actually ready to update weights.</p>"},{"location":"pretraining/training_advanced/ddp/#theoretical-speedup","title":"Theoretical Speedup","text":"<p>The ideal speedup with DDP is nearly linear:  </p> <ul> <li>2 GPUs: ~1.9x speedup  </li> <li>4 GPUs: ~3.8x speedup  </li> <li>8 GPUs: ~7.5x speedup  </li> </ul> <p>Not fully linear due to additional All-Reduce operations and various overheads, though not too much.</p>"},{"location":"pretraining/training_advanced/ddp/#integration-with-other-parallelism-strategies","title":"Integration with Other Parallelism Strategies","text":"<p>DDP can be combined with other parallelism methods:</p> <ul> <li> <p>Pipeline Parallelism   Split model layers across GPUs   DDP handles data parallelism within each stage  </p> </li> <li> <p>Tensor Parallelism   Split individual layers across GPUs   Often used with DDP for extreme scaling  </p> </li> </ul> <p>Our Current Approach: We're using pure data parallelism, which is sufficient for models that fit on a single GPU. As models grow larger, you might need to combine DDP with these other strategies.</p>"},{"location":"pretraining/training_advanced/ddp/#conclusion","title":"Conclusion","text":"<p>DDP is a powerful tool that makes multi-GPU training remarkably straightforward. This implementation demonstrates:</p> <ul> <li>Proper initialization with environment variables  </li> <li>Model wrapping and compilation order  </li> <li>Gradient accumulation with controlled synchronization  </li> <li>Master-process coordinations</li> </ul> <p>The key insight is that DDP allows us to think about training in terms of global batch sizes while automatically handling the distribution across multiple GPUs. This abstraction makes it possible to scale training without rewriting the entire training loop.</p> <p>As models continue to scale, understanding DDP will be essential for efficient resource utilization and achieving state-of-the-art results.</p>"},{"location":"pretraining/training_advanced/final_walkthrough/","title":"Training Script Walkthrough","text":"<p>This final piece of documentation in the training guide section provides a sequential walkthrough of the LLM training script, explaining each major section and how everything connects.</p>"},{"location":"pretraining/training_advanced/final_walkthrough/#initial-setup-and-imports","title":"Initial Setup and Imports","text":"<pre><code>import os\nimport time\nimport random\nimport math\nimport inspect\nimport numpy as np\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nfrom tokenizers import Tokenizer, decoders\n\nfrom simple_llama.pretraining.llama_transformer import LLaMaTransformer\nfrom simple_llama.pretraining.dataset_loader import DatasetLoader\nfrom simple_llama.pretraining.lr_scheduler import Scheduler\nfrom simple_llama.pretraining.utils import load_checkpoint, few_shot_prompts, check_log_file_existence\nfrom simple_llama.pretraining.config import TrainingConfig\n</code></pre> <p>Key imports:</p> <ul> <li><code>torch.distributed</code>: For multi-GPU training support  </li> <li><code>tokenizers</code>: Hugging Face tokenizer for text processing  </li> <li>Custom modules: Model architecture, data loading, and utilities  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#distributed-training-setup","title":"Distributed Training Setup","text":"<pre><code># To run, use `torchrun --standalone --nproc_per_node=8 train.py`\n# Set global variables for DDP\nddp = \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ\nif ddp:\n    assert torch.cuda.is_available(), \"Should have cuda available if using DDP!\"\n    init_process_group(backend=\"nccl\")  # Initialize the distributed communication backend\n    ddp_rank = int(os.environ[\"RANK\"])\n    # Assuming this is single-node multi-GPU setup, so I'm not using local_rank\n    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n    device = f\"cuda:{ddp_rank}\"\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0\nelse:  # Non-Distributed setup. Either CPU or single GPU\n    ddp_rank = 0\n    ddp_world_size = 1\n    master_process = True\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Currently using {device=}\n\")\n</code></pre> <p>What this does:</p> <ul> <li>Checks if we're running in a distributed environment by looking for RANK and WORLD_SIZE environment variables (Automatically set by <code>torchrun</code> when used)</li> <li>Initializes the process group with NCCL backend for GPU communication  </li> <li>Sets device to the appropriate GPU for each process  </li> <li>Designates rank 0 as the <code>master_process</code> for logging and checkpointing  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#reproducibility-and-performance-settings","title":"Reproducibility and Performance Settings","text":"<pre><code># Manual seeding for reproducibility testings\nseed = 89\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n# Setting to 'high' uses TF32 rather than FP32, which makes the training process faster (varies on machines)\n# Can set to 'medium' for even faster training, though will be loss in performance\ntorch.set_float32_matmul_precision(\"high\")\n</code></pre> <p>Using the same random seeds ensure training is reproducible across runs and TF32 precision provides speedup on NVIDIA Ampere+ GPUs while maintaining accuracy  </p>"},{"location":"pretraining/training_advanced/final_walkthrough/#configuration-loading","title":"Configuration Loading","text":"<pre><code># Hyperparameters\n# --------------------------------------\nconfig = TrainingConfig()\n\n# Unpack values from config for convenience\nenable_compilation = config.enable_compilation\n\nbatch_size = config.batch_size\nmax_seq_len = config.max_seq_len\n\neval_interval = config.eval_interval\ntraining_tokens = config.training_tokens\n\nwarmup_iterations = config.warmup_iterations\nmax_lr = config.max_lr\nmin_lr = config.min_lr\nbeta1 = config.beta1\nbeta2 = config.beta2\nweight_decay = config.weight_decay\n\ngrad_accum_steps = config.grad_accum_steps\nload_ckpt = config.load_ckpt\ntoken_ckpt = config.token_ckpt\nuse_prev_scheduler = config.use_prev_scheduler\n\nlog_file = config.log_file\nmodel_gen_multiplier = config.model_gen_multiplier\n\neval_interval *= grad_accum_steps  # So evaluate model after eval_interval number of gradient updates\n# --------------------------------------\n</code></pre> <p>Key configuration values for our 1.3B model:</p> <ul> <li><code>batch_size = 4</code> sequences per GPU  </li> <li><code>max_seq_len = 2048</code> tokens per sequence  </li> <li><code>training_tokens = 45,000,000,000</code> (45B tokens)  </li> <li><code>grad_accum_steps = 64</code> (for effective batch size of 524,288 tokens)  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#distributed-training-adjustments","title":"Distributed Training Adjustments","text":"<pre><code># Need to make sure gradient accumulation step is evenly divisible by # GPUs\nassert grad_accum_steps % ddp_world_size == 0, (f\"{grad_accum_steps=} % {ddp_world_size=} != 0\\n\"\n                                                f\"Please adjust 'tokens_per_update' in config file accordingly!\")\n\ngrad_accum_steps = grad_accum_steps // ddp_world_size\n\n# Do the same for eval interval\nassert eval_interval % ddp_world_size == 0, (f\"{eval_interval=} % {ddp_world_size=} != 0\\n\"\n                                             f\"Please adjust 'eval_interval' in config file accordingly!\")\n\neval_interval = eval_interval // ddp_world_size\n</code></pre> <p>These adjustments are needed in DDP because since each GPU accumulates gradients independently, we need to ensure all GPUs perform the same number of accumulation steps and evaluation intervals must be synchronized across processes.  </p> <p>Note the <code>grad_accum_steps</code> update.  If <code>ddp_world_size</code> is 1, meaning single GPU training, then <code>grad_accum_steps</code> remains the same. However, if <code>ddp_world_size</code> is 8, meaning training is being parallized across 8 GPUs, then <code>grad_accum_steps</code> would be reduced by 1/8.  </p> <p>For the remainder of this walkthrough, we'll assume <code>ddp_world_size=8</code> and <code>grad_accum_steps=8</code></p>"},{"location":"pretraining/training_advanced/final_walkthrough/#logging-setup","title":"Logging Setup","text":"<pre><code>if master_process:  # Check if log_file already exists and deal with it accordingly\n    log_file = check_log_file_existence(log_file, ddp)\n\nif master_process:\n    with open(log_file, \"a\") as f:\n        columns = [\"step\", \"progress (%)\", \"Training Loss\", \"Perplexity\", \"Learning Rate\", \"L2 Norm\",\n                   \"Tokens Processed (Current- In Millions)\", \"Tokens Processed (Total- In Millions)\",\n                   \"Tokens Per Second\", \"Time Per Evaluation\"]\n        f.write(\",\".join(columns))\n        f.write(\"\\n\")\n</code></pre> <p>Logging strategy:</p> <ul> <li>Only the master process handles file I/O to avoid conflicts  </li> <li>CSV format for easy analysis and plotting  </li> <li>Various metrics to monitor training progress  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#training-calculations","title":"Training Calculations","text":"<pre><code>tokens_per_step = batch_size * max_seq_len * ddp_world_size\ntokens_per_opt_step = tokens_per_step * grad_accum_steps   # How many tokens to process before optimization step\ntrain_iterations = int(training_tokens // tokens_per_step)\noptimization_steps = train_iterations // grad_accum_steps  # Number of times to step the optimizer\n\nckpt_dir = config.ckpt_dir\nos.makedirs(ckpt_dir, exist_ok=True)\n</code></pre> <p>Example calculations for 8 GPUs:</p> <ul> <li><code>tokens_per_step = 4 \u00d7 2048 \u00d7 8 = 65,536 tokens/step</code> </li> <li><code>tokens_per_opt_step = 65,536 \u00d7 8 = 524,288 tokens/optimizer_step</code> </li> <li><code>train_iterations = 45,000,000,000 \u00f7 65,536 \u2248 686,645 steps</code> </li> <li><code>optimization_steps = 686,645 \u00f7 8 = 85,830 optimizer steps</code> </li> </ul> <p>Optimization steps is divided by <code>grad_accum_steps</code> because we only step the optimizer (update parameter) after each round of gradient accumulations. </p>"},{"location":"pretraining/training_advanced/final_walkthrough/#model-and-data-initialization","title":"Model and Data Initialization","text":"<pre><code># Instantiate dataset_loader obj\nbytes_per_token = 2  # 2 byte per token (Assuming using uint16)\ndataset_loader = DatasetLoader(batch=batch_size, seq_len=max_seq_len, process_rank=ddp_rank,\n                               num_processes=ddp_world_size, dataset_dir=config.dataset_dir, device=device)\nif master_process:\n    dataset_loader.print_ds_info(bytes_per_token=bytes_per_token)\n    print(f\"{dataset_loader.file_idx=}\")\n    print(f\"{dataset_loader.tok_idx=}\")\n\n\n# Load in pretrained tokenizer\ntokenizer = Tokenizer.from_file(config.tokenizer_path)\ntokenizer.model.unk_token = \"&lt;UNK&gt;\"  # Set unknown token to &lt;UNK&gt;\ntokenizer.decoder = decoders.ByteLevel()  # For byte-level decoding\n\n\n# Create model\nmodel = LLaMaTransformer(\n    config=config,\n    tokenizer=tokenizer,\n    device=device,\n).to(device)\nmodel.train()\n</code></pre> <p>The dataset loader handles sharding across multiple GPUs and streams data from disk to handle large datasets. </p> <p>The 1.3B param model is initialized primarily with: </p> <ul> <li>24 transformer layers  </li> <li>2048 embedding dimension  </li> <li>32 attention heads (64-dim each)  </li> <li>RoPE positional embeddings  </li> <li>SwiGLU activation functions  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#optimizer-and-scheduler-setup","title":"Optimizer and Scheduler Setup","text":"<pre><code># Betas, weight decay, and scheduler follows the LLaMa paper, with the exception of the learning rate\n# Used fused operations if available, from Dr. Karpathy's video\nfused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\nuse_fused = fused_available and (device == \"cuda\" or ddp)\nextra_args = dict(fused=True) if use_fused else dict()\noptimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(beta1, beta2), weight_decay=weight_decay, **extra_args)\nif master_process:\n    print(f\"Using fused optimizer: {use_fused}\\n\")\n\n# Instantiating CE Loss and scheduler\ncriterion = torch.nn.CrossEntropyLoss()\nscheduler = Scheduler(torch_optimizer=optimizer,\n                      schedule=\"cosine\",\n                      training_steps=optimization_steps,\n                      warmup_steps=warmup_iterations,\n                      max_lr=max_lr,\n                      min_lr=min_lr)\n</code></pre> <p>In this section:</p> <ul> <li>Check if AdamW supports fused kernels for better performance before instantiation</li> <li>Create the Criterion (using CrossEntropyLoss which is typical when training LLMs)</li> <li>Create a (custom) scheduler based on cosine learning rate schedule with warmup  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#checkpoint-loading","title":"Checkpoint Loading","text":"<pre><code># prev_tok_trained would be how many tokens the model has already been trained (for loading in models, if applicable)\nprev_tok_trained = 0\n\n# Loading in checkpoint to resume training if needed\nif load_ckpt:\n    ckpt_dict, prev_tok_trained = load_checkpoint(config=config, ckpt_dir=ckpt_dir, ddp=ddp, master_process=master_process)\n\n    model.load_state_dict(ckpt_dict[\"model_state_dict\"])\n    optimizer.load_state_dict(ckpt_dict[\"optimizer_state_dict\"])\n\n    # Manually check the scheduler here, T_max and eta_min should match, if not, can lead to undefined behaviors\n    if use_prev_scheduler:\n        assert ckpt_dict[\"max_lr\"] == max_lr\n        assert ckpt_dict[\"min_lr\"] == min_lr\n        assert ckpt_dict[\"train_iterations\"] == train_iterations\n        scheduler.load_state_dict(ckpt_dict[\"scheduler_state_dict\"])\n\n        dataset_loader.file_idx = ckpt_dict[\"file_idx\"]\n        dataset_loader.tok_idx = ckpt_dict[\"tok_idx\"]\n        dataset_loader.file_data = np.load(dataset_loader.filepaths[dataset_loader.file_idx])\n</code></pre> <p>For checkpoint restoration, this would need to load in the model and optimizer state dicts, and if desired, continue exactly from where the previous run left off at. </p>"},{"location":"pretraining/training_advanced/final_walkthrough/#model-compilation-and-ddp-wrapping","title":"Model Compilation and DDP Wrapping","text":"<pre><code># Compiling the model via torch.compile reduces the training time\n# Though may not be compatible with certain GPUs. If so, turn \"compile_model\" in config to False\nif enable_compilation and ddp:\n    # Interestingly enough, DDP docs recommends applying ddp wrapper before compiling\n    # Karpathy's implementation is the other way around, compile -&gt; ddp wrapper\n    model_handle = torch.compile(DDP(model, device_ids=[ddp_rank]))\nelif enable_compilation and not ddp:\n    model_handle = torch.compile(model)\nelif ddp:\n    model_handle = DDP(model, device_ids=[ddp_rank])\nelse:\n    model_handle = model  # Plain case, not recommended for actual usage\n</code></pre> <p>Notice that no matter if we compile, apply DDP, do both or do none, the resulting model will be called <code>model_handle</code>.  That's because when we need to checkpoint the model, we need the underlying model itself, not the wrapped DDP/Compiled version, and so this deals with separation of concerns.</p> <p>Important note about compilation order:</p> <ul> <li>Current code uses <code>torch.compile(DDP(model))</code> which follows DDP documentation  </li> <li>Some implementations use <code>DDP(torch.compile(model))</code> \u2014 both have tradeoffs  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#training-loop-initialization","title":"Training Loop Initialization","text":"<pre><code>total_tok_trained = 0  # Keeping track of total current tokens that has been processed\nnext_token_ckpt = token_ckpt\n\neos_token = tokenizer.encode(\"&lt;EOS&gt;\").ids[0]\nstart = time.time()\nall_losses = []  # Keeping track of all losses\nsave_ckpt = {}  # Used to save model checkpoint (Holds all state_dicts, hyperparameters, etc.)\nnorm = float(\"inf\")  # A temp placeholder for actual norm\n\n# This autocasts certain parts of the layers (mostly matmul portion) within the model to bf16 for faster training\nuse_amp = torch.cuda.is_available() and (device == \"cuda\" or ddp) and torch.cuda.is_bf16_supported()\nif master_process:\n    print(f\"Using auto mixed precision: {use_amp}\")\n</code></pre> <p>Tracking variables:</p> <ul> <li><code>total_tok_trained</code>: Counts tokens processed in current run  </li> <li><code>next_token_ckpt</code>: Token count for next checkpoint save  </li> <li><code>all_losses</code>: History for checkpoint naming and analysis  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#main-training-loop","title":"Main Training Loop","text":"<pre><code>for step in range(1, train_iterations+1):\n    x, y = dataset_loader.get_batch()\n\n    with torch.autocast(device_type=\"cuda\" if \"cuda\" in device else \"cpu\", dtype=torch.bfloat16 if use_amp else torch.float32):\n        pred = model_handle(x)\n        B, T, C = pred.shape\n        loss = criterion(pred.reshape(B * T, C), y.reshape(B * T))\n</code></pre> <p>Each iteration begins by fetching a batch of input and target sequences, here shaped <code>(4, 2048)</code>, based on the configuration.  The forward pass is run inside a <code>torch.autocast</code> context, which enables mixed-precision execution (BF16 where available) to improve speed and memory efficiency.  The model outputs predictions of shape <code>(B, T, C)</code>, which are then compared against the targets using cross-entropy loss. This loss measures how well the model\u2019s predicted distributions align with the true next tokens across all sequence positions.</p>"},{"location":"pretraining/training_advanced/final_walkthrough/#gradient-accumulation-and-backward-pass","title":"Gradient Accumulation and Backward Pass","text":"<pre><code>    train_loss_value = loss.item()\n    loss /= grad_accum_steps\n\n    if ddp:\n        model_handle.require_backward_grad_sync = (step % grad_accum_steps == 0)\n    loss.backward()\n\n    total_tok_trained += tokens_per_step\n    all_losses.append(train_loss_value)\n</code></pre> <p>The computed loss is divided by the number of accumulation steps so that gradients average correctly across multiple smaller batches.  In distributed setups, gradient synchronization is deferred until the end of an accumulation cycle (<code>step % grad_accum_steps == 0</code>) to reduce communication overhead.  The backward pass then contributes gradients to parameters, while counters track total tokens processed and log raw loss values. This strategy allows training with effectively large batch sizes even on limited GPU memory, while keeping updates consistent across devices.</p>"},{"location":"pretraining/training_advanced/final_walkthrough/#optimizer-step","title":"Optimizer Step","text":"<pre><code>    if step % grad_accum_steps == 0:\n        scheduler.step(step // grad_accum_steps)\n        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n</code></pre> <p>Optimizer step details:</p> <ul> <li>Scheduler steps on optimizer steps, not training steps  </li> <li>Gradient clipping at 1.0 prevents explosion  </li> <li><code>set_to_none=True</code> is more memory efficient than zeroing  </li> </ul>"},{"location":"pretraining/training_advanced/final_walkthrough/#checkpoint-saving","title":"Checkpoint Saving","text":"<pre><code>    if (total_tok_trained &gt; next_token_ckpt or step == train_iterations) and master_process:\n        next_token_ckpt += token_ckpt\n\n        save_ckpt[\"config\"] = config\n        save_ckpt[\"model_state_dict\"] = model.state_dict()\n        save_ckpt[\"optimizer_state_dict\"] = optimizer.state_dict()\n        save_ckpt[\"scheduler_state_dict\"] = scheduler.state_dict()\n        save_ckpt[\"max_lr\"] = max_lr\n        save_ckpt[\"min_lr\"] = min_lr\n        save_ckpt[\"train_iterations\"] = train_iterations\n        save_ckpt[\"total_tok_trained\"] = total_tok_trained + prev_tok_trained\n        save_ckpt[\"file_idx\"] = dataset_loader.file_idx\n        save_ckpt[\"tok_idx\"] = dataset_loader.tok_idx\n\n        n = 2500\n        avg_loss = int((sum(all_losses[-n:]) / len(all_losses[-n:])) * 1000)\n        combined_tokens = total_tok_trained + prev_tok_trained\n        if combined_tokens &lt; 1e10:\n            torch.save(save_ckpt, f\"{ckpt_dir}/model_{int(combined_tokens / 1e6)}M_{avg_loss}L_{max_seq_len}MSQ.pth\")\n        else:\n            torch.save(save_ckpt, f\"{ckpt_dir}/model_{int(combined_tokens / 1e9)}B_{avg_loss}L_{max_seq_len}MSQ.pth\")\n</code></pre> <p>At every <code>token_ckpt</code> token interval (or at the very last step of the training run), save a copy of the state at that point.  Then calculate the average loss in the past <code>n</code> steps which would be used to name the checkpoint file in conjunction with training tokens.</p>"},{"location":"pretraining/training_advanced/final_walkthrough/#evaluation-and-logging","title":"Evaluation and Logging","text":"<pre><code>    if step % eval_interval == 0 and master_process:\n        if torch.cuda.is_available() and device == \"cuda\":\n            torch.cuda.synchronize()\n\n        elapsed = time.time() - start\n        current_lr = optimizer.param_groups[0][\"lr\"]\n        tokens_processed = int(total_tok_trained // 1e6)\n\n        with open(log_file, \"a\") as f:\n            write_data = [step, round((step / train_iterations) * 100, 2), round(train_loss_value, 4), \n                         round(math.e ** train_loss_value, 2), round(current_lr, 4), round(norm.item(), 4),\n                         tokens_processed, int(prev_tok_trained // 1e6) + tokens_processed,\n                         int((eval_interval * tokens_per_step) // elapsed), int(elapsed)]\n            f.write(\",\".join([str(wd) for wd in write_data]))\n            f.write(\"\\n\")\n\n        print(\"----------------\")\n        print(f\"Step: {step} steps   |   Training Progress: {(step / train_iterations) * 100:.2f}%   |   \"\n              f\"Training Loss: {train_loss_value:.4f}   |   Perplexity: {math.e ** train_loss_value:.2f}   |   \"\n              f\"Learning Rate: {current_lr:.5f}   |   Norm: {norm.item():.4f}   |   \"\n              f\"Tokens Processed: {tokens_processed}M ({int(prev_tok_trained // 1e6) + tokens_processed}M)   |   \"\n              f\"tok/s: {int((eval_interval * tokens_per_step) // elapsed)}   |   Time: {int(elapsed)}s\")\n\n        start = time.time()\n</code></pre> <p>At regular intervals, the training script logs key metrics to both console and file.  These include training progress, loss, and perplexity (computed as <code>exp(loss)</code> for easier interpretation), along with learning rate, gradient norm, and tokens processed.  Token throughput (<code>tok/s</code>) is also tracked to measure efficiency.  Synchronizing CUDA before timing ensures accurate elapsed measurements, making these logs a reliable snapshot of both training stability and performance.</p>"},{"location":"pretraining/training_advanced/final_walkthrough/#text-generation-samples","title":"Text Generation Samples","text":"<pre><code>        if 'next_gen_step' not in locals():\n            next_gen_step = step\n\n        if step &gt;= next_gen_step:\n            print(\"\\n\")\n            print(model.generate(random.choice(few_shot_prompts), 64, 1.0, 0.8, eos_token=eos_token))\n            next_gen_step = int(step * model_gen_multiplier)\n            print(\"\\n\")\n            print(f\"Sampled generation at {step=}, next at {next_gen_step=}\")\n\n        print(\"----------------\")\n</code></pre> <p>In addition to numeric metrics, the model is periodically prompted to generate text from a random few-shot example.  The interval between generations grows exponentially and the check helps confirm that the model is learning to produce structured, human-like outputs.</p>"},{"location":"pretraining/training_advanced/final_walkthrough/#cleanup","title":"Cleanup","text":"<pre><code>if ddp:\n    destroy_process_group()\n</code></pre> <p>Final step:</p> <ul> <li>Properly shuts down distributed process group  </li> <li>Ensures clean exit and resource release  </li> </ul>"},{"location":"pretraining/training_advanced/gradient_accumulation/","title":"Gradient Accumulation","text":"<p>Modern large language models need to be trained with very large effective batch sizes \u2014 often hundreds of thousands of tokens per update minimum. Large batches provide a better statistical approximation of the \"true gradient\" (the gradient computed over the entire dataset), which leads to more stabilized training. This is particularly important for LLM training where the signal-to-noise ratio in gradients can be very low with small batches.</p> <p>However, there's a fundamental hardware constraint: GPU memory limitations.</p>"},{"location":"pretraining/training_advanced/gradient_accumulation/#the-memory-bottleneck-why-we-cant-just-increase-batch-size","title":"The Memory Bottleneck: Why We Can't Just Increase Batch Size","text":"<p>When training neural networks, GPU memory is consumed by several components:</p> <ol> <li>Model Parameters: The weights and biases of the model (e.g., 1.3B parameters \u2248 2.6GB in FP16)</li> <li>Optimizer States: Additional values like momentum buffers (e.g., Adam adds ~2x model size)</li> <li>Gradients: Gradients for each parameter (same size as parameters)</li> <li>Activations: Intermediate results from forward pass needed for backward pass</li> </ol> <p>The critical insight is that activations scale linearly with batch size and sequence length. When you double the batch size, you approximately double the memory needed for activations during the forward and backward passes.</p> <p>Let's examine a concrete example:</p> <ul> <li>Sequence length (<code>seq_len</code>) = 2048 tokens  </li> <li>Per-GPU batch size = 4 sequences  </li> <li>Hidden dimension = 2048 (value used in this project)</li> </ul> <p>Each forward/backward pass processes: $$ \\text{tokens per batch} = 4 \\times 2048 = 8,192 \\text{ tokens} $$</p> <p>The memory required for the model/optimizer parameters and gradients alone is almost 10.5GB, not to mention the activations which dominates the total memory usage. If we tried to increase this to the desired effective batch size of ~524,288 tokens, we'd need:</p> \\[ \\text{Required multiplier} = \\frac{524,288}{8,192} = 64\\times \\text{ more tokens} \\] <p>Attempting to push to 64\u00d7 larger effective batch size would imply an enormous jump in memory demand. In practice, activations already dominate memory usage at modest batch sizes, often consuming several times more memory than parameters and optimizer states combined. Scaling this up directly would push requirements well beyond what any single GPU can handle \u2014 potentially into the hundreds of gigabytes range. This is why gradient accumulation (and activation checkpointing, which will not be covered here) is essential, used to reduce memory requirements.</p>"},{"location":"pretraining/training_advanced/gradient_accumulation/#gradient-accumulation-simulating-large-batches-with-limited-memory","title":"Gradient Accumulation: Simulating Large Batches with Limited Memory","text":"<p>Gradient accumulation solves this problem by breaking the large batch into micro-batches and accumulating gradients across multiple forward/backward passes before performing a single optimizer update.</p> <p>The procedure works as follows:</p> <ol> <li>Forward Pass: Process a small micro-batch through the model</li> <li>Backward Pass: Compute gradients for that micro-batch</li> <li>Accumulate: Add these gradients to a running total (instead of updating weights)</li> <li>Repeat: Process <code>grad_accum_steps</code> micro-batches</li> <li>Update: Perform a single optimizer step using the accumulated gradients</li> <li>Reset: Clear the gradient buffer and repeat</li> </ol> <p>Mathematically, this is equivalent to training with the large batch all at once because:  </p> <ul> <li>The gradient of a sum equals the sum of gradients</li> <li>By properly averaging, we get the same update as if we processed all data simultaneously</li> </ul>"},{"location":"pretraining/training_advanced/gradient_accumulation/#gradient-normalization","title":"Gradient Normalization","text":"<p>Since PyTorch's autograd system accumulates gradients by summation, we need to ensure that after accumulating across multiple micro-batches, the gradients represent the average rather than the sum.</p> <p>The most common approach is to scale the loss accordingly:  </p> <p><pre><code>loss /= grad_accum_steps  # Normalize the loss\nloss.backward()  # Each backward adds gradient/accum_steps\n</code></pre> After <code>grad_accum_steps</code>, the gradients will sum to the correct average.</p>"},{"location":"pretraining/training_advanced/gradient_accumulation/#implementation-in-training-code","title":"Implementation in Training Code","text":"<p>Here's how gradient accumulation is typically implemented in a training loop:</p> <pre><code>for step in range(1, train_iterations+1):\n    x, y = dataset_loader.get_batch()\n\n    with torch.autocast(device_type=\"cuda\" if \"cuda\" in device else \"cpu\",\n                        dtype=torch.bfloat16 if use_amp else torch.float32):\n        pred = model_handle(x)\n        B, T, C = pred.shape\n        loss = criterion(pred.reshape(B * T, C), y.reshape(B * T))\n\n    train_loss_value = loss.item()  # Log before normalization\n    loss /= grad_accum_steps        # Normalize loss for accumulation\n    loss.backward()\n\n    if step % grad_accum_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n</code></pre> <p>Let's break down what happens at each step:</p> <ul> <li>Micro-batch Processing: Each call to <code>get_batch()</code> returns a small batch (e.g., 4 sequences \u00d7 2048 tokens = 8,192 tokens)</li> <li>Loss Computation: The model computes loss on this micro-batch</li> <li>Loss Scaling: We divide the loss by <code>grad_accum_steps</code> to ensure proper averaging</li> <li>Gradient Accumulation: <code>loss.backward()</code> adds the normalized gradients to the buffer</li> <li>Conditional Update: Only every <code>grad_accum_steps</code> iterations do we actually update the weights and reset gradients</li> </ul>"},{"location":"pretraining/training_advanced/gradient_accumulation/#calculating-the-accumulation-steps","title":"Calculating the Accumulation Steps","text":"<p>The number of gradient accumulation steps is determined by your target effective batch size:</p> \\[ \\text{grad_accum_steps} = \\frac{\\text{target tokens per update}}{\\text{tokens per micro-batch}} \\] <p>Where:</p> <ul> <li><code>tokens per micro-batch =</code> (batch size \u00d7 sequence length)</li> <li><code>target tokens per update =</code> your desired effective batch size</li> </ul> <p>For example:</p> <ul> <li><code>B = 4</code></li> <li><code>T = 2048</code></li> <li><code>tokens_per_batch = 8192</code></li> <li><code>Desired tokens_per_update = 524,288</code></li> </ul> <p>The calculation would be:</p> \\[ \\text{grad_accum_steps} = \\frac{524,288}{8,192} = 64 \\] <p>This means we accumulate gradients over 64 micro-batches before each optimizer step, effectively training with 524,288 tokens per update while only ever storing 8,192 tokens in memory at once.</p>"},{"location":"pretraining/training_advanced/gradient_accumulation/#practical-considerations","title":"Practical Considerations","text":""},{"location":"pretraining/training_advanced/gradient_accumulation/#memory-vs-time-trade-off","title":"Memory vs. Time Trade-off","text":"<p>Gradient accumulation allows you to simulate large batches with limited memory, but it comes with a time cost:</p> <ul> <li>Without accumulation: 1 forward/backward = 1 optimizer step</li> <li>With accumulation: N forward/backward = 1 optimizer step</li> </ul> <p>You're effectively trading memory for computation time.</p>"},{"location":"pretraining/training_advanced/gradient_accumulation/#gradient-behavior","title":"Gradient Behavior","text":"<p>With gradient accumulation, the optimizer sees less frequent but higher-quality gradient estimates. This often allows for:</p> <ul> <li>Higher stable learning rates</li> <li>Smoother convergence curves</li> <li>Better final model performance</li> </ul>"},{"location":"pretraining/training_advanced/gradient_accumulation/#summary","title":"Summary","text":"<p>Gradient accumulation is an essential technique for LLM training that enables:</p> <ul> <li>Large effective batch sizes despite GPU memory constraints</li> <li>Stable training through better gradient estimates</li> <li>Flexible scaling to match the batch sizes required by modern scaling laws</li> </ul> <p>The implementation requires just two simple modifications to the training loop: loss normalization and conditional optimizer steps, but these few lines of code are critical for successful large-scale model training.</p>"},{"location":"pretraining/training_advanced/loss_function/","title":"Loss Function in Language Modeling","text":"<p>The loss function is the core of how we train language models. It tells the model how wrong it was during training, and provides the signal for how to adjust its parameters. In the case of autoregressive language models (like GPT or LLaMA), the standard loss is the negative log-likelihood (NLL), which is equivalent to the cross-entropy loss.</p>"},{"location":"pretraining/training_advanced/loss_function/#the-goal-modeling-the-entire-sequence","title":"The Goal: Modeling the Entire Sequence","text":"<p>When training a language model, we don\u2019t just want it to guess one token correctly when given the entire sentence. That would make it closer to a conditional classifier. Instead, the goal is to train it to generate full sequences, step by step, in an autoregressive manner.</p> <p>Formally, the probability of a sequence of tokens \\(y_1, y_2, \\dots, y_N\\) is defined as the product of conditional probabilities:</p> \\[ P(y_1, y_2, \\dots, y_N) = \\prod_{t=1}^N p(y_t \\mid x_{&lt;t}) \\] <p>Here: - \\(N\\) = number of tokens in the sequence. - \\(y_t\\) = the true token at timestep \\(t\\). - \\(x_{&lt;t}\\) = all the tokens before timestep \\(t\\). - \\(p(y_t \\mid x_{&lt;t})\\) = the probability that the model assigns to the correct token given the context.  </p> <p>This factorization says: to get the joint probability of the full sequence, the model predicts the first token, then the second token conditioned on the first, then the third conditioned on the first two, and so on. This is why the product shows up: it captures the idea of generating the entire sequence autoregressively.</p>"},{"location":"pretraining/training_advanced/loss_function/#why-logs","title":"Why Logs?","text":"<p>The per-token negative log-likelihood is:</p> \\[ \\mathcal{L}_t = -\\log p(y_t \\mid x_{&lt;t}) \\] <p>This tells us how much the model is \u201cpenalized\u201d for its prediction at timestep \\(t\\). But why take logs in the first place? There are two main reasons:</p>"},{"location":"pretraining/training_advanced/loss_function/#1-turning-products-into-sums","title":"1. Turning Products into Sums","text":"<p>The probability of the whole sequence is a product of many terms (each between 0 and 1). For example:</p> \\[ P = p(y_1)p(y_2 \\mid y_1)p(y_3 \\mid y_1,y_2)... \\] <p>This product can become extremely small. Even if each predicted correct token has a very high probability like ~0.8, for a mere 250 tokens, \\(0.8^{100} \\approx 6 * 10^{-25}\\). That\u2019s practically a value of zero.  </p> <p>Taking the log turns the product into a sum:</p> \\[ \\log P(y_1, \\dots, y_N) = \\sum_{t=1}^N \\log p(y_t \\mid x_{&lt;t}) \\] <p>Overall, that's much easier to compute, and sums don\u2019t collapse to near zero the way products do. Since computers can\u2019t represent extremely tiny numbers well. By taking logs, we keep values in a reasonable numeric range. Instead of multiplying a thousand small decimals, we just add up logs, which are often small negative numbers (like -0.9, -1.2).  </p>"},{"location":"pretraining/training_advanced/loss_function/#2-better-gradients","title":"2. Better Gradients","text":"<p>The derivative of \\(\\log f(x)\\) is \\(\\frac{1}{f(x)} f'(x)\\). This plays especially nicely with softmax outputs, leading to a simple, stable gradient for training. In fact, the widely used \u201csoftmax + cross-entropy loss\u201d is implemented in PyTorch as a single efficient function (<code>CrossEntropyLoss</code>) because of this mathematical property.</p>"},{"location":"pretraining/training_advanced/loss_function/#example-walkthrough","title":"Example Walkthrough","text":"<p>Take the sentence:  </p> <p>\u201cThe cat sat on the mat.\u201d </p> <p>Suppose the model\u2019s vocabulary includes tokens like \u201ccat\u201d, \u201cdog\u201d, \u201cmat\u201d, etc. When predicting the final word \u201cmat\u201d, the model outputs probabilities:</p> <ul> <li>\\(p(\\text{cat} \\mid \\text{The cat sat on the}) = 0.01\\)  </li> <li>\\(p(\\text{dog} \\mid \u2026) = 0.03\\)  </li> <li>\\(p(\\text{mat} \\mid \u2026) = 0.30\\)  </li> </ul> <p>Here, the correct token is \u201cmat,\u201d so the probability we care about is 0.30. The loss for this timestep is:</p> \\[ -\\log(0.30) \\approx 1.20 \\] <p>If the model had been less confident, say \\(p(\\text{mat})=0.15\\), then the loss would be:</p> \\[ -\\log(0.15) \\approx 1.90 \\] <p>So the model is assigned a higher loss when it assigns low probability to the correct answer.</p>"},{"location":"pretraining/training_advanced/loss_function/#averaging-over-timesteps","title":"Averaging Over Timesteps","text":"<p>We compute this log-loss for every token in the sequence and average:</p> \\[ \\mathcal{L} = -\\frac{1}{N}\\sum_{t=1}^N \\log p(y_t \\mid x_{&lt;t}) \\] <p>This way, the loss reflects how well the model predicts the entire sequence, not just one token. Averaging keeps the loss value on a consistent scale, regardless of sequence length.</p>"},{"location":"pretraining/training_advanced/loss_function/#connecting-to-the-training-code","title":"Connecting to the Training Code","text":"<p>It\u2019s useful to see how the theoretical loss we\u2019ve described translates into the actual training loop. Consider the following code snippet:</p> <pre><code>x, y = dataset_loader.get_batch()\n\nwith torch.autocast(device_type=\"cuda\" if \"cuda\" in device else \"cpu\",\n                    dtype=torch.bfloat16 if use_amp else torch.float32):\n    pred = model_handle(x)         # Forward pass\n    B, T, C = pred.shape           # (Batch, Time, Vocabulary size)\n    loss = criterion(pred.reshape(B * T, C), y.reshape(B * T))\n</code></pre> <p>Let\u2019s break this down:</p> <ol> <li> <p>Batch Data </p> <ul> <li><code>x</code>: input tokens of shape <code>(B, T)</code> where <code>B</code> = batch size and <code>T</code> = sequence length.  </li> <li><code>y</code>: target tokens of shape <code>(B, T)</code> \u2014 essentially the same sequence but shifted by one token (next-token prediction).  </li> </ul> </li> <li> <p>Model Output </p> <ul> <li><code>pred = model_handle(x)</code> gives the raw logits (unnormalized scores) for every token in the vocabulary at each position in the sequence.  </li> <li>Shape: <code>(B, T, C)</code> where <code>C</code> = vocabulary size.  </li> </ul> <p>Example: if <code>B=2</code>, <code>T=4</code>, <code>C=50,000</code>, then <code>pred</code> has predictions for every position in both sequences, across the full vocabulary.</p> </li> <li> <p>Reshaping for the Loss </p> <p>PyTorch\u2019s <code>CrossEntropyLoss</code> expects inputs of shape <code>(N, C)</code> and targets of shape <code>(N,)</code>, where <code>N</code> is the number of training examples.  </p> <ul> <li><code>pred.reshape(B*T, C)</code> flattens the batch and sequence dimensions, so we now have <code>N = B*T</code> predictions, each over the vocabulary.  </li> <li><code>y.reshape(B*T)</code> flattens the targets into a single vector of token IDs.  </li> </ul> <p>In other words, each token in the batch is treated as a separate training example.</p> </li> <li> <p>Loss Calculation </p> <ul> <li><code>criterion</code> (referring to <code>torch.nn.CrossEntropyLoss</code> here) internally applies a log-softmax to <code>pred</code> and computes:  </li> </ul> <p>$$  \\mathcal{L} = - \\frac{1}{B \\cdot T}\\sum_{i=1}^{B \\cdot T} \\log p(y_i \\mid x_{&lt;i})  $$</p> <ul> <li>This is exactly the negative log-likelihood we\u2019ve been discussing, but implemented efficiently.  </li> </ul> </li> </ol>"},{"location":"pretraining/training_advanced/loss_function/#summary","title":"Summary","text":"<ul> <li>We want to train models to generate entire sequences, not just single tokens. This leads to the product of conditional probabilities.  </li> <li>Taking logs converts the product into a sum, avoids numerical underflow, and gives clean gradients.  </li> <li>The negative average log-likelihood (cross-entropy loss) is the standard loss function for training LLMs.  </li> </ul>"},{"location":"pretraining/training_advanced/optimizers/","title":"Optimizers: From SGD to AdamW","text":"<p>Note: This section covers four important optimizers used for updating model weights via backpropagation: SGD, SGD with Momentum, Adam, and AdamW. While other optimizers like AdaGrad and RMSProp exist, they fall outside the scope of this guide.</p> <p>This section is heavy in mathematical derivation and can be skipped without impacting further sections. It assumes the reader has a fair understanding of backpropagation and gradient computation. If not, the videos DL Chapter 3 and DL Chapter 4 by 3Blue1Brown are highly recommended.</p>"},{"location":"pretraining/training_advanced/optimizers/#sgd-the-fundamental-optimizer","title":"SGD: The Fundamental Optimizer","text":"<p>Stochastic Gradient Descent (SGD) is the most basic and historically important optimizer, forming the foundation for more advanced variants.</p> <p>The parameter update rule is:</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_\\theta L(\\theta_t) \\] <p>Key Components: </p> <ul> <li>\\(\\theta_{t+1}\\): The updated parameter.</li> <li>\\(\\theta_t\\): The current parameter.</li> <li>\\(\\eta\\): The learning rate.</li> <li>\\(\\nabla_\\theta L(\\theta_t)\\): The gradient of the loss function with respect to the parameter \\(\\theta\\) at timestep \\(t\\).</li> </ul> <p>The update is performed by subtracting the product of the learning rate and the gradient from the current parameter value. This nudges the parameter value towards a (local) minimum. Over many iterations, this process converges to a final set of parameters for the model. It is simple, yet effective.</p>"},{"location":"pretraining/training_advanced/optimizers/#sgd-with-momentum","title":"SGD With Momentum","text":"<p>SGD with Momentum is a variant designed to accelerate convergence and reduce oscillations (the \"zigzag\" behavior) often seen in vanilla SGD.</p> <p>It introduces a velocity term, which is a running average of past gradients.</p>"},{"location":"pretraining/training_advanced/optimizers/#velocity-update","title":"Velocity Update","text":"\\[ v_t = \\beta v_{t-1} + \\nabla_\\theta L(\\theta_t) \\]"},{"location":"pretraining/training_advanced/optimizers/#parameter-update","title":"Parameter Update","text":"\\[ \\theta_{t+1} = \\theta_t - \\eta v_t \\] <p>We introduce a new velocity term, \\(v_t\\), which accumulates the gradient direction. This velocity is composed of the current batch's gradient plus a fraction of the previous timestep's velocity (\\(\\beta v_{t-1}\\)). The hyperparameter \\(\\beta\\) is typically set to a value like 0.9.</p> <p>The learning rate \\(\\eta\\) then scales the entire velocity term in the final parameter update. This separation makes the roles clear: the velocity determines the direction and the learning rate determines the step size.</p> <p>The purpose is to 'smooth out' the path taken during descent. The velocity term acts as a weighted sum of past gradients, giving inertia to the optimization process. If recent gradients point in a consistent direction, the velocity builds up, leading to faster progress.</p> <p>Let's trace the first few steps, starting at \\(t=1\\) (assuming \\(v_0 = 0\\)):  </p> <ul> <li>\\(t=1\\): \\(v_1 = \\beta \\cdot 0 + g_1 = g_1\\) \u2192 \\(\\theta_2 = \\theta_1 - \\eta g_1\\)</li> <li>\\(t=2\\): \\(v_2 = \\beta v_1 + g_2 = \\beta g_1 + g_2\\) \u2192 \\(\\theta_3 = \\theta_2 - \\eta (\\beta g_1 + g_2)\\)</li> <li>\\(t=3\\): \\(v_3 = \\beta v_2 + g_3 = \\beta^2 g_1 + \\beta g_2 + g_3\\) \u2192 \\(\\theta_4 = \\theta_3 - \\eta (\\beta^2 g_1 + \\beta g_2 + g_3)\\)</li> </ul> <p>As this process continues, the influence of a gradient from step \\(t-i\\) is scaled by \\(\\beta^i\\). With \\(\\beta=0.9\\), the optimizer \"remembers\" the direction of the last ~30 steps. This leads to a smoother, faster path towards the minimum.</p> <p>The general form of the velocity at step \\(t\\) can be expressed as: $$ v_t = \\sum_{i=1}^{t} \\beta^{t-i} \\nabla_\\theta L(\\theta_i) $$</p>"},{"location":"pretraining/training_advanced/optimizers/#adam-optimizer","title":"Adam Optimizer","text":"<p>The Adam (Adaptive Moment Estimation) optimizer, introduced in 2015, builds upon SGD with Momentum by incorporating an adaptive learning rate for each parameter. It uses estimates of both the first moment (the mean of gradients, like momentum) and the second moment (the uncentered variance of gradients) of the gradients.</p> <p>Adam introduces a third hyperparameter, \\(\\beta_2\\), which controls the decay rate for the second moment (\\(\\beta\\) in SGD With Momentum is now referred to as \\(\\beta_1\\)). Let \\(g_t\\) be the shorthand for \\(\\nabla_\\theta L(\\theta_t)\\).</p> <p>The parameter update rule is:</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon} \\] <p>We update the parameter by subtracting the bias-corrected first moment (\\(\\hat m_t\\)), normalized by the square root of the bias-corrected second moment (\\(\\sqrt{\\hat v_t}\\)), and scaled by the learning rate \\(\\eta\\). A small constant \\(\\epsilon\\) (e.g., \\(10^{-8}\\)) is added to prevent division by zero.</p>"},{"location":"pretraining/training_advanced/optimizers/#first-moment-momentum","title":"First Moment (Momentum)","text":"\\[ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\] <p>This is similar to the velocity in SGD with Momentum. The key difference is the use of \\((1-\\beta_1)\\) as a scaling factor for the current gradient. This design, along with the subsequent bias correction, ensures that the expected value of \\(m_t\\) is approximately equal to the expected value of the gradients \\(E[g_t]\\). The purpose is identical: to smooth the gradient signal by incorporating past information.</p>"},{"location":"pretraining/training_advanced/optimizers/#second-moment-variance","title":"Second Moment (Variance)","text":"\\[ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 \\] <p>This calculation is analogous to the first moment but uses a different hyperparameter, \\(\\beta_2\\) (typically set to 0.999), and operates on the squared gradients. The squared value provides a measure of the magnitude of the gradients, irrespective of their sign. A large \\(v_t\\) indicates that the parameter has historically had large gradients. This term is used to adaptively scale the learning rate down for such parameters.</p>"},{"location":"pretraining/training_advanced/optimizers/#bias-correction","title":"Bias Correction","text":"<p>A critical step in Adam is bias correction. At the beginning of training (low \\(t\\)), the moving averages \\(m_t\\) and \\(v_t\\) are initialized to zero and are therefore biased towards zero. Bias correction compensates for this initial bias.</p> \\[ \\hat m_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat v_t = \\frac{v_t}{1 - \\beta_2^t} \\] <p>Let's see why this is necessary at \\(t=1\\):  </p> <ul> <li>\\(m_1 = \\beta_1 \\cdot 0 + (1-\\beta_1) g_1 = (1-0.9) g_1 = 0.1 \\cdot g_1\\). This is much smaller than the true gradient \\(g_1\\).</li> <li>\\(\\hat m_1 = \\frac{m_1}{1 - 0.9^1} = \\frac{m_1}{0.1} = g_1\\). The bias correction scales \\(m_1\\) back up to the correct magnitude.</li> </ul> <p>As \\(t\\) increases, \\(\\beta^t\\) approaches zero, and the denominator \\(1-\\beta^t\\) approaches 1, making the bias correction factor negligible. This is why Adam is called an adaptive learning rate optimizer: the effective step size for each parameter is \\(\\eta / \\sqrt{\\hat v_t}\\), which adjusts based on the historical magnitude of the parameter's gradients.</p>"},{"location":"pretraining/training_advanced/optimizers/#adamw-optimizer","title":"AdamW Optimizer","text":"<p>AdamW is a refined version of Adam that is arguably the most widely used optimizer today. The key difference lies in how it handles Weight Decay.</p>"},{"location":"pretraining/training_advanced/optimizers/#the-problem-with-l2-regularization-in-adam","title":"The Problem with L2 Regularization in Adam","text":"<p>Traditionally, L2 regularization is added directly to the loss function: $$ L_{total}(\\theta) = L_{original}(\\theta) + R(\\theta) = L_{original}(\\theta) + \\frac{\\lambda}{2} |\\theta|^2 $$</p> <p>The gradient of the regularization term is \\(\\nabla_\\theta R(\\theta) = \\lambda \\theta\\). Therefore, the gradient used for the update in standard Adam becomes: $$ \\nabla L_{total} = \\nabla L_{original}(\\theta_t) + \\lambda \\theta_t $$</p> <p>This intertwining of weight decay with the adaptive gradient mechanism is suboptimal. Because Adam adapts the learning rate per parameter based on \\(v_t\\), the effective strength of the weight decay becomes coupled with the history of the gradients. This coupling can prevent weight decay from acting as a consistent regularizer, often making the hyperparameter \\(\\lambda\\) harder to tune and leading to worse generalization.</p>"},{"location":"pretraining/training_advanced/optimizers/#the-solution-decoupled-weight-decay","title":"The Solution: Decoupled Weight Decay","text":"<p>AdamW decouples the weight decay term from the adaptive gradient update. Instead of adding it to the loss, weight decay is applied directly to the weights after the adaptive update.</p> <p>The parameter update rule for AdamW is:</p> \\[ \\theta_{t+1} = \\theta_t - \\eta \\left( \\frac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon} \\right) - \\eta \\lambda \\theta_t \\] <p>Key Difference: The term \\(- \\eta \\lambda \\theta_t\\) is now separate. It is applied directly to the weights as a straightforward weight decay, independent of the adaptive scaling factor \\(1/(\\sqrt{\\hat v_t} + \\epsilon)\\).</p> <p>This decoupling ensures that the weight decay is applied with a strength determined solely by the product \\(\\eta \\lambda\\), making it a true regularization term that acts consistently on the weights. This approach has been shown to improve generalization performance and makes the tuning of the weight decay hyperparameter \\(\\lambda\\) more reliable and consistent across different models and datasets. AdamW is often the default choice for training modern deep neural networks, particularly Transformers.</p>"},{"location":"pretraining/training_advanced/optimizers/#optimizer-summary-table","title":"Optimizer Summary Table","text":"Optimizer Update Rule (simplified) Extra Hyperparams Intuition Pros Cons SGD \\(\\theta_{t+1} = \\theta_t - \\eta g_t\\) \u2013 Take a step opposite the gradient. Simple, fast, widely understood. Can be very noisy, slow convergence. SGD + Momentum \\(v_t = \\beta v_{t-1} + g_t\\) \\(\\theta_{t+1} = \\theta_t - \\eta v_t\\) \\(\\beta\\) (momentum) Averages past gradients to smooth updates (\u201cball rolling downhill\u201d). Faster convergence, reduces zigzagging. Still uses global LR, sensitive to hyperparam tuning. Adam \\(m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\\) \\(v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\) \\(\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon}\\) \\(\\beta_1\\), \\(\\beta_2\\), \\(\\epsilon\\) Combines momentum (trend of gradients) with variance scaling (stability). Adaptive per-parameter learning rate; works well out of the box. Can overfit, sometimes worse generalization than SGD. AdamW \\(\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon} - \\eta \\lambda \\theta_t\\) \\(\\beta_1\\), \\(\\beta_2\\), \\(\\epsilon\\), \\(\\lambda\\) Same as Adam, but with decoupled weight decay. Strong generalization; default choice for Transformers and large-scale models. More compute/memory than SGD; still sensitive to hyperparams."},{"location":"pretraining/training_advanced/recap/","title":"Basics Recap","text":""},{"location":"pretraining/training_advanced/recap/#recap-of-the-basics","title":"Recap of the Basics","text":"<p>Before we move on to advanced topics such as distributed training, large-scale data pipelines, and mixed precision optimization, it is worth grounding ourselves again in the fundamentals of pretraining a language model. Pretraining refers to the stage in which a model is exposed to massive amounts of raw text in order to learn the statistical structure of language. Instead of being taught one specific task, the model is trained in a self-supervised fashion: given a sequence of words or tokens, it learns to predict the next one. This deceptively simple objective forces the network to acquire knowledge of grammar, semantics, style, and even common patterns of reasoning, all of which become useful later when the model is adapted to narrower applications.  </p>"},{"location":"pretraining/training_advanced/recap/#data-tokens-and-the-training-loop","title":"Data, Tokens, and the Training Loop","text":"<p>The process begins with data. The dataset is typically an enormous text corpus, drawn from sources such as books, articles, code, or web content. Since raw text cannot be directly processed by neural networks, it is first broken down into tokens, which are usually subword units chosen to balance efficiency and coverage. Each training example consists of a sequence of these tokens as input, and the model\u2019s goal is to generate the next token in the sequence. For example:  </p> <ul> <li>Input: <code>The cat sat on the</code> </li> <li>Target Output: <code>mat</code> </li> </ul> <p>Over billions of such examples, the model gradually learns the probability distribution of words and phrases in natural language.  </p> <p>The training loop that drives this process is conceptually straightforward, though computationally demanding:  </p> <ul> <li>Take a batch of token sequences from the dataset.  </li> <li>Feed them through the model to produce predictions.  </li> <li>Compare predictions against the actual next tokens using cross-entropy loss.  </li> <li>Propagate the error backward and update the weights.  </li> </ul> <p>This cycle repeats many times, gradually refining the model\u2019s parameters.  </p>"},{"location":"pretraining/training_advanced/recap/#loss-perplexity-and-why-it-matters","title":"Loss, Perplexity, and Why It Matters","text":"<p>The loss function provides the primary training signal. Cross-entropy loss measures how far off the model\u2019s predicted probability distribution is from the true one. From this, we also derive perplexity, which is easier to interpret: it represents how \u201csurprised\u201d the model is by the data. A lower perplexity means the model has become better at anticipating what comes next. (More about perplexity will be described further below)</p> <p>Understanding these basics is important because the same principles apply whether we are training a toy model on a laptop or a frontier-scale model on a GPU cluster.  In the beginner guide, we showed that even a very small network can capture recognizable patterns in English with a simple dataset.  In this more advanced guide, we\u2019ll extend those ideas to the real-world scale: billions of tokens, multi-GPU setups, and the optimizations needed to make it all possible.</p>"},{"location":"pretraining/training_advanced/throughput_optimizations/","title":"Throughput Optimizations","text":"<p>Achieving high training throughput is crucial for practical LLM development, and this section outlines key optimizations in the training script that deliver noticeable speedups.</p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#1-bf16-vs-fp32fp16-the-precision-trade-off","title":"1. BF16 vs FP32/FP16: The Precision Trade-off","text":""},{"location":"pretraining/training_advanced/throughput_optimizations/#understanding-numerical-precision","title":"Understanding Numerical Precision","text":"<ul> <li> <p>FP32: 32-bit floating point (standard float)  </p> <ul> <li>Range: \u00b11.7\u00d710\u00b3\u2078, Precision: ~7 decimal digits  </li> <li>High precision, large memory footprint  </li> </ul> </li> <li> <p>FP16: 16-bit floating point (half precision)  </p> <ul> <li>Range: \u00b165,504, Precision: ~4 decimal digits  </li> <li>Small memory, but limited range can cause overflow/underflow  </li> </ul> </li> <li> <p>BF16: Brain Floating Point (Google's solution)  </p> <ul> <li>Range: \u00b13.4\u00d710\u00b3\u2078 (same exponent as FP32, much larger than FP16)  </li> <li>Precision: ~3 decimal digits (7 mantissa bits, less than FP16)</li> <li>Ideal for deep learning: preserves range, sacrifices precision  </li> </ul> </li> </ul>"},{"location":"pretraining/training_advanced/throughput_optimizations/#why-bf16-wins-for-llm-training","title":"Why BF16 Wins for LLM Training","text":"<pre><code># BF16 has the dynamic range of FP32 with the memory footprint of FP16\nfp32_tensor = torch.randn(1000, 1000, dtype=torch.float32)  # 4MB\nbf16_tensor = torch.randn(1000, 1000, dtype=torch.bfloat16)  # 2MB\n\nprint(f\"FP32 range: ~10^38, BF16 range: ~10^38 (same!)\")\nprint(f\"FP16 range: ~10^4 (much smaller - risk of overflow)\")\n</code></pre> <p>The key is that LLM training is more sensitive to range than precision. Gradient values can span many orders of magnitude, making BF16's preserved range crucial.</p> <p>Do note that although this would reduce memory usage substantially, but would not directly halve the memory usage, a few layers/activations in the model are kept in FP32 due to precision sensitivity, along with optimizer state dicts which stays in FP32. </p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#2-automatic-mixed-precision-amp-with-autocast","title":"2. Automatic Mixed Precision (AMP) with Autocast","text":"<p>As mentioned, not all operations benefit from lower precision. Some need FP32 for numerical stability:</p> <ul> <li>BF16-friendly: Heavy matrix multiplications, elementwise operations, gradients</li> <li>FP32-required: Reductions, loss computation, certain activations that requires numeric precision  </li> </ul> <p>PyTorch handles this by providing <code>torch.autocast</code> to automatically manage precision:</p> <pre><code>use_amp = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\nwith torch.autocast(device_type=\"cuda\" if \"cuda\" in device else \"cpu\",\n                   dtype=torch.bfloat16 if use_amp else torch.float32):\n    # Everything in this context uses mixed precision\n    pred = model_handle(x)  # Most ops in BF16\n    B, T, C = pred.shape\n    loss = criterion(pred.reshape(B * T, C), y.reshape(B * T))  # Loss in FP32\n</code></pre>"},{"location":"pretraining/training_advanced/throughput_optimizations/#how-autocast-works","title":"How Autocast Works","text":"<ul> <li>Operator-Level Precision Rules: PyTorch has built-in rules for each operation type  </li> <li>Automatic Casting: Inputs are cast to appropriate precision  </li> <li>Output Casting: Results are cast back to expected types  </li> </ul> <p>Overall, this results not only in memory usage reduction, operations in BF16 is much faster compared to in FP32 on modern tensor cores. </p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#3-torchcompile-graph-level-optimizations","title":"3. Torch.Compile: Graph-Level Optimizations","text":""},{"location":"pretraining/training_advanced/throughput_optimizations/#from-eager-mode-to-graph-mode","title":"From Eager Mode to Graph Mode","text":"<p>PyTorch normally runs in eager mode: each operation executes immediately. This is flexible but inefficient due to Python overhead.</p> <p><code>torch.compile</code> converts your model to a graph that can be optimized:</p> <pre><code># Before compilation (eager mode)\nif enable_compilation:\n    model_handle = torch.compile(model)\n\n# After compilation - same API, much faster execution\npred = model_handle(x)  # Now runs as optimized graph\n</code></pre>"},{"location":"pretraining/training_advanced/throughput_optimizations/#what-compilation-optimizes","title":"What Compilation Optimizes","text":"<ul> <li>Kernel Fusion: Combine multiple operations into single kernels  </li> <li>Memory Layout Optimization: Optimize tensor memory access patterns  </li> <li>Dead Code Elimination: Remove unnecessary operations  </li> <li>Constant Propagation: Precompute constant expressions  </li> </ul>"},{"location":"pretraining/training_advanced/throughput_optimizations/#real-world-example-attention-mechanism","title":"Real-World Example: Attention Mechanism","text":"<pre><code># Eager mode: many small operations\ndef attention_eager(Q, K, V):\n    scores = torch.matmul(Q, K.transpose(-2, -1))  # Separate kernel\n    scores = scores / math.sqrt(Q.size(-1))        # Separate kernel  \n    attn = torch.softmax(scores, dim=-1)           # Separate kernel\n    output = torch.matmul(attn, V)                 # Separate kernel\n    return output\n</code></pre> <p>Compiled: potentially fused into fewer kernels.  </p> <p>The compiler can merge these operations for better memory locality.</p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#4-fused-optimizer-kernels","title":"4. Fused Optimizer Kernels","text":"<p>Similar to how <code>torch.compile</code> offers kernel fusion operation in the model, optimizers can also provide fusion operations to more efficiently compute and update model parameters. </p> <p>In a more naive implementation pytorch may launch a lot of small kernels which would be relatively inefficient with overhead costs, coming from many small GPU operations.</p> <p>Here we can inspect if the optimizer supports fused operations, and pass that kwarg during optimizer instantiation to combine multiple operations into single kernels:</p> <pre><code># Check for fused optimizer support\nfused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\nuse_fused = fused_available and (device == \"cuda\" or ddp)\nextra_args = dict(fused=True) if use_fused else dict()\n\noptimizer = torch.optim.AdamW(\n    model.parameters(), \n    lr=max_lr, \n    betas=(beta1, beta2), \n    weight_decay=weight_decay, \n    **extra_args  # Uses fused kernels if available\n)\n</code></pre> <p>In general, there would be some speed up optimizations, though that would be dependent upon the optimizer and operations within the model.  It reduces memory bandwidth usage and is especially beneficial for models with many small parameters  .  The speedup varies by hardware and model size, but it\u2019s typically a \u201cfree\u201d optimization worth enabling when available.</p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#5-flashattention","title":"5. FlashAttention","text":"<p>One of the biggest computational and memory bottlenecks in Transformers comes from the attention mechanism. Standard attention computes a large score matrix of shape <code>(B, n_heads, T, T)</code>. </p> <p>Imagine we train a language model using a sequence length, <code>T</code>, with a moderate value like 32k, batch size of 4, and 32 heads. A single matrix of that shape using BF16 would require over 250GB of memory alone! Granted, that's the most naive implementation, but for long sequences, this quadratic cost in both time and memory quickly dominates training, due to it's quadratic nature.</p> <p>FlashAttention is a memory-efficient attention algorithm that reorders the way attention is computed so that the large intermediate matrix never needs to be materialized in GPU memory. Instead, it streams blocks of the computation through high-bandwidth GPU SRAM, drastically reducing memory usage and speeding up execution. Overall computation requirement remains quadratic, however the memory requirement is now linear with respect to sequence length. </p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#standard-attention","title":"Standard Attention","text":"<ol> <li>Compute scores: <code>QK^T / sqrt(d_head)</code> \u2192 shape <code>(B, n_heads, T, T)</code> </li> <li>Apply softmax \u2192 still <code>(B, n_heads, T, T)</code> </li> <li>Multiply by <code>V</code> </li> </ol> <p>Both the score matrix and the softmax output is stored for backpropagation, which can be quite large, depending on the sequence length.</p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#flashattention-approach","title":"FlashAttention Approach","text":"<ul> <li>The key trick is to compute attention block by block directly in GPU SRAM, never materializing the full <code>(T, T)</code> matrix.  </li> <li>Uses a numerically stable online softmax to handle blocks sequentially.  </li> <li>Backward pass recomputes pieces instead of storing them, further saving memory.</li> </ul> <p>The benefits is that this approach reduces activation memory from O(T\u00b2) to O(T), allowing much longer context lengths without blowing up VRAM and often results in 2\u20134\u00d7 faster computation for attention at long sequences.  </p> <p>It's important to recognize that this is just a very high level overview of what it does. Flash Attention is quite complex and explaining the details would fall outside the scope of this project.  If interested, here's the link to read more about this: Flash Attention Paper</p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#example-usage","title":"Example Usage","text":"<p>In this project, Flash Attention is applied as: </p> <pre><code># (batch, seq_len, n_heads, h_dim) -&gt; (batch, n_heads, seq_len, h_dim)\nq = q.permute(0, 2, 1, 3)\nk = k.permute(0, 2, 1, 3)\nv = v.permute(0, 2, 1, 3)\n\nif self.use_flash_attention:\n    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\nelse:\n    # (batch, n_heads, seq_len, h_dim) @ (batch, n_heads, h_dim, seq_len) -&gt; (batch, n_heads, seq_len, seq_len)\n    y = q @ k.transpose(-2, -1) / (self.h_dim ** 0.5)\n    mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=self.device))\n    y = y.masked_fill(~mask, float(\"-inf\"))\n    y = F.softmax(y, dim=-1)\n\n    # (batch, n_heads, seq_len, seq_len) @ (batch, n_heads, seq_len, h_dim) -&gt; (batch, n_heads, seq_len, h_dim)\n    y = y @ v\n</code></pre> <p>Given the Q, K, V matrices of shape <code>(batch, n_heads, seq_len, h_dim)</code>, we pass it to <code>nn.functional.scaled_dot_product_attention</code>, telling it that this uses causal masking, and would use flash attention to compute the result. </p> <p>Whether or not it's used (depending if compatible with torch version/gpu device) is set in the configuration, which is then passed to the model during instantiation. </p>"},{"location":"pretraining/training_advanced/throughput_optimizations/#summary","title":"Summary","text":"<p>Throughput optimizations transform LLM training from impractical to feasible. This implementation demonstrates common approaches used in development:</p> <ul> <li>Layered optimizations that compound benefits  </li> <li>Automatic precision management with AMP  </li> <li>Graph-level optimizations via torch.compile  </li> <li>Hardware-aware kernels for maximum performance  </li> </ul> <p>While each provides individual benefits, these combination of optimizations enables the scale of modern LLM training.</p>"},{"location":"pretraining/training_beginner/dataset_and_batching/","title":"Dataset and Batching","text":"<p>In the introduction, we explained that the goal of training is for the model to learn how to predict the next token in a sequence. But how do we actually present and organize this information to the model during training? </p> <p>This is where the Dataset Loader comes in. Its job is to take the large tokenized dataset stored on disk and feed the model with manageable \u201cmini-batches\u201d of tokens at every training step. Without this loader, we would have no practical way to handle billions of tokens, because we cannot load everything into memory or train on an endless stream of raw text.  </p> <p>When training a language model, we usually start with a massive corpus of text \u2014 sometimes hundreds of gigabytes. This raw text has already been tokenized and stored in NumPy arrays for efficiency. These files are then fed into the Dataset Loader.</p> <p>If you tried to feed the entire dataset into the model in one go, three things would immediately go wrong:  </p> <ol> <li>The model would run out of memory, because GPUs cannot hold billions of tokens at once.  </li> <li>Training would be extremely inefficient, since we want to update weights frequently rather than waiting for one giant pass.  </li> <li>We would lose the ability to shuffle, divide across GPUs, or checkpoint easily.  </li> </ol> <p>The Dataset Loader solves all of these problems by breaking the token stream into smaller, more manageable pieces. At each step, it delivers a batch of sequences \u2014 small slices of the dataset that the model can process in parallel.</p>"},{"location":"pretraining/training_beginner/dataset_and_batching/#code-example-with-toy-data","title":"Code Example with Toy Data","text":"<p>Here is a small code example using the <code>DatasetLoader</code> class:</p> <pre><code>from simple_llama.pretraining.dataset_loader import DatasetLoader\n\n# Small example\nloader = DatasetLoader(\n    batch=2,           # Number of sequences in a batch\n    seq_len=4,         # Number of tokens per sequence\n    process_rank=0,    # Single-process case\n    num_processes=1,\n    dataset_dir=\"data\", \n    device=\"cpu\"\n)\n\nx, y = loader.get_batch()\nprint(\"x:\", x)\nprint(\"y:\", y)\n</code></pre> <p>Example output (toy data):</p> <pre><code>x: tensor([[1202,  850,  149, 4211],\n           [ 769, 1839, 3521, 4879]])\ny: tensor([[ 850,  149, 4211,  769],\n           [1839, 3521, 4879, 2035]])\n</code></pre>"},{"location":"pretraining/training_beginner/dataset_and_batching/#the-structure-of-x-y","title":"The Structure of <code>(x, y)</code>","text":"<p>Each batch returned by the loader consists of two tensors:  </p> <ul> <li><code>x</code>: The input sequences of tokens.  </li> <li><code>y</code>: The same sequences, shifted one position to the right.  </li> </ul> <p>This shifting mechanism is what allows the model to learn \u201cnext token prediction.\u201d  </p> <p>Here's another example. Suppose the dataset contains a chunk the following six tokens:  </p> <pre><code>Tokens: [1202, 850, 149, 4211, 769, 1839]\n</code></pre> <p>If we set <code>batch = 1</code> and <code>seq_len = 5</code>, then the loader will slice the data like this:  </p> <pre><code>x = [[1202, 850, 149, 4211, 769]]\ny = [[ 850, 149, 4211,  769, 1839]]\n</code></pre> <p>At first glance, this looks like we are simply training a bigram model \u2014 for every token in <code>x</code>, we just predict the token in the same position in <code>y</code>. But that\u2019s not really what is happening inside the transformer. The important detail is that the model doesn\u2019t just see the token at position t and try to guess token t+1. Instead, it sees the entire sequence up to position t, and from that context, it tries to guess the next token.</p> <p>So in this case, the training targets look more like this:  </p> <ul> <li>Given <code>[1202]</code>, predict <code>850</code>.  </li> <li>Given <code>[1202, 850]</code>, predict <code>149</code>.  </li> <li>Given <code>[1202, 850, 149]</code>, predict <code>4211</code>.  </li> <li>Given <code>[1202, 850, 149, 4211]</code>, predict <code>769</code>.  </li> <li>Given <code>[1202, 850, 149, 4211, 769]</code>, predict <code>1839</code>.  </li> </ul> <p>Notice the subtle difference. A bigram model would only look at one previous token at a time, while the transformer looks at the entire history of the sequence and uses self-attention to weigh the importance of different past tokens when predicting the next one. This is what allows it to capture long-range dependencies in language, like subject\u2013verb agreement across many words.</p>"},{"location":"pretraining/training_beginner/dataset_and_batching/#why-batching-matters","title":"Why Batching Matters","text":"<p>The idea of batching deserves attention as well. If we only trained on one sequence at a time, the model would make progress, but it would be extremely slow and would not take full advantage of the GPU. By grouping multiple sequences together into a batch, we can exploit the GPU\u2019s ability to perform large matrix multiplications efficiently.</p> <p>Suppose we use:  </p> <ul> <li><code>batch = 32</code> </li> <li><code>seq_len = 2048</code> </li> </ul> <p>In that case, the model processes 65,536 tokens at every step. This is a large increase in efficiency compared to processing a single sequence, which would otherwise incur additional overhead in each forward/backward pass. This batching strategy is one of the main reasons why modern transformers can be trained at such large scales. It allows us to feed in huge amounts of data per optimization step, stabilize the gradients, and make much faster progress than would otherwise be possible.</p> <p>The Dataset Loader is therefore the bridge between the dataset on disk and the mini-batches that the model actually learns from. It provides structure to the training process, ensuring that at every step, the model sees just enough data to make a meaningful update \u2014 and then moves on to the next batch.</p>"},{"location":"pretraining/training_beginner/dataset_and_batching/#inside-the-dataset-loader-how-it-works","title":"Inside the Dataset Loader: How It Works","text":"<p>When you create a <code>DatasetLoader</code>, you pass in the batch size, sequence length, dataset directory, and a few distributed training arguments:</p> <pre><code>class DatasetLoader:\n    def __init__(self, batch: int, seq_len: int, process_rank: int, num_processes: int, dataset_dir: str, device: str):\n        \"\"\"\n        :param batch: Batch size\n        :param seq_len: Max seq len\n        :param process_rank: Rank of the process that initializes an instance of this class\n        :param num_processes: Total number of processes (World Size)\n        :param dataset_dir: Dataset directory\n        :param device: \"cuda\" or \"cpu\"\n        \"\"\"\n\n        self.batch = batch\n        self.seq_len = seq_len\n        self.process_rank = process_rank\n        self.num_processes = num_processes\n        self.device = device\n\n        # Holds all the filepaths\n        self.filepaths = sorted([os.path.join(dataset_dir, p) for p in os.listdir(dataset_dir)])\n\n        self.file_data = np.load(self.filepaths[0])\n        self.file_idx = 0  # Current file index\n        self.tok_idx = batch * seq_len * process_rank  # Current token idx\n</code></pre> <p>Here\u2019s what happens under the hood in <code>__init__</code>:</p> <ol> <li>Instance Attribute: It sets the instance attributes using the given arguments  </li> <li>File discovery: It scans the dataset directory and gathers all <code>.npy</code> files (each file stores a large array of token IDs).  </li> <li>Pointers/Tracker Initialization:         - <code>file_data</code>: At startup, the loader reads the first <code>.npy</code> file in the dataset directory into memory. This array contains a long sequence of token IDs.         - <code>file_idx</code>: A counter that starts at <code>0</code>, meaning we are currently working with the first file in the dataset. As training progresses and one file is exhausted, this index is incremented to load the next file.         - <code>tok_idx</code>: A pointer into the current file that tells the loader where to start slicing tokens for the next batch. This is critical because each call to <code>get_batch()</code> must pick up right where the last one left off.         - Multi-GPU offset: If using multiple GPUs (distributed training), each process is assigned a different starting offset for <code>tok_idx</code>. This prevents all GPUs from training on the exact same data, ensuring better utilization of the dataset.  </li> </ol> <p>Together, these three trackers (<code>file_data</code>, <code>file_idx</code>, <code>tok_idx</code>) allow the loader to move seamlessly through massive token arrays spread across multiple files, while keeping every batch aligned and avoiding duplication across processes.</p>"},{"location":"pretraining/training_beginner/dataset_and_batching/#getting-a-batch","title":"Getting a Batch","text":"<p>The heart of the class is <code>get_batch()</code>. This is the function called during training to get new <code>(x, y)</code> tensors.</p> <ol> <li> <p>Slice out a chunk of tokens: </p> <pre><code>batch = self.file_data[self.tok_idx : self.tok_idx + (self.batch * self.seq_len) + 1]\n</code></pre> <p>Here we grab just enough tokens for a full batch (<code>batch * seq_len</code>) plus one extra, since <code>y</code> is shifted.</p> </li> <li> <p>Reshape into 2D arrays: </p> <pre><code>x = batch[:-1].reshape(self.batch, self.seq_len)\ny = batch[1:].reshape(self.batch, self.seq_len)\n</code></pre> <p>This step converts the flat token slice into two matrices: - <code>x</code> for the inputs, - <code>y</code> for the targets, shifted by one token.</p> </li> <li> <p>Advance the token index: </p> <pre><code>self.tok_idx += (self.batch * self.seq_len * self.num_processes)  # Increment the index counter\n\n# If we reach the end of file, move on to the next one\nif self.tok_idx + (self.batch * self.seq_len * self.num_processes) + 1 &gt;= len(self.file_data):\n    self.file_idx += 1\n    if self.file_idx &gt;= len(self.filepaths):\n        self.file_idx = 0\n\n    self.file_data = np.load(self.filepaths[self.file_idx])\n    self.tok_idx = self.batch * self.seq_len * self.process_rank\n</code></pre> <p>After returning a batch, the loader moves its pointer forward. If we reach the end of a file, it automatically loads the next one and update corresponding counters accordingly. </p> </li> <li> <p>Convert to tensors: </p> <pre><code>return torch.from_numpy(x.astype(np.int32)).long().to(self.device), torch.from_numpy(y.astype(np.int32)).long().to(self.device)\n</code></pre> <p>The NumPy arrays are cast to <code>torch.long</code> (integer type needed for embeddings) and moved to the correct device (CPU or GPU).</p> </li> </ol>"},{"location":"pretraining/training_beginner/dataset_and_batching/#why-this-design","title":"Why This Design?","text":"<p>Overall, the Dataset Loader is designed for training efficiency:</p> <ul> <li>Streaming from disk: It only loads one dataset file at a time, so memory usage stays low.  </li> <li>Batch alignment: It guarantees that <code>(x, y)</code> line up perfectly for next-token prediction.  </li> <li>Distributed training friendly: The <code>process_rank</code> and <code>num_processes</code> arguments make sure multiple GPUs can work on different slices of the dataset without overlap.  </li> <li>Scalable: As long as your dataset is tokenized into <code>.npy</code> files, this loader can handle billions of tokens just as easily as thousands.  </li> </ul> <p>One can think of it as a neat wrapper around:  </p> <ul> <li>slicing arrays,  </li> <li>reshaping them into <code>(batch, seq_len)</code> form,  </li> <li>shifting by one token, and  </li> <li>handing them to PyTorch.</li> </ul> <p>This simplicity makes it both easy to understand and powerful enough for large-scale training.</p>"},{"location":"pretraining/training_beginner/introduction/","title":"Introduction","text":""},{"location":"pretraining/training_beginner/introduction/#introduction-what-are-we-doing","title":"Introduction: What Are We Doing?","text":"<p>Before we dive into training, let\u2019s step back and ask: what exactly does it mean to \u201ctrain a language model\u201d?</p> <p>At its core, the goal is very simple: We want the model to learn how to predict the next token in a sequence of text.</p>"},{"location":"pretraining/training_beginner/introduction/#the-idea-in-plain-english","title":"The Idea in Plain English","text":"<p>Imagine you are reading a sentence:</p> <p>\"The cat sat on the ____\"</p> <p>Even without seeing the last word, your brain automatically guesses it might be \u201cmat\u201d, \"couch\", \"floor\" or any other plausible words. That\u2019s because you\u2019ve learned patterns of how words usually appear together.  </p> <p>A language model is doing the same thing, but instead of words, it deals with tokens (small units of text such as subwords, characters, or whole words depending on the tokenizer).  </p> <p>During training, the model sees billions of examples of token sequences and learns which token is most likely to come next. Over time, this builds up into a powerful statistical understanding of language.</p>"},{"location":"pretraining/training_beginner/introduction/#from-text-to-tokens","title":"From Text to Tokens","text":"<p>Computers can\u2019t directly process raw text like <code>\"The cat sat on the mat\"</code>. We first need to break it down into numerical form.</p> <ol> <li> <p>Raw Text: <code>\"The cat sat on the mat\"</code></p> </li> <li> <p>Tokenized Text (IDs): <code>[1202, 850, 149, 4211, 769, 1839]</code>    (each number is a token ID from a vocabulary)</p> </li> <li> <p>Input Tensor (batch of sequences): <pre><code>tensor([[1202,  850,  149, 4211,  769]])\n</code></pre></p> </li> </ol> <p>The vocabulary maps every possible token (like \u201ccat\u201d, \u201csat\u201d, \u201con\u201d) to a unique integer. The model only sees these integers.</p>"},{"location":"pretraining/training_beginner/introduction/#how-the-model-thinks","title":"How the Model Thinks","text":"<p>Now, let\u2019s show the high-level flow of how these tokens are processed:</p> <pre><code>flowchart LR\n    A[Raw Text] --&gt; B[Tokenizer]\n    B --&gt; C[Tokens: IDs]\n    C --&gt; D[Transformer Model]\n    D --&gt; E[Logits: raw scores]\n    E --&gt; F[Softmax: probabilities]\n    F --&gt; G[Loss Function]</code></pre> <p>Breaking down the important parts:</p> <ul> <li> <p>Logits: The model outputs a vector of raw scores for each possible token in the vocabulary. For example, if the vocab size is 50,000, the logits are a vector of 50,000 numbers. These are not yet probabilities.</p> </li> <li> <p>Softmax: We apply the softmax function to logits, which turns them into probabilities (values between 0 and 1 that sum to 1). For example, the model might say:</p> <ul> <li>\u201cmat\u201d: 0.72  </li> <li>\u201cfloor\u201d: 0.12  </li> <li>\u201ccouch\u201d: 0.03  </li> <li>\u2026 and so on for every other token.  </li> </ul> </li> <li> <p>Loss: The training process compares the predicted probability distribution to the correct answer (the actual next token). It calculates how far off the prediction was. This is the \u201closs.\u201d The smaller the loss, the better the model is at predicting.</p> </li> </ul> <p>This \u201cpredict the next token\u201d setup is deceptively simple, but it\u2019s powerful. By learning these probabilities across massive amounts of text, the model starts to capture grammar, facts, reasoning, and even world knowledge \u2014 all as a side effect of next-token prediction.</p> <p>So when we say \u201ctrain a transformer model,\u201d what we really mean is:  </p> <ul> <li>Give the model tons of sequences of tokens.  </li> <li>Ask it to guess the next token.  </li> <li>Measure how wrong it was.  </li> <li>Adjust the model\u2019s weights to improve its guesses.  </li> <li>Repeat billions of times.  </li> </ul> <p>That\u2019s the heart of language model training.</p>"},{"location":"pretraining/training_beginner/model_config/","title":"Model Configurations","text":""},{"location":"pretraining/training_beginner/model_config/#model-configuration","title":"Model Configuration","text":"<p>Before we can train our transformer, we need to decide on all the hyperparameters and settings that control both the model and the training process. These settings are stored in a configuration object, which in our case is implemented using a Python dataclass called <code>TrainingConfig</code> located in <code>SimpleLLaMA\\simple_llama\\pretraining\\config.py</code>.  </p> <p>The configuration file may look intimidating at first, since it lists dozens of parameters. But many of them are straightforward once you understand the categories they fall into. The following are the most important ones.</p> <p>The first group defines where the data and outputs are stored. For example:  </p> <ul> <li><code>dataset_dir</code> tells the program where to find the pre-tokenized dataset files.  </li> <li><code>tokenizer_path</code> points to the JSON file that contains the trained tokenizer.  </li> <li><code>ckpt_dir</code> specifies the folder where model checkpoints will be saved during training.  </li> <li><code>log_file</code> is a simple text file where progress (like loss values) is recorded.  </li> </ul> <p>Together, these ensure the training script knows both where to read the data from and where to save its results.  </p> <p>Next, we have the batch and sequence length parameters, which directly control how much data the model processes at once.  </p> <ul> <li><code>batch_size</code> is the number of sequences per batch. If you set this to 4, then each step processes 4 separate chunks of text in parallel.  </li> <li><code>max_seq_len</code> is the maximum number of tokens per sequence. For example, if <code>max_seq_len = 2048</code>, then each input sequence is capped at 2048 tokens long. Longer documents must be split into smaller pieces.  </li> <li><code>tokens_per_update</code> defines how many tokens are processed before the optimizer takes a step. Since this touches upon gradient accumulation, which is outside the scope of this basic explanation, it will be covered in the <code>training_advanced.md</code> file.  </li> </ul> <p>These three parameters together determine how much work the model is doing in each training step and a major factor of how much GPU memory the model will consume.</p> <p>Then comes the model architecture itself. These parameters define the shape and capacity of the transformer network:  </p> <ul> <li><code>n_embd</code> is the embedding dimension, the size of the vector used to represent each token internally. Larger values allow the model to capture richer relationships, but also make it heavier to train.  </li> <li><code>n_heads</code> sets how many attention heads are used per layer. Each head can focus on different relationships in the sequence, so more heads allow for more diverse patterns.  </li> <li><code>n_layers</code> is the number of stacked decoder layers. Each layer refines the token representations further, so deeper models are generally more powerful.  </li> <li><code>multiple_of</code> controls the feedforward layer\u2019s hidden dimension. Instead of choosing an arbitrary number, this ensures the size is a multiple of a fixed value (like 256), which helps optimize matrix multiplications on GPUs.  </li> <li><code>eps</code> is a tiny value added in normalization layers to avoid division by zero errors. It\u2019s not something you usually tweak, but it is essential for numerical stability.  </li> <li><code>theta</code> sets the base frequency for Rotary Position Embeddings (RoPE), which are used to encode token positions into the model. Again, you typically leave this at its default.  </li> <li><code>dropout</code> is a regularization mechanism where some connections are randomly \u201cdropped\u201d during training. For large pretraining, this is often set to <code>0.0</code> because the dataset itself provides enough variety, but in smaller-scale experiments you might increase it to avoid overfitting.  </li> </ul> <p>These architecture parameters is the core of the model. Changing them fundamentally alters the size and behavior of the transformer.</p> <p>Another critical part of the config is the training schedule. Training a large language model is not just about choosing an optimizer and running it \u2014 we also need to carefully plan how the learning rate evolves over time.  </p> <ul> <li><code>warmup_iterations</code> specifies how many steps are used to gradually increase the learning rate at the start of training. This prevents the model from diverging early on.  </li> <li><code>max_lr</code> is the peak learning rate reached after warmup.  </li> <li><code>min_lr</code> is the final learning rate at the end of training, typically reached through a cosine decay schedule.  </li> <li><code>beta1</code> and <code>beta2</code> are parameters of the AdamW optimizer, which control how much past gradients influence the updates.  </li> <li><code>weight_decay</code> is a form of regularization that prevents weights from growing too large, helping the model generalize better.  </li> </ul> <p>Together, these define the \u201cpace\u201d at which the model learns.</p> <p>Finally, we have the training tokens and evaluation settings.  </p> <ul> <li><code>training_tokens</code> is the total number of tokens the model will see during training. For example, <code>45e9</code> means 45 billion tokens in total.  </li> <li><code>eval_interval</code> controls how often the model\u2019s progress is evaluated. For instance, every 32 steps the model might generate text and log its loss.  </li> <li><code>model_gen_multiplier</code> adjusts how frequently sample generations are produced during training.  </li> </ul> <p>The config also includes checkpointing settings such as <code>token_ckpt</code> (how often to save the model in terms of tokens processed) and <code>load_ckpt</code> (whether to resume from a previous run).</p> <p>Even though this configuration object looks large, most of its parameters can be grouped into four main categories: paths, batching, model architecture, and training schedule. For the beginner doc, you don\u2019t need to memorize every single field \u2014 the important thing is to understand what each group does. The rest can be treated as implementation details that you return to once you start experimenting.</p>"},{"location":"pretraining/training_beginner/scheduler/","title":"Scheduler","text":"<p>The learning rate (LR) is one of the most important hyperparameters in training deep neural networks, where it's used to adjusts the learning rate dynamically during training, instead of keeping it fixed.</p> <p>This project includes a custom <code>Scheduler</code> class that implements warmup and three different scheduling strategies: cosine decay, linear decay, and constant LR.</p>"},{"location":"pretraining/training_beginner/scheduler/#why-use-a-scheduler","title":"Why Use a Scheduler?","text":"<p>Schedulers help address two common issues in optimization:</p> <ul> <li>Exploding/vanishing gradients \u2013 keeping the LR too high/low throughout training often leads to instability or poor convergence.  </li> <li>Training dynamics \u2013 a model often benefits from a short warmup phase (slowly ramping LR up), followed by a gradual decay to smaller values.  </li> <li>Generalization \u2013 decaying the LR near the end of training often improves final accuracy/perplexity.</li> </ul> <p>Instead of manually adjusting LR mid-training, a scheduler automates the process.</p>"},{"location":"pretraining/training_beginner/scheduler/#scheduler-implementation","title":"Scheduler Implementation","text":"<p>The <code>Scheduler</code> class wraps around a PyTorch optimizer. It is initialized with a few key parameters:</p> <pre><code>class Scheduler:\n    def __init__(self, torch_optimizer: Optimizer, schedule: str, training_steps: int,\n                 warmup_steps: int, max_lr: float, min_lr: float):\n        # schedule \u2208 [\"cosine\", \"linear\", \"constant\"]\n        # training_steps = total number of steps\n        # warmup_steps = steps spent ramping LR up\n        # max_lr = peak LR\n        # min_lr = final LR (ignored for \"constant\")\n</code></pre> <ul> <li>schedule: strategy (\"cosine\", \"linear\", or \"constant\").  </li> <li>training_steps: total steps in training run.  </li> <li>warmup_steps: number of warmup steps (linear ramp up).  </li> <li>max_lr: highest LR used during training.  </li> <li>min_lr: final LR (for decay-based schedules).  </li> </ul> <p>Warmup</p> <p>During warmup, LR increases linearly from near zero to <code>max_lr</code>:</p> <pre><code>def _update_warmup(self, current_step: int):\n    lr = (max(1, current_step) / self.warmup_steps) * self.max_lr\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n</code></pre> <p>This prevents unstable updates at the beginning of training.</p> <p>Cosine Decay</p> <p>Cosine decay smoothly lowers the LR from <code>max_lr</code> to <code>min_lr</code>:</p> <pre><code>def _update_cosine(self, current_step: int):\n    current_step -= self.warmup_steps\n    scale = (current_step / self.decay_steps) * math.pi\n    lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + math.cos(scale))\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n</code></pre> <p>This schedule is popular in modern LLM training because it decays aggressively at first, then flattens out.</p> <p>Linear Decay</p> <p>Linear decay reduces LR steadily over time:</p> <pre><code>def _update_linear(self, current_step: int):\n    current_step -= self.warmup_steps\n    lr = self.max_lr - (current_step / self.decay_steps) * (self.max_lr - self.min_lr)\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n</code></pre> <p>Simpler than cosine, but still effective.</p> <p>Constant</p> <p>Sometimes you may want to keep LR fixed at <code>max_lr</code> (e.g., for debugging).</p> <pre><code>if schedule == \"constant\":\n    for param_group in self.optimizer.param_groups:\n        param_group['lr'] = max_lr\n</code></pre> <p>Step Method</p> <p>The central logic is in the <code>step</code> method, which updates LR depending on the phase of training:</p> <pre><code>def step(self, current_step: int):\n    if current_step &lt; self.warmup_steps and self.schedule != \"constant\":\n        self.current_lr = self._update_warmup(current_step)\n        return\n\n    if self.schedule == \"cosine\":\n        self.current_lr = self._update_cosine(current_step)\n    elif self.schedule == \"linear\":\n        self.current_lr = self._update_linear(current_step)\n    elif self.schedule == \"constant\":\n        self.current_lr = self.max_lr\n</code></pre> <p>This ensures the correct schedule is applied at every step.</p>"},{"location":"pretraining/training_beginner/scheduler/#visualizing-the-schedules","title":"Visualizing the Schedules","text":"<p>To make things concrete, below are plots showing how the LR evolves across steps: (All are 100k total steps, 1k of which is warmup steps, max_lr set to 1e-3 and min_lr set to 1e-4)</p> <p>Cosine with Warmup:</p> <p></p> <p>Linear with Warmup:</p> <p></p> <p>Constant LR:</p> <p></p> <p>You can generate these plots using the included test script in the class (<code>__main__</code> block).</p>"},{"location":"pretraining/training_beginner/scheduler/#summary","title":"Summary","text":"<ul> <li>Warmup prevents instability at the start of training.  </li> <li>Cosine decay \u2192 smooth, effective, widely used in LLMs.  </li> <li>Linear decay \u2192 simpler, still works well.  </li> <li>Constant \u2192 mostly for experiments/debugging.  </li> </ul> <p>This custom scheduler is flexible, checkpointable, and provides good control for projects like this.</p>"},{"location":"pretraining/training_beginner/training_loop/","title":"Training Loop","text":"<p>Training a large language model (LLM), even a small\u2011scale one like in this project, comes down to a repeated cycle: take a batch of data, run it through the model, calculate how wrong the predictions are, push the error backwards to update the weights, and repeat. This cycle is what we call the training loop. This section will discuss in detail through the core parts of the loop. </p>"},{"location":"pretraining/training_beginner/training_loop/#instantiation","title":"Instantiation","text":"<p>Before we can train the model, we need to set up all the core building blocks. Once everything is in place, the training loop itself becomes a straightforward repetition of forward pass, loss calculation, backward pass, and optimization.</p> <p>1. Configuration Object</p> <p>The first thing we need is a configuration object that stores all of our hyperparameters. Instead of scattering values like batch size, learning rate, and number of layers across different files, it\u2019s cleaner to place them in a single file/class. This makes the code easier to manage, debug, and extend.  </p> <p>In this project, it will be the <code>TrainingConfig</code> class, located within the <code>simple_llama/pretraining/config.py</code> file</p> <pre><code>@dataclass\nclass TrainingConfig:\n    # === Paths and Dataset ===\n    dataset_dir: str = root_path(\"simple_llama\", \"dataset\", \"short\")       # Path to tokenized training data\n    tokenizer_path: str = root_path(\"simple_llama\", \"dataset\", \"bpe_8k.json\")          # Path to tokenizer model\n    ckpt_dir: str = root_path(\"simple_llama\", \"pretraining\", \"checkpoints\")   # Directory to store checkpoints\n    log_file: str = root_path(\"simple_llama\", \"pretraining\", \"training_progress.txt\")  # File to log training progress\n\n    # === Batch &amp; Sequence ===\n    batch_size: int = 4             # Minibatch size\n    max_seq_len: int = 2048         # Maximum sequence length per sample\n    tokens_per_update: int = 2**19  # ~512K tokens per optimizer update\n\n    # === Model Architecture ===\n    n_embd: int = 2048               # Embedding dimension\n    n_heads: int = 32                # Number of attention heads\n    n_layers: int = 24               # Number of transformer layers\n    multiple_of: int = 256           # Feedforward dim multiple for efficient matmul\n    eps: float = 1e-5                # Epsilon value to prevent div-by-zero in normalization layers\n    theta: int = 10_000              # Theta for RoPE rotation frequency\n    dropout: float = 0.0             # Dropout rate; typically 0.0 for pretraining\n\n    ...  # And many more\n\nconfig = TrainingConfig()\n</code></pre> <p>This way, if we want to adjust hyperparameters like <code>n_heads</code> or experiment with a different <code>max_lr</code>, it\u2019s a single line change.</p> <p>2. Dataset Loader</p> <p>Next, instantiate a dataset loader object that is defined, passing in hyperparameters as needed, extracted from the configuration object:</p> <pre><code>from simple_llama.pretraining.dataset_loader import DatasetLoader\n\ndataset_loader = DatasetLoader(batch=batch_size, seq_len=max_seq_len, \n                               process_rank=ddp_rank, num_processes=ddp_world_size, \n                               dataset_dir=config.dataset_dir, device=device)\n</code></pre> <p>3. The Model</p> <p>Now comes the centerpiece: the transformer model itself. In this project, we\u2019ve implemented <code>LLaMaTransformer</code>, which includes embeddings, attention blocks, feedforward layers, normalization, and output projection.</p> <pre><code>model = LLaMaTransformer(config, tokenizer, device=\"cuda\")\n</code></pre> <p>Here:  </p> <ul> <li><code>config</code> gives the model hyperparameters.  </li> <li><code>tokenizer</code> provides the vocabulary size.  </li> <li><code>device=\"cuda\"</code> places the model on GPU.</li> </ul> <p>Initially, the model\u2019s parameters are random. Training gradually adjusts them so that token predictions become more accurate.</p> <p>4. The Loss Function</p> <p>Next, we define how the model will be judged. For language modeling, the go-to choice is cross-entropy loss:</p> <pre><code>criterion = torch.nn.CrossEntropyLoss()\n</code></pre> <p>Cross-entropy measures how \u201csurprised\u201d the model is by the correct next token.  </p> <ul> <li>If the model assigns high probability \u2192 low loss.  </li> <li>If it assigns low probability \u2192 high loss.</li> </ul> <p>5. The Optimizer</p> <p>Finally, we define the optimizer. We use AdamW, which is the de facto standard for transformers because it combines Adam\u2019s adaptive gradient updates with weight decay for stability.</p> <pre><code>optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(beta1, beta2), weight_decay=weight_decay, **extra_args)\n</code></pre> <p>This way, every training step will use the optimizer to update the model parameters and the scheduler to adjust the learning rate.</p> <p>(More about AdamW optimizer will be covered in the Advanced section of the documentation)</p> <p>6. Learning Rate Scheduler</p> <p>Finally, the learning rate scheduler. The scheduler controls how the learning rate evolves over time, which is important for training.</p> <p>We\u2019re using the custom <code>Scheduler</code> class implemented earlier, which supports linear decay, cosine decay, or just a constant learning rate.</p> <pre><code>scheduler = Scheduler(torch_optimizer=optimizer,\n                      schedule=\"cosine\",\n                      training_steps=optimization_steps,\n                      warmup_steps=warmup_iterations,\n                      max_lr=max_lr,\n                      min_lr=min_lr)\n</code></pre> <p>At this point, we\u2019ve instantiated:  </p> <ul> <li>Configuration object</li> <li>Dataset loader</li> <li>Transformer model</li> <li>Loss function</li> <li>Optimizer</li> <li>Learning rate scheduler</li> </ul> <p>All the main components are ready. The next step is to actually run them inside the training loop.</p>"},{"location":"pretraining/training_beginner/training_loop/#the-model-forward-pass","title":"The Model Forward Pass","text":"<p>We begin with a batch of input tokens, grabbed from the DatasetLoader object via the <code>get_batch()</code> method. Each integer corresponds to a token ID from our vocabulary.  </p> <p>Let\u2019s say our batch size is <code>B = 4</code>, and the sequence length we train on is <code>T = 16</code>. The shape of a batch from the dataset loader would look like:</p> <pre><code>x.shape = (B, T) = (4, 16)\n</code></pre> <p>So <code>x</code> is a 2D tensor of integers. Each row is one training sequence, and each entry is a token ID.  </p> <p>When we feed this into the model:</p> <pre><code>logits = model(x)\n</code></pre> <p>the transformer runs all of its layers: embedding lookup, multiple decoder blocks, attention, feedforward layers, normalization, and finally a linear projection back to vocabulary size.  </p> <p>The key here is the shape change:  </p> <ul> <li>Input: <code>(B, T)</code> \u2014 integers.  </li> <li>Output: <code>(B, T, C)</code> \u2014 floats, where <code>C</code> is the vocabulary size.  </li> </ul> <p>Why <code>(B, T, C)</code>? Because for every position in every sequence, the model outputs a vector of size <code>C</code>, which are the raw unnormalized scores for each possible token in the vocabulary. These are called logits.</p>"},{"location":"pretraining/training_beginner/training_loop/#the-loss-function","title":"The Loss Function","text":"<p>Once we have logits, we want to measure how good the predictions are. That is the role of the loss function. For language modeling, the standard is cross entropy loss.</p> <p>The goal is simple: the model is asked to predict the next token in the sequence. If the input sequence is <code>[The, cat, sat, on, the]</code>, the correct output is <code>[cat, sat, on, the, mat]</code>. Each token should map to the next token.  </p> <p>Cross entropy measures how \u201csurprised\u201d the model is by the correct answer. If the model already places high probability on the true next token, the loss is small. If the model thought another token was much more likely, the loss is large.  </p> <p>In PyTorch, we use:</p> <pre><code>criterion = nn.CrossEntropyLoss()\n</code></pre> <p>However, <code>CrossEntropyLoss</code> expects inputs of shape <code>(N, C)</code> where <code>N</code> is the number of items and <code>C</code> is the number of classes, and targets of shape <code>(N,)</code>.  </p> <p>Our logits are <code>(B, T, C)</code> and our targets are <code>(B, T)</code>. So we flatten them:</p> <pre><code>loss = criterion(logits.view(-1, C), targets.view(-1))\n</code></pre> <p>This reshapes:</p> <ul> <li><code>logits.view(-1, C)</code> \u2192 <code>(B*T, C)</code> </li> <li><code>targets.view(-1)</code> \u2192 <code>(B*T,)</code> </li> </ul> <p>Effectively, we treat the whole batch as one big list of token predictions.</p> <p>Mathematically, cross entropy loss is:</p> \\[ L = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\big( \\text{softmax}(\\text{logits})_{i, y_i} \\big) \\] <p>where <code>y_i</code> is the true class (the correct next token).  </p> <p>More details will be covered in the Advanced Training section</p>"},{"location":"pretraining/training_beginner/training_loop/#the-backward-pass","title":"The Backward Pass","text":"<p>Now comes the critical part: telling the model how wrong it was. This is done with:</p> <pre><code>loss.backward()\n</code></pre> <p>This triggers PyTorch\u2019s autograd engine, which walks backwards through the computational graph.  </p> <p>Every tensor operation in PyTorch (matrix multiplies, nonlinearities, normalizations) records how it was computed. During <code>.backward()</code>, PyTorch applies the chain rule of calculus to compute gradients of the loss with respect to every parameter in the model.</p> <p>So, if our model has parameters \u03b8 = {W1, W2, \u2026}, then after <code>loss.backward()</code> we now have stored gradients \u2202L/\u2202W for each parameter. These gradients are stored in each parameter tensor within the <code>.grad</code> attribute, which is a matrix of gradients the shape as the weight matrix. </p> <p>These gradients tell us: \u201cIf you nudge this weight slightly, the loss would go up/down this much.\u201d </p> <p>Effectively, they are the signals that will guide weight updates.</p>"},{"location":"pretraining/training_beginner/training_loop/#the-optimizer-step","title":"The Optimizer Step","text":"<p>With gradients calculated, we actually update the weights. This is the job of the optimizer.  </p> <p>In this project, we use AdamW:</p> <pre><code>optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n</code></pre> <p>AdamW is a variant of stochastic gradient descent that adapts learning rates per parameter and includes proper weight decay. It\u2019s widely used in training transformers.</p> <p>The update cycle is:</p> <pre><code>optimizer.zero_grad(set_to_none=True)  # reset gradients\n\n# Between these two steps, perform forward pass, calculate loss, back propagation\n\noptimizer.step()       # update parameters using gradients\n</code></pre> <p>Zeroing the gradients is crucial because PyTorch accumulates gradients by default. If we didn\u2019t zero them, gradients from multiple steps would pile up, leading to artificially large parameter updates, quickly destabilizing the model.</p> <p>So the full cycle is:</p> <ol> <li>Zero gradients \u2192 prepare for next step.</li> <li>Forward pass \u2192 compute logits and loss.</li> <li>Loss calculation \u2192 use criterion to calculate loss.</li> <li>Backward pass \u2192 compute gradients.  </li> <li>Optimizer step \u2192 update weights.  </li> </ol>"},{"location":"pretraining/training_beginner/training_loop/#a-minimal-training-loop","title":"A Minimal Training Loop","text":"<p>Putting everything together:</p> <pre><code>for step in range(num_steps):\n    # Get a batch\n    x, y = dataset_loader.get_batch()   # x: (B, T), y: (B, T)\n\n    # Forward pass\n    logits = model(x)                   # (B, T, C)\n\n    # Compute loss\n    loss = criterion(logits.view(-1, C), y.view(-1))\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Update\n    optimizer.step()\n\n    if step % 100 == 0:\n        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n</code></pre> <p>Granted the actual implementation in <code>simple_llama.pretraining.train</code> file is much more complex, however this is the backbone of training. Every sophisticated training pipeline \u2014 from GPT to LLaMA \u2014 reduces to these few lines.  </p>"},{"location":"pretraining/training_beginner/training_loop/#evaluation-and-monitoring","title":"Evaluation and Monitoring","text":"<p>Training is only half the story. We need to know if the model is improving. The simplest way is to track the training loss. Over time, as the model sees more data, loss should decrease, which means the model is getting progressively better at predicting the next token, given an input sequence of tokens.</p> <p>At the very beginning, before the model has learned anything meaningful, predictions are essentially random. In this case, the expected loss can be approximated by the natural logarithm of the vocabulary size, since each token is equally likely under a uniform distribution.  </p> <p>For our project, the vocabulary size is 8192. So if the predictions were truly uniform, the expected initial loss would be:</p> <pre><code>ln(8192) \u2248 9.01\n</code></pre> <p>However, in practice, most parameters in the model (such as linear layers) are initialized from Gaussian distributions using Kaiming or Xavier initialization. This breaks the perfect uniformity and introduces biases. As a result, the observed loss at the very start of training would likely be higher than the theoretical value \u2014 for example, around 9.2 or 9.3 instead of exactly 9.01.  </p> <p>Why the log of vocab size?</p> <p>Cross-Entropy Loss (CEL) is essentially a Negative Log Likelihood (NLL) loss. For a dataset of size \\(N\\) with true labels \\(y_i\\) and predicted probabilities \\(p(y_i)\\):</p> \\[ CEL = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p(y_i) \\] <p>For a single example where the true class is \\(c\\):</p> \\[ CEL = -\\log p(c) \\] <p>If the model predicts uniformly over all \\(V\\) classes, then \\(p(c) = \\frac{1}{V}\\). Plugging this in:</p> \\[ CEL = -\\log \\left(\\frac{1}{V}\\right) = \\log V \\] <p>So under uniform predictions, the expected loss equals the log of vocabulary size.</p> <p>Example: </p> <ul> <li>\\(V = 8192\\) </li> <li>\\(CEL = \\log(8192)\\) </li> <li>\\(CEL \\approx 9.01\\) </li> </ul> <p>This is the theoretical baseline for random guessing. In practice, initialization bias may push it to ~9.3 at step 0.</p> <p>Training Dynamics</p> <p>As training continues, the loss should decrease steadily. For instance, a drop from ~9.3 to ~3 means the model is learning meaningful statistical patterns in the data. Lower loss translates directly into the model being less \u201csurprised\u201d when predicting the next token.</p> <p>Think of it this way:  </p> <ul> <li>At loss \u2248 9, the model is basically clueless, assigning ~1/8192 probability to every token.  </li> <li>At loss \u2248 3, the model assigns ~1/20 probability to the correct token on average.  </li> <li>At loss \u2248 1, the model is strongly confident, giving ~1/3 probability to the correct token.</li> </ul> <p>Even at a loss of around 3.0, the model assigns roughly 5% probability to the observed \u201ccorrect\u201d token on average. That may sound low if one interpret it as \"The model only have a 5% chance of choosing the correct token, for a given sequence\" However that is a bit misleading. In English (or just about all languages) there is natural entropy to it. Vast majority of the time, there are multiple valid answers to a given sequence.  </p> <p>Taking the previous example, we give the model: <code>[The, cat, sat, on, the]</code> and want it to predict the next token. Our true label should be the token corresponding to the word <code>mat</code> however, in general, there isn't just a single right-wrong answer.  Words like <code>floor</code>, <code>ground</code>, <code>couch</code> and such are also completely valid. Hence a probability of 1/20 chance choosing the 'correct' token isn't as bad a it may numerically seem to be. </p> <p>Validation?</p> <p>It\u2019s also common to periodically evaluate on a held-out validation set. This prevents overfitting, since training loss always decreases but validation loss may rise if the model memorizes.  </p> <p>However, in this project, no validation set is used. Why? Because the dataset (50B tokens gathered from FineWebEdu) is mostly unique. Training is done in a single epoch \u2014 the model will only see each token sequence once. Under this regime, overfitting is theoretically impossible.  </p> <p>In fact, if a model with ~1B parameters were able to fully overfit on 50B unique tokens, that would be remarkable \u2014 it would essentially mean the model is acting as a form of near-lossless compression of the dataset. From that perspective, it might even be considered desirable. But in practice, that's nearly impossible. Here we will only go through one pass using the 50B tokens, simply track training loss as the main signal of progress.</p> <p>A Tiny Generation Example</p> <p>Even early in training, it\u2019s fun (and useful) to test the model by generating text.  </p> <p>We take a prompt, tokenize it, and call a generate function:</p> <pre><code>print(model.generate(\"Once upon a time\", max_new_tokens=20))\n</code></pre> <p>At the start, the output will be nonsense \u2014 the model has learned almost nothing. But as loss decreases, generated samples gradually improve. They start forming grammatical sentences, then coherent paragraphs.</p> <p>This qualitative check is as important as loss curves, because it directly shows what the model is learning.</p> <p>Here is an example output log print when training a small model:</p> <pre><code>Step: 256 steps   |   Training Progress: 0.02%   |   Training Loss: 8.0160   |   Perplexity: 3029.09   |   Learning Rate: 0.00008   |   Norm: 1.0915   |   Tokens Processed: 8M (8M)   |   tok/s: 157961   |   Time: 53s\n----------------\nStep: 512 steps   |   Training Progress: 0.04%   |   Training Loss: 7.0701   |   Perplexity: 1176.23   |   Learning Rate: 0.00015   |   Norm: 0.2549   |   Tokens Processed: 16M (16M)   |   tok/s: 142851   |   Time: 58s\n----------------\n----------------\nStep: 768 steps   |   Training Progress: 0.06%   |   Training Loss: 6.5323   |   Perplexity: 686.96   |   Learning Rate: 0.00023   |   Norm: 0.1649   |   Tokens Processed: 25M (25M)   |   tok/s: 187962   |   Time: 44s\n----------------\n----------------\nStep: 1024 steps   |   Training Progress: 0.07%   |   Training Loss: 5.8950   |   Perplexity: 363.23   |   Learning Rate: 0.00031   |   Norm: 0.2274   |   Tokens Processed: 33M (33M)   |   tok/s: 187884   |   Time: 44s\n----------------\n----------------\nStep: 1280 steps   |   Training Progress: 0.09%   |   Training Loss: 5.6318   |   Perplexity: 279.16   |   Learning Rate: 0.00038   |   Norm: 0.2636   |   Tokens Processed: 41M (41M)   |   tok/s: 187881   |   Time: 44s\n----------------\n----------------\nStep: 1536 steps   |   Training Progress: 0.11%   |   Training Loss: 5.2796   |   Perplexity: 196.28   |   Learning Rate: 0.00046   |   Norm: 0.2596   |   Tokens Processed: 50M (50M)   |   tok/s: 187936   |   Time: 44s\n----------------\n----------------\nStep: 1792 steps   |   Training Progress: 0.13%   |   Training Loss: 5.1112   |   Perplexity: 165.87   |   Learning Rate: 0.00054   |   Norm: 0.2780   |   Tokens Processed: 58M (58M)   |   tok/s: 187930   |   Time: 44s\n----------------\n----------------\nStep: 2048 steps   |   Training Progress: 0.15%   |   Training Loss: 5.0083   |   Perplexity: 149.65   |   Learning Rate: 0.00060   |   Norm: 0.4782   |   Tokens Processed: 67M (67M)   |   tok/s: 188105   |   Time: 44s\n----------------\n----------------\nStep: 2304 steps   |   Training Progress: 0.17%   |   Training Loss: 4.7851   |   Perplexity: 119.71   |   Learning Rate: 0.00060   |   Norm: 0.3195   |   Tokens Processed: 75M (75M)   |   tok/s: 188087   |   Time: 44s\n----------------\n----------------\nStep: 2560 steps   |   Training Progress: 0.19%   |   Training Loss: 4.6413   |   Perplexity: 103.68   |   Learning Rate: 0.00060   |   Norm: 0.4755   |   Tokens Processed: 83M (83M)   |   tok/s: 187940   |   Time: 44s\n----------------\n----------------\nStep: 2816 steps   |   Training Progress: 0.21%   |   Training Loss: 4.6120   |   Perplexity: 100.68   |   Learning Rate: 0.00060   |   Norm: 0.2608   |   Tokens Processed: 92M (92M)   |   tok/s: 187844   |   Time: 44s\n----------------\n----------------\nStep: 3072 steps   |   Training Progress: 0.22%   |   Training Loss: 4.4724   |   Perplexity: 87.57   |   Learning Rate: 0.00060   |   Norm: 0.6141   |   Tokens Processed: 100M (100M)   |   tok/s: 187720   |   Time: 44s\n</code></pre> <p>Focusing on the training loss curve, we can see that it declines rapidly before slowly starts to plateau a bit.  Perplexity and Norm metric will be covered in the Advanced Guide section</p> <p>Here is an example generation output by the model at the very start, after just 256 steps:</p> <pre><code>&lt;SOS&gt; Classify the sentiment as \"Positive\" or \"Negative\":\nInput: I absolutely loved the movie, it was fantastic!\nSentiment: Positive\n\nInput: The service was terrible and I won't come back.\nSentiment: Negative\n\nInput: This book was boring and hard to finish.\nSentiment: Negative\n\nInput: The app keeps crashing and it's really frustrating.\nSentiment: Negative\n\nInput: I'm impressed by how fast and easy this process was.\nSentiment: Positive\n\nInput: The food was delicious and the atmosphere was wonderful.\nSentiment: told preventing capac appoint aboutterfacesstandingomeotheods char AcuseAAes applications governments Theseind energy\nbe electroesietThese Discussteries contains spendasma critaries treatels 190 facilitaintically majority might 13 calculate honey Colle robot Orony soils Fin dest confirmed7 financialcom. highest Denheastoet rec branch\n</code></pre> <p>In the above example, the provided text sequence ended at:</p> <pre><code>Input: The food was delicious and the atmosphere was wonderful.\nSentiment: \n</code></pre> <p>Where after the 'Sentiment: ' part, we can see that the output is garbled, as we would expect.  However, at the next evaluation, the following resulted from the model:</p> <pre><code>&lt;SOS&gt; Write a polite email reply:\nInput: Hi, can you send me the report by tomorrow? Thanks.\nReply: Hi, sure thing! I'll send the report to you by tomorrow. Let me know if you need anything else.\n\nInput: Hi, just checking if you're available for a meeting this Friday.\nReply: Hi, thanks for reaching out. I'm available on Friday - what time works best for you?\n\nInput: Hi, could you help me with the project deadline?\nReply: Hi, of course. Let me know what you need help with, and I'll do my best to assist.\n\nInput: Hi, do you have the updated slides for the presentation?\nReply: Hi, yes - I'll send over the updated slides shortly. Let me know if you'd like me to walk you through them.\n\nInput: Hi, do you have time to meet with the manager later today?\nReply: How do I have no force?\nLearn't have an error in good, and I am looking for a call while you should check.\nUse students learn about three different environments, including:\nReportness (such as CCM)\nYou can use a sample is\nClick\n</code></pre> <p>Here, the provided text sequence is different (to provide variations during evaluations)</p> <p>In the above example, the provided text sequence ended at: </p> <pre><code>Input: Hi, do you have time to meet with the manager later today?\nReply: \n</code></pre> <p>The model is clearly learning valid words and starts to piece them together in a somewhat structured way.</p>"},{"location":"pretraining/training_beginner/training_loop/#bringing-it-all-together","title":"Bringing It All Together","text":"<p>To summarize, each training step does:</p> <ol> <li>Take a batch <code>(B, T)</code> of token IDs.  </li> <li>Run through model \u2192 get logits <code>(B, T, C)</code>.  </li> <li>Compute cross entropy loss with targets <code>(B, T)</code>.  </li> <li>Backpropagate loss \u2192 compute gradients.  </li> <li>Optimizer updates weights.  </li> <li>Zero gradients.  </li> </ol> <p>This loop runs millions of times. At small scale, it might be just tens of thousands of steps. At large scale (GPT\u20113, LLaMA), training can take trillions of tokens.</p> <p>But the essence is always the same. The beauty of the transformer is that all of this complexity \u2014 embeddings, attention, normalization, feedforward layers \u2014 reduces down to the training loop you\u2019ve just seen.</p>"},{"location":"rlhf/methods/","title":"Rlhf Overview","text":"<p>methods.md file</p>"},{"location":"rlhf/overview/","title":"Rlhf Overview","text":"<p>overview.md file</p>"},{"location":"sft/dataset/","title":"Supervised Fine-Tuning Dataset Creation","text":"<p>This document explains in detail how the Supervised Fine-Tuning (SFT) dataset was created for the SimpleLLaMA project. The dataset contains 216,000 synthetic samples generated using DeepSeek and Gemini 2.5 (Flash and Pro). It is designed specifically for a 1.3B parameter model \u2014 which is much smaller and simpler than the 7B+ models often targeted by publicly available datasets \u2014 and therefore required careful curation and balance.</p>"},{"location":"sft/dataset/#why-synthetic-instead-of-using-hugging-face-datasets","title":"Why Synthetic Instead of Using Hugging Face Datasets?","text":"<p>While there are many instruction-tuning datasets available on Hugging Face, such as Alpaca, ShareGPT, OASST, etc., they are generally constructed for larger-scale models (7B\u201370B parameters). Using them directly could misalign a smaller model in several ways:</p> <ul> <li>Level of difficulty \u2013 Datasets like OASST or ShareGPT contain complex, nuanced conversations. For a 1.3B model, this would often lead to failure cases. A controlled dataset is used to confirm examples are at the right difficulty level.  </li> <li>Distribution mismatch \u2013 Public datasets are often skewed toward open-domain chit-chat, or contain an imbalanced ratio of tasks. Here, the dataset was constructed with explicit category proportions (see below).  </li> <li>Control over content \u2013 By generating synthetic data, we can tightly control what categories exist, their frequency, and their complexity. This reduces noise and makes the model more reliable for its intended use.  </li> </ul> <p>In short: synthetic generation provides balance, scalability, and suitability for a smaller model, while avoiding the pitfalls of \u201covershooting\u201d with datasets made for much larger systems.</p>"},{"location":"sft/dataset/#dataset-composition","title":"Dataset Composition","text":"<p>The dataset is broken down into 8 categories, each representing a different skill domain. These categories were chosen to ensure broad but manageable coverage of practical use cases. Below is the distribution:</p> Category Share Purpose Example Instruction Following 30% Core alignment skill: teaching the model to obey explicit instructions. \u201cSummarize this article in 2 sentences.\u201d User Conversation 20% Multi-turn dialogues, simulating back-and-forth exchanges. Adds robustness to conversational flow. \u201cHello! How are you?\u201d \u2192 \u201cI\u2019m good, how are you?\u201d Reasoning &amp; Logic 15% Arithmetic, logical reasoning, multi-step thought. Strengthens structure in answers. \u201cIf Sarah has 5 apples and eats 2, how many are left?\u201d QA (Factual) 10% Short, grounded factual responses. Prevents drift and strengthens retrieval-like capabilities. \u201cWhat is the capital of Canada?\u201d Error Correction 5% Simple grammar/spelling fixes. Teaches token-level precision. Input: \u201cShe go to market.\u201d \u2192 Output: \u201cShe goes to the market.\u201d Structured Transformation 5% Converting between formats, summarization, bullet-pointing, JSON transformations. \u201cTurn this paragraph into a bulleted list.\u201d Long Response Generation 5% Encourages extended outputs, storytelling, and maintaining coherence across long sequences. \u201cWrite a short story about a lost robot.\u201d Identity / Miscellaneous 10% Handles \u201cWho are you?\u201d queries, opinions, small-talk, and personality alignment. \u201cWho are you?\u201d \u2192 \u201cI\u2019m SimpleLLaMA, your assistant.\u201d"},{"location":"sft/dataset/#subcategories","title":"Subcategories","text":"<p>Within each major category, subcategories were mixed in to increase diversity. For example:</p> <ul> <li> <p>Instruction Following</p> <ul> <li>Summarization (\u201cSummarize this paragraph in 3 bullet points\u201d).  </li> <li>Classification (\u201cClassify this review as positive or negative\u201d).  </li> <li>Style constraints (\u201cExplain this like I\u2019m 5\u201d).  </li> </ul> </li> <li> <p>User Conversation</p> <ul> <li>Multi-turn casual conversation.  </li> <li>Task-based (helping with scheduling, answering follow-ups).  </li> </ul> </li> <li> <p>Reasoning &amp; Logic</p> <ul> <li>Arithmetic problems.  </li> <li>Deductive logic.  </li> <li>Simple word problems.  </li> </ul> </li> <li> <p>Structured Transformation</p> <ul> <li>Bullet point compression.  </li> <li>Extracting keywords.  </li> </ul> </li> </ul> <p>This is to make sure that within each category, the model isn\u2019t just repeating a narrow skill, but instead sees varied applications of the same general ability.</p>"},{"location":"sft/dataset/#generation-pipeline","title":"Generation Pipeline","text":""},{"location":"sft/dataset/#step-1-few-shot-prompting","title":"Step 1: Few-Shot Prompting","text":"<p>Examples were generated using few-shot prompts with Gemini and DeepSeek. A handful of verified examples were embedded into the system prompt, ensuring that outputs were consistent with the required format:</p> <pre><code>{\n  \"Template\": [\"Speak like a teacher\"],\n  \"User\": [\"Why does the sun set?\"],\n  \"Assistant\": [\"The sun appears to set because the Earth rotates, making it seem like the sun is moving across the sky.\"]\n}\n</code></pre>"},{"location":"sft/dataset/#step-2-strict-formatting","title":"Step 2: Strict Formatting","text":"<p>The output had to always be a list of JSON dicts with exactly three keys: <code>Template</code>, <code>User</code>, <code>Assistant</code>. Each value was wrapped in a list of strings to ensure uniformity with the later dataset loader.</p>"},{"location":"sft/dataset/#step-3-ascii-enforcement","title":"Step 3: ASCII Enforcement","text":"<p>Since the tokenizer was trained only on ASCII English text, all outputs were passed through a normalization step that replaced or removed non-ASCII characters. For example:</p> <ul> <li>\u201ccaf\u00e9\u201d \u2192 \u201ccafe\u201d  </li> <li>\u201c\u03c0\u201d \u2192 \u201cpi\u201d  </li> <li>\u201c\u2013\u201d (en dash) \u2192 \u201c-\u201d  </li> </ul> <p>This avoided subtle tokenization mismatches that could destabilize training.</p>"},{"location":"sft/dataset/#step-4-quality-controls","title":"Step 4: Quality Controls","text":"<ul> <li>Multi-model variation \u2013 Three different LLMs (DeepSeek, Gemini Flash, Gemini Pro) were used for generation. This provided diverse styles and avoided overfitting to one model\u2019s quirks.  </li> <li>Non-ASCII filter \u2013 All responses passed through a script that rejected or normalized invalid characters.  </li> <li>Manual inspection \u2013 Roughly 0.5% of the dataset (\u22481,000 samples) was manually checked. Results showed very low hallucination or formatting issues.  </li> <li>Broad scope \u2013 Prompts were intentionally kept simple and general (instruction following, reasoning, factual questions). This avoided cases where even state-of-the-art models might hallucinate.</li> </ul>"},{"location":"sft/dataset/#example-of-prompt-construction","title":"Example of Prompt Construction","text":"<p>Below is a simplified version of the generation prompt used for instruction-following data:</p> <pre><code>You are generating high-quality instruction-following examples for an SFT dataset.\nMake sure responses are ASCII only.\nEach example must be a JSON dict with keys: \"Template\", \"User\", \"Assistant\".\nResponses must be coherent, varied, and reflect the style in \"Template\".\nGenerate 5 JSON dicts, each containing a unique instruction-response pair.\n</code></pre> <p>In the actual generation script, varying values such as <code>top_p</code>, <code>temperature</code>, fewshot examples, were injected into the prompt to prevent the LLM from generating the same, or extremely similar, results. </p>"},{"location":"sft/dataset/#generation-samples","title":"Generation Samples","text":"<p>Here are some examples within the SFT dataset: </p> <pre><code>[\n    {\n        \"Template\": [\n            \"CUSTOM\"\n        ],\n        \"User\": [\n            \"Can you explain the greenhouse effect in simple terms and how it relates to climate change? I've heard the term a lot but don't fully grasp the mechanism.\",\n            \"Switching topics completely, what's your favorite method for organizing a messy garage that hasn't been cleaned in years? I need practical steps to tackle this overwhelming project.\"\n        ],\n        \"Assistant\": [\n            \"The greenhouse effect is when certain gases in Earth's atmosphere (like carbon dioxide and methane) trap heat from the sun, similar to how glass traps heat in a greenhouse. Normally this keeps Earth warm enough for life, but human activities have increased these gases, causing more heat retention and global climate changes. Burning fossil fuels and deforestation are major contributors to this enhanced greenhouse effect.\",\n            \"Start by sorting items into categories: keep, donate, recycle, trash. Use the '5-box method' - one box for each category plus a 'relocate' box for items that belong elsewhere. Work in small sections to avoid overwhelm. Install wall hooks for tools, use clear bins for visibility, and label everything. Schedule regular maintenance sessions to prevent future buildup.\"\n        ]\n    },\n    {\n        \"Template\": [\n            \"CUSTOM\"\n        ],\n        \"User\": [\n            \"All birds have feathers. A penguin is a bird. Does a penguin have feathers?\"\n        ],\n        \"Assistant\": [\n            \"Yes, a penguin has feathers. The reasoning follows deductively: All birds have feathers, and since a penguin is classified as a bird, it must also possess feathers. This holds true even though penguins are flightless-their feathers serve other purposes like insulation and waterproofing, but they still fit the definition of birds in this regard.\"\n        ]\n    },\n    {\n        \"Template\": [\n            \"Explain concepts as if you are a wise, patient, and old librarian. Use metaphors related to books, stories, and knowledge.\"\n        ],\n        \"User\": [\n            \"Can you explain what a 'database' is?\"\n        ],\n        \"Assistant\": [\n            \"Ah, a wonderful question. Imagine this library is the whole of your information. A 'database' is like a magical card catalog. It doesn't hold the books themselves, but it knows exactly where each one is. It has a card for every story, every fact, every character. When you need to find something, you don't wander the stacks for ages. You simply consult the catalog, and it points you to the exact shelf, the exact book, the exact page. It's a system of perfect order for a world of chaotic information.\"\n        ]\n    }\n]\n</code></pre> <ul> <li>The first example shows a <code>CUSTOM</code> system prompt and demonstrates flexibility \u2014 answering a science question and then switching to a practical, unrelated task.  </li> <li>The second example uses categorical logic (\u201call birds have feathers\u201d), which strengthens the model\u2019s structured reasoning abilities.  </li> <li>The third example illustrates a style template (the whimsical librarian), which enriches the dataset with persona-following behavior.</li> </ul> <p>Most entries use <code>CUSTOM</code> as the template, which defaults to the general system prompt. Around 20\u201325% explicitly define styles or roles, broadening stylistic adaptability and improving robustness. More will be discussed in the next section, Prompt Formatting. </p>"},{"location":"sft/dataset/#jsondatasetloader","title":"JSONDatasetLoader","text":"<p>Finally, the <code>JSONDatasetLoader</code> class. This is used for loading, processing, and serving the SFT dataset during fine-tuning. It handles both the training and validation data splits, manages batching, and keeps track of epoch progress.</p>"},{"location":"sft/dataset/#constructor","title":"Constructor","text":"<p>Let\u2019s start with the constructor, step by step:</p> <pre><code>def __init__(self, json_filepath: str, batch_size: int, train_split: float):\n    assert batch_size &gt; 0\n    assert 0 &lt; train_split &lt;= 1\n\n    # Load in the dataset, should be a list of dicts\n    # Should have User, Assistant, and Template keys, of types list[str], list[str] and str respectively\n    with open(json_filepath, \"r\", encoding=\"utf-8\") as f:\n        dataset = json.load(f)\n\n    random.shuffle(dataset)\n</code></pre> <p>The constructor accepts three arguments:</p> <ul> <li><code>json_filepath</code>: the path to the JSON dataset file (e.g., <code>merged_ft_dataset.json</code>).</li> <li><code>batch_size</code>: how many examples are returned per batch.</li> <li><code>train_split</code>: the proportion of examples allocated for training versus validation.</li> </ul> <p>After confirming valid inputs, the JSON file is opened and loaded. This file should contain a list of dictionaries where each dictionary corresponds to a conversation sample, containing the three keys: <code>User</code>, <code>Assistant</code>, and <code>Template</code>. The dataset is shuffled to ensure randomness before splitting.</p> <p>Next, the dataset is converted into a list of formatted <code>(x, y)</code> string pairs using the <code>format_training_prompt</code> function:</p> <pre><code>dataset = [format_training_prompt(user=d[\"User\"],\n                                  assistant=d[\"Assistant\"],\n                                  template=(d[\"Template\"][0])\n                                  ) for d in dataset]\n\nn = int(len(dataset) * train_split)\nself.train_dataset = dataset[:n]\nself.val_dataset = dataset[n:]\n</code></pre> <p>Each entry is passed through <code>format_training_prompt</code>, which transforms raw JSON entries into fully formatted text prompts and target responses suitable for tokenization. This function handles insertion of special tokens (<code>&lt;SOT&gt;</code>, <code>&lt;SOU&gt;</code>, <code>&lt;EOU&gt;</code>, <code>&lt;SOA&gt;</code>, <code>&lt;EOA&gt;</code>) and confirms that the final assistant output aligns correctly with the training objective. (More detail about this will be covered in the next section, <code>Prompt Formatting</code>)</p> <p>The dataset is then divided into training and validation sets according to the provided split ratio. For instance, if <code>train_split=0.99</code>, 99% of examples go to training and 1% to validation.</p> <p>Finally, we initialize several bookkeeping variables:</p> <pre><code>self.batch_size = batch_size\nself.train_epoch = 0\nself.val_epoch = 0\nself.train_idx = 0\nself.val_idx = 0\n\n# Remove from memory\ndel dataset\n</code></pre> <ul> <li><code>train_epoch</code> and <code>val_epoch</code> track the number of completed epochs for each split.</li> <li><code>train_idx</code> and <code>val_idx</code> track where we are in the dataset for each epoch.</li> <li>The original <code>dataset</code> variable is deleted to conserve memory, as it can be quite large in full-scale fine-tuning.</li> </ul>"},{"location":"sft/dataset/#get_batch-method","title":"<code>get_batch</code> Method","text":"<p>The <code>get_batch</code> method is the main interface for retrieving data batches during training and evaluation.</p> <pre><code>def get_batch(self, train: bool, increment_val_idx=True):\n    # \"increment_val_idx\" is set to False when needing to eval a small section\n\n    if train:\n        batch = self.train_dataset[self.train_idx: self.train_idx + self.batch_size]\n        self.train_idx += self.batch_size\n\n        if self.train_idx + self.batch_size &gt;= len(self.train_dataset):\n            self.train_idx = 0\n            self.train_epoch += 1\n            random.shuffle(self.train_dataset)\n</code></pre> <p>When <code>train=True</code>, the function slices a batch from <code>self.train_dataset</code> using the current index. After retrieving the batch, it increments the index to point to the next section. If the index reaches the end of the dataset, it resets back to zero, increments the epoch counter, and reshuffles the training data before proceeding to the nexte poch.</p> <p>This cyclical behavior ensures that the data pipeline continuously feeds new permutations of the dataset throughout fine-tuning.</p> <p>When <code>train=False</code>, the loader instead retrieves data from the validation set:</p> <pre><code>else:\n    batch = self.val_dataset[self.val_idx: self.val_idx + self.batch_size]\n    if increment_val_idx:\n        self.val_idx += self.batch_size\n\n    if self.val_idx + self.batch_size &gt;= len(self.val_dataset):\n        self.val_idx = 0\n        self.val_epoch += 1\n\n    return batch\n</code></pre> <p>Here, the <code>increment_val_idx</code> parameter is important. During full validation (when evaluating the entire validation set), this flag remains <code>True</code> so that the loader moves through the data sequentially. However, for quick validation intervals during training \u2014 which happen often \u2014 <code>increment_val_idx</code> is set to <code>False</code>, ensuring the same small validation batch is reused for efficiency, to avoid excess computational cost. </p>"},{"location":"sft/finetuning/","title":"Supervised Fine-Tuning Script (<code>finetune.py</code>)","text":""},{"location":"sft/finetuning/#overview","title":"Overview","text":"<p>Supervised Fine-Tuning (SFT) is the first post-pretraining stage in the instruction-tuning pipeline. Its purpose is to teach a pretrained model to follow instructions, adopt roles or personas, and provide structured answers in a format expected by users.</p> <p>This script implements a full fine-tuning loop for this stage, using a JSON dataset and pre-trained weights as the base. In the following sections we'll walk through the <code>finetune.py</code> script located in <code>simple_llama\\finetune\\full_sft\\finetune.py</code> step by step, covering it in depth.</p>"},{"location":"sft/finetuning/#1-configuration","title":"1. Configuration","text":"<p>(Most of the setup is very similar to the pretraining script, which was covered in the previous section, and will only be briefly covered here.)</p> <p>At the start of the script, key imports and configurations are handled:</p> <pre><code>import os\nimport time\nimport random\nimport math\nimport torch\nimport inspect\nimport numpy as np\nfrom tokenizers import Tokenizer, decoders\n\nfrom simple_llama.pretraining.llama_transformer import LLaMaTransformer\nfrom simple_llama.pretraining.lr_scheduler import Scheduler\nfrom simple_llama.pretraining.config import TrainingConfig\nfrom simple_llama.finetune.full_sft.sft_config import SFTConfigs\nfrom simple_llama.finetune.json_dataset_loader import JSONDatasetLoader\nfrom simple_llama.finetune.utils import tokenize_and_pad_data, eval_model, sft_prompts\nfrom simple_llama.finetune.format_llm_prompt import format_inference_prompt\n</code></pre> <p>Next, set a manual seed for reproducibility:</p> <pre><code>seed = 89\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n</code></pre> <p>Then specify PyTorch matmul precision and device:</p> <pre><code>torch.set_float32_matmul_precision(\"high\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</code></pre> <p>All hyperparameters for SFT are loaded from the <code>SFTConfigs</code> dataclass, which mirrors <code>TrainingConfig</code> but is adjusted for finetuning:</p> <pre><code>sft_configs = SFTConfigs()\n\n# Hyperparameters\n# --------------------------------------\nft_json_path = sft_configs.ft_json_path\nenable_compilation = sft_configs.enable_compilation\nbatch_size = sft_configs.batch_size\neval_interval = sft_configs.eval_interval\n\nwarmup_iterations = sft_configs.warmup_iterations\nmax_lr = sft_configs.max_lr\nmin_lr = sft_configs.min_lr\nbeta1 = sft_configs.beta1\nbeta2 = sft_configs.beta2\nweight_decay = sft_configs.weight_decay\ntrain_split = sft_configs.train_split\n...\n</code></pre> <p>This includes batch size, gradient accumulation, learning rates, dropout, epochs, and logging/checkpoint paths.</p> <p>Finally, set up a log file and print out configuration, similar to how pretraining script does it.</p>"},{"location":"sft/finetuning/#2-initialization","title":"2. Initialization","text":"<p>A <code>JSONDatasetLoader</code> object is instantiated to manage train/validation splits:</p> <pre><code>dataset_loader = JSONDatasetLoader(json_filepath=ft_json_path, batch_size=batch_size, train_split=train_split)\n</code></pre> <p>This loader is responsible for retrieving <code>(x, y)</code> pairs and manages batching and epoch updates.</p> <p>That's then followed by loading the tokenizer and saved checkpoint:</p> <pre><code>tokenizer = Tokenizer.from_file(sft_configs.tokenizer_path)\ntokenizer.model.unk_token = \"&lt;UNK&gt;\"  # Set unknown token to &lt;UNK&gt;\ntokenizer.decoder = decoders.ByteLevel()  # For byte-level decoding\n\nckpt = torch.load(sft_configs.model_path, map_location=device)\n</code></pre> <p>The pad token <code>&lt;PAD&gt;</code> is set to maintain consistent masking:</p> <pre><code>pad_token = \"&lt;PAD&gt;\"\npad_id = tokenizer.encode(pad_token).ids[0]\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n</code></pre> <p>After that, initialize a model instance with the loaded configuration:</p> <pre><code>model = LLaMaTransformer(config=training_config, tokenizer=tokenizer, device=device).to(device)\nmodel.train()\n\nmodel.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n\nn_params = sum([p.numel() for p in model.parameters()])\nprint(f\"There is {n_params / 1e6:.1f}M parameters in the model\")\n</code></pre> <p>The pretrained weights are loaded directly from the checkpoint, and the total parameter count is displayed for verification.</p> <p>Next, configure the optimizer and check for fused kernel support (a small speed optimization for GPUs):</p> <pre><code>fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\nuse_fused = fused_available and device == \"cuda\"\nextra_args = dict(fused=True) if use_fused else dict()\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, betas=(beta1, beta2), weight_decay=weight_decay, **extra_args)\nprint(f\"Using fused optimizer: {use_fused}\\n\")\n\npad_token = \"&lt;PAD&gt;\"\npad_id = tokenizer.encode(pad_token).ids\nassert len(pad_id) == 1, f\"{pad_token=} should be a special token with a single value!\"\npad_id = pad_id[0]\n</code></pre> <p>Then, the scheduler and optional model compilation step are set up:</p> <pre><code>criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n\nscheduler = Scheduler(torch_optimizer=optimizer,\n                      schedule=\"cosine\",\n                      training_steps=optimization_steps,\n                      warmup_steps=warmup_iterations,\n                      max_lr=max_lr,\n                      min_lr=min_lr)\n\nif enable_compilation:\n    compiled_model = torch.compile(model)\n</code></pre> <p>The loss function (<code>CrossEntropyLoss</code>) is initialized with <code>ignore_index=pad_id</code> so that padding tokens do not affect training. The cosine scheduler controls learning rate decay over time. Model compilation (via <code>torch.compile</code>) can further accelerate training on supported GPUs.</p>"},{"location":"sft/finetuning/#3-training-loop","title":"3. Training Loop","text":"<p>Right before entering the training loop, </p> <pre><code>start = time.time()\nall_losses = []  # Keeping track of all losses\nsave_ckpt = {}  # Used to save model checkpoint (Holds all state_dicts, hyperparameters, etc.)\nnorm = float(\"inf\")  # A temp place holder for actual norm\nstep = 1  # Step here is an approximate since this is now epoch-based rather than token-based iterations\n\n# This autocasts certain parts of the layers (mostly matmul portion) within the model to bf16 for faster training\nuse_amp = torch.cuda.is_available() and device == \"cuda\" and torch.cuda.is_bf16_supported()\nprint(f\"Using auto mixed precision: {use_amp}\")\n</code></pre> <p>Here\u2019s what is initialized:</p> <ul> <li>Timer (<code>start</code>) \u2014 Tracks elapsed training time.  </li> <li><code>all_losses</code> \u2014 Stores raw loss values per iteration for later averaging/plotting.  </li> <li><code>save_ckpt</code> \u2014 A dictionary to collect all model states and optimizer/scheduler info for checkpointing.  </li> <li><code>norm</code> \u2014 Placeholder for gradient norm, updated later each optimization step for stability checks.  </li> <li><code>step</code> \u2014 Tracks global step count, which approximates iteration progress across epochs.  </li> <li><code>use_amp</code> \u2014 Checks if the device supports automatic mixed precision (AMP), which speeds up training by casting certain matrix operations to <code>bfloat16</code>.</li> </ul> <p>After that setup, the true training loop begins. </p> <p>First, </p> <pre><code>for epoch in range(epochs):\n    current_val_epoch = dataset_loader.val_epoch\n    current_train_epoch = dataset_loader.train_epoch\n\n    print(\"\\n\" + \"=\" * 20)\n    print(f\"Current Epoch: {epoch+1}\")\n    print(\"=\" * 20 + \"\\n\")\n</code></pre> <p>We loop <code>epochs</code> times, as set by the sft config. Save the current validation and training epoch, and print the current progress.</p> <p>Next, enter a while loop. While we are within the same training epoch, keep sampling batches from the training set until complete. </p> <p>In the first part of the while loop: </p> <pre><code>    while dataset_loader.train_epoch == current_train_epoch:\n        batch = dataset_loader.get_batch(train=True)\n\n        # Encode and pad the x-y strings\n        x, y = tokenize_and_pad_data(batch=batch,\n                                     tokenizer=tokenizer,\n                                     pad_id=pad_id,\n                                     max_seq_len=max_seq_len,\n                                     dynamic=dynamic_padding,\n                                     device=device\n                                     )\n</code></pre> <p>Grab a batch from the dataset loader, use the <code>tokenize_and_pad_data</code> function that was discussed previously to convert the raw strings into x-y tensors.</p> <pre><code>        with torch.autocast(device_type=device, dtype=torch.bfloat16 if use_amp else torch.float32):\n            if enable_compilation:\n                pred = compiled_model(x)\n            else:\n                pred = model(x)\n\n            B, T, C = pred.shape\n            loss = criterion(pred.reshape(B * T, C), y.reshape(B * T))\n</code></pre> <p>Using the <code>torch.autocast</code> context manager, pass the input tensor <code>x</code> into the model, and compute the loss accordingly. Mixed precision training can significantly speed up computation and reduce memory usage on modern GPUs.</p> <pre><code>        train_loss_value = loss.item()  # Before normalization for backprop\n        loss /= grad_accum_steps  # Normalize loss\n        loss.backward()\n\n        all_losses.append(train_loss_value)  # Add the loss\n\n        if step % grad_accum_steps == 0:\n            scheduler.step(step // grad_accum_steps)  # Set the lr first\n            norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Prevents unstable learning\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n</code></pre> <p>Save the training loss, normalize it based on <code>grad_accum_steps</code> for gradient accumulation, then backpropagate the loss. If we have accumulated gradients across <code>grad_accum_steps</code> steps, update the learning rate, clip gradient norm if needed, step the optimizer, and then reset all gradients.</p> <p>The next section after this is just about intermediate evaluation and checkpoint saving, which is pretty much the same as the part in pretraining with just very minor changes, I will not go over it again here. </p> <p>Finally, at the end of each epoch, the model evaluates across the full validation set:</p> <pre><code>    full_val_loss = eval_model(model=model, criterion=criterion, tokenizer=tokenizer,\n                               dataset_loader=dataset_loader, use_amp=use_amp, full_eval=True, pad_id=pad_id,\n                               max_seq_len=max_seq_len, dynamic=dynamic_padding, device=device)\n\n    print(\"\\n\" + \"=\" * 20)\n    print(f\"Validation Loss: {full_val_loss:.4f}\")\n    print(\"=\" * 20 + \"\\n\")\n</code></pre> <p>Once the training loop finishes an epoch, we calculate validation loss across all validation examples, print it out, and proceed to the next epoch. This ensures that each epoch ends with a full model performance check rather than just a partial metric.</p>"},{"location":"sft/finetuning/#summary","title":"Summary","text":"<p>The <code>finetune.py</code> script completes the SFT stage by transforming the pretrained LLaMA model into an instruction-aligned assistant. While smaller than pretraining, it is conceptually critical: it teaches the model to respond as an assistant rather than merely continue text.</p> <p>In later iterations, LoRA fine-tuning can be integrated to reduce compute and memory costs. This version, however, performs full fine-tuning to maximize learning fidelity and alignment clarity for educational demonstration.</p> <p>Final Notes: Because SFT is much lighter than pretraining (on smaller scale training), DDP is not used, as the fine-tuning process is fast enough to run efficiently on a single GPU.</p>"},{"location":"sft/overview/","title":"Supervised Fine-Tuning (SFT) Overview","text":""},{"location":"sft/overview/#introduction-from-pretraining-to-instruction-following","title":"Introduction: From Pretraining to Instruction Following","text":"<p>Supervised Fine-Tuning (SFT) represents the crucial second stage in developing capable language models. While pretraining gives the model a comprehensive understanding of language structure and world knowledge through next-token prediction on vast text corpora, SFT transforms this \"knowledgeable but unguided\" model into a helpful assistant that can follow instructions, engage in conversations, and provide useful responses.</p>"},{"location":"sft/overview/#the-fundamental-shift","title":"The Fundamental Shift","text":"<p>Pretraining Objective: Input: \"Explain gravity in simple words.\" Target (continuation): \"Explain relativity in simple words. Explain quantum mechanics in simple words.\" </p> <p>Here, the model is trained to continue text, so it often just echoes or extends patterns.  Typically, this produces verbose or repetitive completions rather than addressing intent, where the knowledge is broad but behavior is mostly unguided. </p> <p>SFT Objective: Input: \"Explain gravity in simple words.\" Target (response): \"Gravity is the force that pulls objects toward each other, like how Earth pulls things down.\" </p> <p>After SFT, the goal is to have the model trained to interpret input as an instruction.  The resulting generation should be a direct, helpful answer instead of raw continuation.  It would also learn assistant-like behavior, such as prioritizing conciseness, being cooperative, and user-focused. </p>"},{"location":"sft/overview/#what-this-section-covers","title":"What This Section Covers","text":"<p>This SFT documentation section provides a comprehensive guide to transforming the pretrained model into an instruction-following assistant. We'll cover:</p>"},{"location":"sft/overview/#1-dataset-preparation-sftdatasetmd","title":"1. Dataset Preparation (<code>sft/dataset.md</code>)","text":"<ul> <li>Instruction Dataset Structure: Understanding the JSON format with User, Assistant, and Template fields</li> <li>Data Loading Pipeline: The <code>JSONDatasetLoader</code> class and its batching strategy</li> <li>Quality Considerations: What makes good SFT data and how to curate effective training examples</li> </ul>"},{"location":"sft/overview/#2-prompt-formatting-sftprompt_formattingmd","title":"2. Prompt Formatting (<code>sft/prompt_formatting.md</code>)","text":"<ul> <li>Special Token System: Explanation of the 6 custom tokens (<code>&lt;SOT&gt;</code>, <code>&lt;EOT&gt;</code>, <code>&lt;SOU&gt;</code>, <code>&lt;EOU&gt;</code>, <code>&lt;SOA&gt;</code>, <code>&lt;EOA&gt;</code>) and their semantic roles</li> <li>Training Format: How to structure prompts during model training</li> <li>System Prompt Integration: Incorporating instructions and model personality through template fields</li> <li>Multi-turn Handling: Formatting complex conversations with multiple exchanges</li> </ul>"},{"location":"sft/overview/#3-training-process-sfttrainingmd","title":"3. Training Process (<code>sft/training.md</code>)","text":"<ul> <li>Training Differences: How SFT training diverges from pretraining</li> <li>Loss Masking Strategy: Only computing loss on assistant responses, not user queries or system prompts</li> <li>Validation Strategy: Using validation examples during finetuning to evaluate model progress</li> <li>Hyperparameter Tuning: Optimal learning rates, batch sizes, and training duration for SFT</li> </ul>"},{"location":"sft/overview/#4-utilities-and-implementation-sftutilsmd","title":"4. Utilities and Implementation (<code>sft/utils.md</code>)","text":"<ul> <li>Core Functions: <code>tokenize_and_pad_data</code>, the heart of SFT data processing</li> <li>Padding Logic: For sequence alignment and loss computation</li> </ul>"},{"location":"sft/overview/#key-technical-concepts","title":"Key Technical Concepts","text":""},{"location":"sft/overview/#the-sft-training-objective","title":"The SFT Training Objective","text":"<p>Unlike pretraining where we train on a massive text corpora, SFT is more about behavior tuning, using a much smaller dataset, composed to 3 main components:  </p> <ul> <li>System Instructions</li> <li>User Query</li> <li>Assistant Response</li> </ul> <p>In the pretraining phase, the input is a long stream of tokens, and output is essentially the same tokens, just shifted by one.  However in SFT, the goal is now to generate a response that answers the user questions. It would be formatted as something like:   </p> <p>Input Sequence (x): \"System instructions\\n\\nUser query\\n\" Target Sequence (y): \"Assistant response\"</p> <p>In SFT, the loss is ONLY computed on positions corresponding to \"Assistant response\" All other tokens (system prompt, user query, special tokens) are ignored via padding</p> <p>This ensures the model learns to generate appropriate responses without being penalized for not predicting user inputs or system instructions.</p>"},{"location":"sft/overview/#why-sft-works","title":"Why SFT Works","text":"<p>SFT leverages the foundational knowledge acquired during pretraining and redirects it toward helpful behavior:</p> <ol> <li>Knowledge Preservation: The model retains all linguistic patterns and factual knowledge from pretraining  </li> <li>Behavioral Alignment: Learns to apply this knowledge in response to user instructions  </li> <li>Format Compliance: Adopts consistent response patterns and conversation structures  </li> <li>Helpfulness: Develops tendencies toward beneficial rather than generic responses  </li> </ol>"},{"location":"sft/overview/#implementation-notes","title":"Implementation Notes","text":"<p>Single-GPU Training: Unlike the DDP implementation in pretraining, the SFT pipeline currently uses single-GPU training. This decision is mostly because SFT typically requires orders of magnitude fewer iterations than pretraining and results in faster iteration and debugging without synchronization complexities that DDP introduces.</p> <p>Full Fine-Tuning Approach: This currently implement full parameter updates rather than parameter-efficient methods like LoRA, though the infrastructure for LoRA integration exists and might be documented in future updates.</p>"},{"location":"sft/overview/#performance-expectations","title":"Performance Expectations","text":"<p>For this 1.3B parameter model: (Need to come back and update this later on after going through full process)</p> <ul> <li>Training Time: ~12 hours (vs months for pretraining)  </li> <li>Dataset Size: 200,000 high-quality examples  </li> <li>Convergence: Usually within 3-5 epochs  </li> <li>Quality Improvement: Dramatic improvement in instruction following with relatively little training</li> </ul>"},{"location":"sft/prompt_formatting/","title":"Prompt Formatting","text":"<p>When creating a supervised fine-tuning (SFT) dataset, one of the most critical stages is how the data is formatted before it ever reaches the model. Even if the dataset contains high-quality examples, the model will not learn properly if the structure is inconsistent, ambiguous, or misaligned between inputs and outputs. </p> <p>The formatting process for this project is handled by two main functions: <code>format_training_prompt</code> and <code>format_inference_prompt</code>, located in <code>simple_llama\\finetune\\format_llm_prompt.py</code>. Together, these functions take the raw dataset entries and transform them into structured prompts that include special tokens. These tokens act like markers that clearly delineate system instructions, user input, and assistant responses. By enforcing this strict formatting scheme, we create a consistent \u201clanguage of conversation structure\u201d that the model can rely on.</p>"},{"location":"sft/prompt_formatting/#why-prompt-formatting-matters","title":"Why Prompt Formatting Matters","text":"<p>To understand why prompt formatting matters so much, it helps to recall what pretraining and supervised fine-tuning are doing differently. Pretraining exposes the model to raw text data, teaching it general language patterns and next-token prediction. However, the model has no inherent concept of conversation, alignment, or task boundaries. Without explicit formatting, the model cannot distinguish between instruction, query, and response.</p> <p>For example, if we simply gave the model text like:</p> <pre><code>User: What is 2+2?\nAssistant: 4\n</code></pre> <p>it might appear obvious to us, but the model does not know that \u201cUser\u201d is a role marker, that \u201cAssistant\u201d should follow with an aligned answer, or that there are clear start and end boundaries for each role. Worse, if the formatting changes from example to example (sometimes <code>Q:</code> and <code>A:</code>, sometimes <code>Human:</code> and <code>AI:</code>), the model will learn a messy, inconsistent interface.</p> <p>The use of special tokens like <code>&lt;SOT&gt;</code>, <code>&lt;SOU&gt;</code>, and <code>&lt;SOA&gt;</code> solves this problem by creating unambiguous delimiters. Every prompt has the same structure. The model sees not only the words but also these unique markers, which act like anchors in the training data. Over time, the model comes to associate <code>&lt;SOU&gt;</code> with the beginning of a user message, <code>&lt;SOA&gt;</code> with the start of its own reply, and <code>&lt;EOA&gt;</code> with the conclusion of its generated output. This consistency is what makes supervised fine-tuning effective for the model to learn. </p>"},{"location":"sft/prompt_formatting/#special-tokens","title":"Special Tokens","text":"<p>It is worth emphasizing why this token scheme works so well. Transformers do not understand roles, but they are excellent at picking up statistical regularities. By embedding special tokens directly into the training data, we effectively give the model a rule for conversational formats. The <code>&lt;SOT&gt;</code> token always signals the beginning of the context, <code>&lt;SOU&gt;</code> always signals a human query, and <code>&lt;SOA&gt;</code> always signals the assistant\u2019s role.</p> <p>Here is the list of special tokens used during Supervised Fine Tuning:  </p> <ul> <li><code>&lt;SOT&gt;</code>: (Start of Template) Marks the start of System Prompt</li> <li><code>&lt;EOT&gt;</code>: (End of Template) Marks the end of System Prompt</li> <li><code>&lt;SOU&gt;</code>: (Start of User) Marks the start of User Query</li> <li><code>&lt;EOU&gt;</code>: (End of User) Marks the end of User Query</li> <li><code>&lt;SOA&gt;</code>: (Start of Assistant) Marks the start of Assistant Response</li> <li><code>&lt;EOA&gt;</code>: (End of Assistant) Marks the end of Assistant Response</li> </ul> <p>In addition to those Start/Ending special token markers, there's also a special <code>&lt;PAD&gt;</code> token used to pad the example sequences until they are of uniform length. Further details will be covered in the <code>utils</code> section</p>"},{"location":"sft/prompt_formatting/#prompt-formatting_1","title":"Prompt Formatting","text":"<p>Prompt formatting is a crucial step in supervised fine-tuning (SFT). It defines how raw conversational data\u2014system prompts, user messages, and assistant responses\u2014are transformed into the structured sequences that the model will see during training. Without careful formatting, the model may learn the wrong objectives (such as imitating the user or repeating system messages), which can reduce the quality and reliability of responses.</p>"},{"location":"sft/prompt_formatting/#the-formatting-function","title":"The Formatting Function","text":"<p>The project defines a central function, <code>format_training_prompt</code>, that prepares data for SFT. Here is the actual implementation:</p> <pre><code>def format_training_prompt(user: list[str], assistant: list[str], template: str):\n    \"\"\"\n    Formats conversation into a single training prompt with special tokens.\n\n    Returns:\n        A tuple of:\n            - formatted_prompt (str): Full prompt string with system prompt, user turns, and assistant responses.\n            - target_response (str): Only the final assistant response (with &lt;EOA&gt;), to be used for loss masking.\n\n    Notes:\n        - The full prompt is structured with &lt;SOT&gt;, &lt;EOT&gt;, &lt;SOU&gt;, &lt;EOU&gt;, &lt;SOA&gt;, and &lt;EOA&gt; tokens.\n        - If template == 'CUSTOM', default system prompt will be used\n        - Assumes assistant[-1] is the target response we want to supervise on.\n\n    Example:\n        full_prompt, target = format_training_prompt([\"Hi\"], [\"Hello there.\"], \"CUSTOM\")\n        full_prompt -&gt; \"&lt;SOT&gt;...&lt;SOU&gt;Hi&lt;EOU&gt;...&lt;SOA&gt;Hello there.&lt;EOA&gt;\"\n        target      -&gt; \"Hello there.&lt;EOA&gt;\"\n    \"\"\"\n\n    assert len(user) == len(assistant), f\"Number of user strings, {len(user)=}, must be equal to number of assistant strings, {len(assistant)=}\"\n\n    if template == \"CUSTOM\":\n        template = SYSTEM_PROMPT\n\n    concat_UA = [f\"&lt;SOU&gt;{u}&lt;EOU&gt;\\n&lt;SOA&gt;{a}&lt;EOA&gt;\" for u, a in zip(user, assistant)]\n    joined_UA = \"\\n\\n\".join(concat_UA)\n\n    data = (f\"&lt;SOT&gt;{template}&lt;EOT&gt;\\n\\n{joined_UA}\", f\"{assistant[-1]}&lt;EOA&gt;\")\n\n    assert data[0].endswith(data[1]), (\n        f\"Final response mismatch:\\n\"\n        f\"Expected ending: '{data[1]}'\\n\"\n        f\"Actual ending:   '{data[0][-len(data[1]):]}'\"\n    )\n\n    return data\n</code></pre> <p>The function accepts three arguments:  </p> <ul> <li><code>user</code>: a list of user queries (strings). Each element is one user message.  </li> <li><code>assistant</code>: a list of assistant responses (strings). Each element corresponds to the user message at the same index.  </li> <li><code>template</code>: a string that specifies the system prompt. If <code>\"CUSTOM\"</code> is provided, the function automatically inserts the project\u2019s default system prompt (a general instruction describing the model\u2019s role and behavior).  </li> </ul> <p>The formatting process proceeds in several stages:</p> <ol> <li> <p>System Prompt Wrapping    The system-level template is wrapped in special start and end tokens: <pre><code>&lt;SOT&gt; ... &lt;EOT&gt;\n</code></pre>    This ensures the model can clearly distinguish between global instructions and user/assistant turns.</p> </li> <li> <p>User\u2013Assistant Turns    Each user input is wrapped with <code>&lt;SOU&gt; ... &lt;EOU&gt;</code>, and each assistant reply is wrapped with <code>&lt;SOA&gt; ... &lt;EOA&gt;</code>. These pairs are concatenated sequentially: <pre><code>&lt;SOU&gt;User message&lt;EOU&gt;\n&lt;SOA&gt;Assistant response&lt;EOA&gt;\n</code></pre>    If there are multiple conversational turns, they are joined together with line breaks.</p> </li> <li> <p>Final Data Output    The function returns a tuple <code>(full_prompt, target_response)</code>:  </p> </li> <li>full_prompt: Contains the system template and the entire conversation history, including all user messages and assistant replies.  </li> <li>target_response: Only the last assistant reply (ending with <code>&lt;EOA&gt;</code>). Loss is calculated exclusively on this portion.  </li> </ol> <p>This final design prevents the model from being penalized for predicting the system or user text, and instead forces learning to focus on producing the assistant\u2019s output.</p>"},{"location":"sft/prompt_formatting/#example-walkthrough","title":"Example Walkthrough","text":"<p>Let\u2019s take a minimal example with a single turn:</p> <pre><code>full_prompt, target = format_training_prompt(\n    user=[\"What is 2+2?\"],\n    assistant=[\"4\"],\n    template=\"CUSTOM\"\n)\n</code></pre> <p>The result would be:</p> <pre><code>full_prompt =\n&lt;SOT&gt;Your name is Simple LLaMA - a language model...&lt;EOT&gt;\n\n&lt;SOU&gt;What is 2+2?&lt;EOU&gt;\n&lt;SOA&gt;4&lt;EOA&gt;\n\n\n\ntarget =\n4&lt;EOA&gt;\n</code></pre> <p>Here the system prompt is replaced with the project\u2019s default instructions, because the template argument was <code>\"CUSTOM\"</code>. The <code>full_prompt</code> contains everything, but the <code>target</code> is only the final assistant response. This allows us to mask loss outside of the assistant\u2019s output.</p> <p>For multi-turn conversations:</p> <pre><code>full_prompt, target = format_training_prompt(\n    user=[\"What is the capital of France?\", \"I see. What about the capital of Japan?\"],\n    assistant=[\"The capital of France is Paris.\", \"It is Japan.\"],\n    template=\"CUSTOM\"\n)\n</code></pre> <p>We would get:</p> <pre><code>full_prompt =\n&lt;SOT&gt;Your name is Simple LLaMA...&lt;EOT&gt;\n\n&lt;SOU&gt;What is the capital of France?&lt;EOU&gt;\n&lt;SOA&gt;The capital of France is Paris.&lt;EOA&gt;\n\n&lt;SOU&gt;I see. What about the capital of Japan?&lt;EOU&gt;\n&lt;SOA&gt;It is Japan.&lt;EOA&gt;\n\n\n\ntarget =\nIt is Japan.&lt;EOA&gt;\n</code></pre> <p>Notice how only the final assistant reply is selected for loss.</p> <p>This formatting convention enforces role separation. The model is never trained to generate user queries or repeat system instructions\u2014it is only trained to complete the assistant\u2019s role. This distinction is essential for alignment: the assistant role must remain coherent and helpful across turns, while system and user text is treated as conditioning context only.</p> <p>By structuring prompts this way, we also make it easy to scale up to multi-turn conversations without ambiguity. Each turn is delimited with clear markers, ensuring that the model can track conversational roles and responsibilities.</p>"},{"location":"sft/prompt_formatting/#summary","title":"Summary","text":"<ul> <li>The function <code>format_training_prompt</code> converts raw system, user, and assistant data into structured training-ready text.  </li> <li>System instructions are wrapped with <code>&lt;SOT&gt;</code> and <code>&lt;EOT&gt;</code>.  </li> <li>User and assistant turns are wrapped with <code>&lt;SOU&gt; ... &lt;EOU&gt;</code> and <code>&lt;SOA&gt; ... &lt;EOA&gt;</code>.  </li> <li>The function outputs <code>(full_prompt, target_response)</code>, where only the final assistant response is used for loss calculation.  </li> </ul> <p>This design provides clarity in supervised fine-tuning, to make sure that the model learns to generate assistant responses without being distracted by other roles in the conversation.</p>"},{"location":"sft/utils/","title":"Utilities for SFT","text":""},{"location":"sft/utils/#tokenize-and-pad-data","title":"Tokenize and Pad Data","text":"<p>The next step after formatting the training examples is the transformation of raw <code>(x, y)</code> string pairs into properly padded tensors. In this project, the function <code>tokenize_and_pad_data</code> is responsible for doing exactly that.  </p> <p>At first glance, the function may look overwhelming: it includes several layers of tokenization, checks, and padding strategies. But once broken down into its main stages, the logic becomes much clearer.  </p> <p>Let\u2019s start by looking at the function signature and its purpose:  </p> <pre><code>def tokenize_and_pad_data(batch: list[tuple], tokenizer: Tokenizer, pad_id: int, max_seq_len: int, dynamic: bool, device: str):\n    \"\"\"\n    Tokenizes and pads a batch of (x, y) training pairs for SFT.\n\n    Each element in `batch` is a tuple:\n        - x: full prompt including template, user queries, and assistant responses\n        - y: only the final assistant response to supervise with loss\n\n    The x sequence is right-truncated to `max_seq_len`.\n    The y sequence is left-padded so it aligns with the end of x.\n    Left-padded values in y are filled with `pad_id` and are ignored in loss via `ignore_index`.\n    \"\"\"\n</code></pre> <p>At its core, the function takes in a list of <code>(x, y)</code> examples (batch)  where:  </p> <ul> <li><code>x</code> is the full formatted training prompt (system + user + assistant conversation).  </li> <li><code>y</code> is the target assistant response only (with <code>&lt;EOA&gt;</code> at the end).  </li> </ul> <p>It also takes in the tokenizer, to convert the x-y strings into token ids, pad_id corresponds to the token id for a pad token, <code>max_seq_len</code>, the hyper parameter that controls the maximum sequence length allowed, dynamic is a bool meaning if to dynamically pad based on longest example or all up to max_seq_len, device is usually cpu or cuda Don't worry too much about the other inputs for now, just note the most important inputs is the batch, tokenizer, pad_id and max_seq_len Further details below. </p> <p>The goal is to produce two tensors:  </p> <ol> <li><code>x_tensor</code>: tokenized, padded input prompts.  </li> <li><code>y_tensor</code>: tokenized, padded targets aligned to <code>x</code>, where padding positions are filled with <code>pad_id</code> so the loss function ignores them.  </li> </ol>"},{"location":"sft/utils/#step-1-tokenization-and-padding","title":"Step 1: Tokenization and Padding","text":"<pre><code>assert len(batch) != 0, \"Given batch data should not be empty!\"\n\nx_data, y_data = [], []\nmax_len = 0  # Maximum tensor len\nexceed_len = 0\nfor example in batch:\n    x, y = example  # Unpack values\n\n    # Tokenize x-y pair\n    x = tokenizer.encode(x).ids\n    y = tokenizer.encode(y).ids\n\n    if len(x) &gt; max_seq_len:  # max_seq_len is inclusive of context, so x shouldn't be &gt;= that\n        exceed_len += 1\n        continue\n\n    max_len = max(max_len, len(x))\n    x_data.append(torch.tensor(x, dtype=torch.long, device=device))\n\n    y = torch.tensor(y, dtype=torch.long, device=device)\n    num_left_pad = len(x) - len(y) - 1  # Need an additional right pad later on for left-shift\n\n    if num_left_pad &lt; 0:\n        warnings.warn(f\"Target response longer than input window. Skipping.\")\n        continue\n\n    y_left_pad = torch.full((num_left_pad,), pad_id, device=device)\n    y_data.append(torch.cat((y_left_pad, y), dim=-1))\n</code></pre> <p>First, ensure that the given batch is not empty. Initialize two empty lists, <code>x_data</code> and <code>y_data</code>, which will later hold the two resulting x-y tensors.  </p> <p>Next, the two variables <code>max_len</code> and <code>exceed_len</code> are initialized. <code>max_len</code> keeps track of the length of the longest sequence of tokens seen in the current batch, while <code>exceed_len</code> records how many training examples surpassed the maximum allowed sequence length.  </p> <p>Now we iterate through the batch:</p> <p><code>for example in batch:</code></p> <p>Where each <code>example</code> is the <code>(x, y)</code> string pair.  </p> <p>Both <code>x</code> and <code>y</code> are transformed into lists of integers, each representing tokens from the vocabulary.  </p> <p>We now check if the number of tokens in tokenized string <code>x</code> is greater than <code>max_seq_len</code>. If so, we discard that example. Truncation could be done instead, but that\u2019s a separate concern. For now, the goal is to prevent prompts from overflowing the model\u2019s context window. If discarded, increment <code>exceed_len</code> by 1. (Though the naming could be improved, it represents the number of examples that exceeded <code>max_seq_len</code>.)  </p> <p>If it does not exceed <code>max_seq_len</code>, then continue by updating <code>max_len</code> with:  </p> <p><code>max_len = max(max_len, len(x))</code></p> <p>Here, <code>max_seq_len</code> refers to the fixed hyperparameter (e.g. 2048 or 4096 tokens), while <code>max_len</code> is the longest sequence of input tokens seen so far in this batch. The former filters out overly long examples, the latter is used to determine how much padding is needed later.  </p> <p>Then, transform <code>x</code> into a tensor and append it to <code>x_data</code>. This will be the input tensor to the model as one example sequence.  </p> <p>For <code>y</code>, it\u2019s more nuanced. We cannot simply tokenize and append because we need alignment for the loss function. First, calculate the amount of left padding required, accounting for a shift (<code>-1</code>). Then create a left padding tensor, and concatenate it with <code>y</code>. Finally, append the result to <code>y_data</code>.  </p> <p>Let\u2019s illustrate with a concrete example. Suppose we have an <code>(x, y)</code> pair where:  </p> <p>x: </p> <pre><code>&lt;SOU&gt;What is 2+2?&lt;EOU&gt;\n&lt;SOA&gt;4&lt;EOA&gt;\n</code></pre> <p>y: </p> <pre><code>4&lt;EOA&gt;\n</code></pre> <p>(We omit the system prompt here for brevity.)  </p> <p>Tokenized, <code>x</code> might look like: <code>[4, 4909, 4862, 981, 3172, 981, 2356, 5, 6, 411, 7]</code> </p> <p>And <code>y</code>: <code>[411, 7]</code> </p> <p>Here, <code>411</code> corresponds to token \u201c4\u201d and <code>7</code> corresponds to <code>&lt;EOA&gt;</code>. Importantly, <code>y</code> is always the suffix of <code>x</code>.  </p> <p>For <code>x</code>, we immediately convert it into a tensor and append it to <code>x_data</code>.  </p> <p>For <code>y</code>, convert to a tensor and compute:  </p> <p><code>num_left_pad = len(x) - len(y) - 1 = 11 - 2 - 1 = 8</code> </p> <p>Then create:  </p> <p><code>y_left_pad = torch.tensor([2, 2, 2, 2, 2, 2, 2, 2])</code> </p> <p>(assuming <code>pad_id = 2</code>).  </p> <p>Concatenate with <code>y</code>:  </p> <p><code>torch.tensor([2, 2, 2, 2, 2, 2, 2, 2, 411, 7])</code> </p> <p>Append this to <code>y_data</code>.  </p> <p>Unrolling side by side:  </p> <pre><code>x: [4, 4909, 4862, 981, 3172, 981, 2356, 5,   6, 411, 7]\ny: [2,    2,    2,   2,    2,   2,    2, 2, 411,   7]\n</code></pre> <p>Notice that the targets are padding until we hit token id <code>411</code>. This design ensures the model is not trained to predict the system prompt or the user queries \u2014 only the assistant\u2019s outputs.  </p> <p>When the model sees:  </p> <pre><code>&lt;SOU&gt;What is 2+2?&lt;EOU&gt;\n&lt;SOA&gt;\n</code></pre> <p>It is expected to predict token <code>411</code> (the number 4). Then, given the sequence with <code>411</code> included:  </p> <pre><code>&lt;SOU&gt;What is 2+2?&lt;EOU&gt;\n&lt;SOA&gt;4\n</code></pre> <p>The model should predict <code>&lt;EOA&gt;</code> (<code>7</code>).  </p> <p>Thus, this remains next-token prediction, but now constrained to assistant responses. Instead of uncontrolled continuation, the model learns structured QA behavior.  </p> <p>Do note the if conditional block where <code>if num_left_pad &lt; 0</code>, meaning the target response is longer than the input, the example is malformed and skipped. This normally shouldn't occur, since <code>y</code> is expected to be a suffix of <code>x</code>, but added as a safety check. </p>"},{"location":"sft/utils/#step-2-validation-check","title":"Step 2: Validation Check","text":"<p>After populating <code>x_data</code> and <code>y_data</code>, the function performs a quick check:</p> <pre><code># return x_data, y_data\nassert len(x_data) != 0, f\"All examples has been skipped due to all chat conversations exceeding {max_seq_len=}\"\nif exceed_len/len(batch) &gt;= 0.1:\n    warnings.warn(f\"{100 * exceed_len/len(batch):.2f}% of examples in this batch has been skipped due to assistant responses exceeding {max_seq_len=}\")\n\nmax_len = max_len if dynamic else max_seq_len\n</code></pre> <p>The first assertion ensures that the dataset did not collapse entirely. If <code>len(x_data)</code> is zero, it means that all of the examples in the batch were skipped, usually because they exceeded the maximum allowed sequence length (<code>max_seq_len</code>). This would be a problematic error, since the model would have no valid training examples to work with, and so the assertion is a necessary safeguard.</p> <p>The second check introduces a soft warning mechanism. It calculates the ratio of discarded examples (<code>exceed_len / len(batch)</code>) and, if at least 10% of the examples in the batch were rejected, issues a warning. This does not stop execution but serves as a red flag: your dataset or chosen hyperparameters may be suboptimal, and the model could be missing out on significant amounts of training data.</p> <p>Finally, <code>max_len</code> is updated. If the <code>dynamic</code> flag is set to <code>True</code>, the function uses the maximum length observed in the current batch. This keeps padding minimal, ensuring better efficiency since padding consumes memory and compute without adding training signal. However, if <code>dynamic</code> is <code>False</code>, the function defaults to the global <code>max_seq_len</code>, which ensures all batches are consistently padded to the same fixed length. This option is particularly useful when stress-testing hardware capacity, making sure everything fits into memory.</p>"},{"location":"sft/utils/#step-3-return-tensors","title":"Step 3: Return Tensors","text":"<p>Finally, the function decides how to pad <code>x_data</code> and <code>y_data</code> to a consistent length:  </p> <pre><code>x_data = torch.stack([\n    torch.concat((x, torch.full((max_len - len(x),), pad_id, device=device)), dim=-1)\n    for x in x_data\n])\n\ny_data = torch.stack([\n    torch.concat((y, torch.full((max_len - len(y),), pad_id, device=device)), dim=-1)\n    for y in y_data\n])\n\nassert x_data.shape == y_data.shape\nassert len(x_data.shape) == 2\nreturn x_data, y_data\n</code></pre> <p>Let\u2019s break it down step by step.  </p> <p>Padding the Input (<code>x_data</code>):</p> <p>At this point, <code>x_data</code> is a list of 1D tensors, each tensor corresponding to the tokenized version of a single input string. Since these strings will almost always be of different lengths, we cannot directly batch them together into a single 2D tensor. PyTorch requires all rows in a tensor to have the same length (no ragged tensors). To fix this, we add padding tokens so that every example reaches the same length.  </p> <p>This line handles it:  </p> <pre><code>torch.concat((x, torch.full((max_len - len(x),), pad_id, device=device)), dim=-1)\n</code></pre> <p>Here,  </p> <ul> <li><code>x</code> is the original sequence of token IDs.  </li> <li><code>torch.full((max_len - len(x),), pad_id, device=device)</code> creates a 1D tensor filled with the padding token ID (<code>pad_id</code>), with just enough elements to extend <code>x</code> to <code>max_len</code>.  </li> <li><code>torch.concat(...)</code> joins the original sequence <code>x</code> with the padding tensor along the last dimension, effectively extending <code>x</code> to the full <code>max_len</code>.  </li> </ul> <p>After this, each input sequence has exactly the same length, ensuring they can all be stacked together.  </p> <p>Padding the Targets (<code>y_data</code>)</p> <p>The process for <code>y_data</code> is nearly identical. Each target sequence is extended with padding tokens until it also reaches <code>max_len</code>. One subtle difference is that <code>y_data</code> already had left padding applied in Step 1 to align assistant responses with the correct positions in the input. This step adds any right padding that is still needed so that <code>y_data</code> fully matches the shape of <code>x_data</code>.  </p> <p>This ensures both <code>x_data</code> and <code>y_data</code> align perfectly in shape, with every row corresponding to one example in the batch.  </p> <p>Stacking into Final Tensors</p> <p>Finally, both padded lists are wrapped in <code>torch.stack([...])</code>. This function takes a list of tensors of identical shape and combines them into a single tensor by adding a new leading dimension. In this case, that leading dimension corresponds to the batch size.  </p> <ul> <li>After stacking, <code>x_data</code> has shape <code>(batch_size, max_len)</code>.  </li> <li>Similarly, <code>y_data</code> has shape <code>(batch_size, max_len)</code>.  </li> </ul> <p>This step transforms our batch from a list of variable-length tensors into two consistent 2D tensors ready for model training.  </p> <p>Before returning, the function runs two assertions:  </p> <pre><code>assert x_data.shape == y_data.shape\nassert len(x_data.shape) == 2\n</code></pre> <p>To make sure that inputs and targets are aligned and that the final tensors are indeed two-dimensional, with one axis for the batch and one for the sequence length.  </p> <p>If either check fails, it indicates a problem in preprocessing that must be fixed before training continues.</p> <p>The function then returns the pair <code>(x_data, y_data)</code>, now guaranteed to be properly padded, aligned, and in a form suitable for direct use in the model\u2019s forward pass.  </p>"},{"location":"sft/utils/#model-evaluation","title":"Model Evaluation","text":"<p>The <code>eval_model</code> function provides the evaluation mechanism for measuring how well the model performs in generating assistant responses to user inputs during supervised fine-tuning. Here's the function signature: </p> <pre><code>@torch.no_grad()\ndef eval_model(model: LLaMaTransformer,\n               criterion: CrossEntropyLoss,\n               tokenizer: Tokenizer,\n               dataset_loader: JSONDatasetLoader,\n               use_amp: bool,\n               full_eval: bool,\n               pad_id: int,\n               max_seq_len: int,\n               dynamic: bool,\n               device: str) -&gt; float:\n</code></pre> <p>The <code>@torch.no_grad()</code> decorator is used so that gradients are not tracked during evaluation. This reduces computational overhead since we are not performing backpropagation (so gradients aren't needed).</p>"},{"location":"sft/utils/#parameters","title":"Parameters","text":"<ul> <li>model: The model that's currently undergoing fine-tuning.</li> <li>criterion: The loss function (<code>CrossEntropyLoss</code>) used to measure prediction error.</li> <li>tokenizer: Tokenizer used to encode text into token IDs.</li> <li>dataset_loader: Instance of <code>JSONDatasetLoader</code> that manages training and validation data.</li> <li>use_amp: Boolean controlling automatic mixed precision (BF16 or FP32) for performance optimization.</li> <li>full_eval: Determines whether to evaluate the entire validation set or a single batch.</li> <li>pad_id: Token ID used for padding, ignored by the loss function.</li> <li>max_seq_len: Maximum allowable sequence length.</li> <li>dynamic: Whether to use dynamic padding (based on longest sequence in batch) or fixed padding.</li> <li>device: Target device ('cuda' or 'cpu').</li> </ul>"},{"location":"sft/utils/#full-evaluation-path","title":"Full Evaluation Path","text":"<p>When <code>full_eval=True</code>, the function runs through the entire validation dataset, calculating loss across all batches:</p> <pre><code>if full_eval:  # Meaning we want to iterate over the entire validation epoch\n    current_val_epoch = dataset_loader.val_epoch\n    losses = []\n    while current_val_epoch == dataset_loader.val_epoch:\n        batch = dataset_loader.get_batch(train=False, increment_val_idx=True)\n\n        x, y = tokenize_and_pad_data(batch=batch, tokenizer=tokenizer, pad_id=pad_id, max_seq_len=max_seq_len,\n                                     dynamic=dynamic, device=device)\n\n        with torch.autocast(device_type=device, dtype=torch.bfloat16 if use_amp else torch.float32):\n            pred = model(x)\n\n            B, T, C = pred.shape\n            loss = criterion(pred.reshape(B * T, C), y.reshape(B * T))\n\n        losses.append(loss.item())\n\n    return sum(losses)/len(losses)\n</code></pre> <p>Here\u2019s what happens step-by-step:</p> <ol> <li>Initialize tracking variables: The function first records the current validation epoch (<code>current_val_epoch</code>) and creates an empty list <code>losses</code> to store batch-level losses.</li> <li>Iterate through validation data: It loops until one full validation epoch is processed.</li> <li>Batch retrieval: Each batch of validation examples is fetched from the loader.</li> <li>Tokenization &amp; Padding: The batch is passed through <code>tokenize_and_pad_data()</code> to ensure uniform tensor sizes.</li> <li>Forward pass: The model processes <code>x</code> under mixed precision, outputting logits of shape <code>(batch, seq_len, vocab_size)</code>.</li> <li>Loss computation: The predictions and labels are reshaped into 2D matrices, and cross-entropy loss is computed.</li> <li>Aggregation: Each batch loss is appended to the list, and finally, the average loss across all batches is returned.</li> </ol> <p>This mode is used at the end of each epoch to measure the model\u2019s complete validation performance.</p>"},{"location":"sft/utils/#single-batch-evaluation-path","title":"Single-Batch Evaluation Path","text":"<p>When <code>full_eval=False</code>, only a single batch of validation data is evaluated. This is used for quick checks during training, at evaluation intervals:</p> <pre><code>else:  # Just want a single evaluation\n    batch = dataset_loader.get_batch(train=False, increment_val_idx=False)\n\n    x, y = tokenize_and_pad_data(batch=batch, tokenizer=tokenizer, pad_id=pad_id, max_seq_len=max_seq_len,\n                                 dynamic=dynamic, device=device)\n\n    with torch.autocast(device_type=device, dtype=torch.bfloat16 if use_amp else torch.float32):\n        pred = model(x)\n        B, T, C = pred.shape\n        loss = criterion(pred.reshape(B * T, C), y.reshape(B * T))\n\n    return loss.item()\n</code></pre> <p>This version skips looping and calculates the loss for a single batch. It is far faster and is typically used to monitor progress during training \u2014 allowing the user to see whether the loss is trending downward.</p>"},{"location":"sft/utils/#usage-context","title":"Usage Context","text":"<p>During fine-tuning, <code>eval_model()</code> is called repeatedly:</p> <ul> <li>Full evaluation: Once per epoch to get the complete validation loss.</li> <li>Single-batch evaluation: At regular intervals (e.g., every few hundred optimizer steps) for quick feedback.</li> </ul> <p>This provides a balance between speed and accuracy in tracking model performance. Over time, decreasing validation loss (and its corresponding perplexity) signals effective alignment and improved response generation behavior.</p>"}]}