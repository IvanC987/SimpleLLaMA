site_name: SimpleLLaMA Documentation
theme:
  name: material
  features:
    - content.math
  palette:
    - scheme: default
      primary: indigo
      accent: indigo
      toggle:
        icon: material/weather-night
        name: Switch to dark mode
    - scheme: slate
      primary: indigo
      accent: indigo
      toggle:
        icon: material/weather-sunny
        name: Switch to light mode

nav:
  - Home: index.md
  - Pretraining:
      - Overview: pretraining/overview.md
      - Dataset Preparation: pretraining/dataset.md
      - Tokenization:
          - Overview: pretraining/tokenization/overview.md
          - Algorithms: pretraining/tokenization/algorithms.md
          - Examples: pretraining/tokenization/examples.md
          - Project Usage: pretraining/tokenization/project.md
      - Model Architecture:
          - Overview: pretraining/model_architecture/overview.md
          - Embeddings: pretraining/model_architecture/embeddings.md
          - Attention: pretraining/model_architecture/attention.md
          - Normalization: pretraining/model_architecture/normalization.md
          - FeedForward: pretraining/model_architecture/feedforward.md
          - Layer Block: pretraining/model_architecture/decoder_block.md
          - End To End: pretraining/model_architecture/end_to_end.md
      - Training Process (Beginner):
          - Introduction: pretraining/training_beginner/introduction.md
          - Model Configurations: pretraining/training_beginner/model_config.md
          - Dataset and Batching: pretraining/training_beginner/dataset_and_batching.md
          - Scheduler: pretraining/training_beginner/scheduler.md
          - Training Loop: pretraining/training_beginner/training_loop.md
      - Training Process (Advanced):
          - Basics Recap: pretraining/training_advanced/recap.md
          - Optimizer Details: pretraining/training_advanced/optimizers.md
          - Cross Entropy Loss: pretraining/training_advanced/loss_function.md
          - Gradient Accumulation: pretraining/training_advanced/gradient_accumulation.md
          - Distributed Data Parallel: pretraining/training_advanced/ddp.md
          - Throughput Optimizations: pretraining/training_advanced/throughput_optimizations.md
          - Checkpointing and Evaluations: pretraining/training_advanced/ckpt_and_eval.md
          - Final Walkthrough: pretraining/training_advanced/final_walkthrough.md
  - Supervised Fine-Tuning:
      - Overview: sft/overview.md
      - Dataset: sft/dataset.md
      - Prompt Formatting: sft/prompt_formatting.md
      - Utilities: sft/utils.md
      - Finetuning Process: sft/finetuning.md
  - RLHF:
      - Overview: rlhf/overview.md
      - Methods: rlhf/methods.md
  - Misc:
      - Benchmarking: misc/benchmarking.md
      - Inference: misc/inference.md
      - Notes: misc/notes.md
  - Custom Training:
      - Pretraining: custom_training/pretraining.md
      - Supervised Fine Tuning: custom_training/sft.md
      - RLHF: custom_training/rlhf.md


markdown_extensions:
  - admonition
  - codehilite
  - footnotes
  - pymdownx.arithmatex:
      generic: true
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid

extra_javascript:
  - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js

